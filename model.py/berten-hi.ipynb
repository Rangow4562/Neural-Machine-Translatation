{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "specific-brush",
   "metadata": {
    "gradient": {
     "editing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.8/site-packages (4.9.2)\n",
      "Requirement already satisfied: sentencepiece in /opt/conda/lib/python3.8/site-packages (0.1.95)\n",
      "Requirement already satisfied: datasets in /opt/conda/lib/python3.8/site-packages (1.11.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.8/site-packages (from transformers) (1.19.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.8/site-packages (from transformers) (2020.11.13)\n",
      "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /opt/conda/lib/python3.8/site-packages (from transformers) (0.10.3)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.8/site-packages (from transformers) (20.9)\n",
      "Requirement already satisfied: huggingface-hub==0.0.12 in /opt/conda/lib/python3.8/site-packages (from transformers) (0.0.12)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.8/site-packages (from transformers) (5.4.1)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.8/site-packages (from transformers) (2.24.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.8/site-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.8/site-packages (from transformers) (4.53.0)\n",
      "Requirement already satisfied: sacremoses in /opt/conda/lib/python3.8/site-packages (from transformers) (0.0.35)\n",
      "Requirement already satisfied: pyarrow!=4.0.0,>=1.0.0 in /opt/conda/lib/python3.8/site-packages (from datasets) (5.0.0)\n",
      "Requirement already satisfied: fsspec>=2021.05.0 in /opt/conda/lib/python3.8/site-packages (from datasets) (2021.7.0)\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.8/site-packages (from datasets) (2.0.2)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.8/site-packages (from datasets) (1.1.4)\n",
      "Requirement already satisfied: dill in /opt/conda/lib/python3.8/site-packages (from datasets) (0.3.4)\n",
      "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.8/site-packages (from datasets) (0.70.12.2)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.8/site-packages (from packaging->transformers) (2.4.7)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.8/site-packages (from huggingface-hub==0.0.12->transformers) (3.7.4.3)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.8/site-packages (from requests->transformers) (1.25.11)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /opt/conda/lib/python3.8/site-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.8/site-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.8/site-packages (from requests->transformers) (2020.12.5)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.8/site-packages (from sacremoses->transformers) (1.15.0)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.8/site-packages (from sacremoses->transformers) (1.0.1)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.8/site-packages (from sacremoses->transformers) (7.1.2)\n",
      "Requirement already satisfied: pytz>=2017.2 in /opt/conda/lib/python3.8/site-packages (from pandas->datasets) (2021.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.8/site-packages (from pandas->datasets) (2.8.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers sentencepiece datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "assumed-western",
   "metadata": {
    "gradient": {
     "editing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: seaborn in /opt/conda/lib/python3.8/site-packages (0.11.2)\n",
      "Requirement already satisfied: numpy>=1.15 in /opt/conda/lib/python3.8/site-packages (from seaborn) (1.19.2)\n",
      "Requirement already satisfied: pandas>=0.23 in /opt/conda/lib/python3.8/site-packages (from seaborn) (1.1.4)\n",
      "Requirement already satisfied: scipy>=1.0 in /opt/conda/lib/python3.8/site-packages (from seaborn) (1.6.0)\n",
      "Requirement already satisfied: matplotlib>=2.2 in /opt/conda/lib/python3.8/site-packages (from seaborn) (3.3.4)\n",
      "Requirement already satisfied: pytz>=2017.2 in /opt/conda/lib/python3.8/site-packages (from pandas>=0.23->seaborn) (2021.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.8/site-packages (from pandas>=0.23->seaborn) (2.8.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.8/site-packages (from matplotlib>=2.2->seaborn) (1.3.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.8/site-packages (from matplotlib>=2.2->seaborn) (8.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.8/site-packages (from matplotlib>=2.2->seaborn) (0.10.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in /opt/conda/lib/python3.8/site-packages (from matplotlib>=2.2->seaborn) (2.4.7)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.8/site-packages (from python-dateutil>=2.7.3->pandas>=0.23->seaborn) (1.15.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "addressed-persian",
   "metadata": {
    "gradient": {
     "editing": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/IPython/html.py:12: ShimWarning: The `IPython.html` package has been deprecated since IPython 4.0. You should import from `notebook` instead. `IPython.html.widgets` has moved to `ipywidgets`.\n",
      "  warn(\"The `IPython.html` package has been deprecated since IPython 4.0. \"\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "from IPython.display import display\n",
    "from IPython.html import widgets\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from torch import optim\n",
    "from torch.nn import functional as F\n",
    "from transformers import AdamW, AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "binary-atmosphere",
   "metadata": {
    "gradient": {
     "editing": false
    }
   },
   "outputs": [],
   "source": [
    "model_repo = 'Helsinki-NLP/opus-mt-en-hi'\n",
    "model_path = 'model/t5_translation.pt'\n",
    "max_seq_len = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "located-translator",
   "metadata": {
    "gradient": {
     "editing": false
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc7342c1e8a14676a64463c81ba953e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=44.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f6869f912084938b26d3005851cad43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=1145.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef6b40b111e64b2a8a882399e9ccd397",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=812240.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92bb3872461f4a0b9ba874b785b80735",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=1067935.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9afa8d66644042918e14e88fea57a02c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=2097836.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_repo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "pharmaceutical-carter",
   "metadata": {
    "gradient": {}
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "624ff06d57884ca7a8c0bc9039929b6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=305826357.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_repo)\n",
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "czech-possibility",
   "metadata": {
    "gradient": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 2843,  4955,  3520,   239,    46,    42,  6732,     7,  5875,    61,\n",
      "            17, 38203, 12944,    13,     0]], device='cuda:0')\n",
      "tensor([[61949,  2843,   232,  3520,    60,  9144,    18, 15943,    78,   368,\n",
      "            61,    17, 23583,     5,    13,     0]], device='cuda:0')\n",
      "<pad> <d> यह जर्मन को अनुवादित किया जाएगा! (आशा है)\n"
     ]
    }
   ],
   "source": [
    "token_ids = tokenizer.encode(\n",
    "    '<de> This will be translated to German! (hopefully)',\n",
    "    return_tensors='pt').cuda()\n",
    "print(token_ids)\n",
    "\n",
    "model_out = model.generate(token_ids)\n",
    "print(model_out)\n",
    "\n",
    "output_text = tokenizer.convert_tokens_to_string(\n",
    "    tokenizer.convert_ids_to_tokens(model_out[0]))\n",
    "print(output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "circular-convert",
   "metadata": {
    "gradient": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input IDs: tensor([[ 2843,  1411,  3520,  1687,    23,   469,    19,  2564,  6532, 17448,\n",
      "          7320,     3,     0]])\n",
      "Tokens: ['▁<', 'es', '>', 'This', '▁is', '▁just', '▁a', '▁test', '▁n', 'bu', 'ig', '.', '</s>']\n"
     ]
    }
   ],
   "source": [
    "example_input_str = '<es>This is just a test nbuig.'\n",
    "\n",
    "input_ids = tokenizer.encode(example_input_str, return_tensors='pt')\n",
    "print('Input IDs:', input_ids)\n",
    "\n",
    "tokens = tokenizer.convert_ids_to_tokens(input_ids[0])\n",
    "print('Tokens:', tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "hybrid-pollution",
   "metadata": {
    "gradient": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset opus100/en-hi (download: 34.33 MiB, generated: 120.06 MiB, post-processed: Unknown size, total: 154.39 MiB) to /root/.cache/huggingface/datasets/opus100/en-hi/0.0.0/a87abd612d82947c7a2c3991f71095a98f55141af7ad37516dfb31bfa3511ddc...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a1a008e9f2b4d7fbca98f195843a62d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=35993452.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4fa71c60c074d88ae403817831bc747",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8fbc81039d9a49549641548fa6dc1129",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e191122ce9740f8b334f730323ba25e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset opus100 downloaded and prepared to /root/.cache/huggingface/datasets/opus100/en-hi/0.0.0/a87abd612d82947c7a2c3991f71095a98f55141af7ad37516dfb31bfa3511ddc. Subsequent calls will reuse this data.\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "\n",
    "dataset = load_dataset(\"opus100\", \"en-hi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "rational-asian",
   "metadata": {
    "gradient": {}
   },
   "outputs": [],
   "source": [
    "train_dataset = dataset['train']\n",
    "test_dataset = dataset['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "comprehensive-million",
   "metadata": {
    "gradient": {}
   },
   "outputs": [],
   "source": [
    "LANG_TOKEN_MAPPING = {\n",
    "    'en': '<en>',\n",
    "    'hi': '<hi>',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "efficient-christianity",
   "metadata": {
    "gradient": {}
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(61952, 512)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "special_tokens_dict = {'additional_special_tokens': list(LANG_TOKEN_MAPPING.values())}\n",
    "tokenizer.add_special_tokens(special_tokens_dict)\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "certified-denmark",
   "metadata": {
    "gradient": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 2843,  1411,  3520,  1687,    23,   469,    19,  2564,  6532, 17448,\n",
      "          7320,     3,     0, 61949, 61949, 61949, 61949, 61949, 61949, 61949]])\n",
      "['▁<', 'es', '>', 'This', '▁is', '▁just', '▁a', '▁test', '▁n', 'bu', 'ig', '.', '</s>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n"
     ]
    }
   ],
   "source": [
    "token_ids = tokenizer.encode(\n",
    "    example_input_str, return_tensors='pt', padding='max_length',\n",
    "    truncation=True, max_length=max_seq_len)\n",
    "print(token_ids)\n",
    "\n",
    "tokens = tokenizer.convert_ids_to_tokens(token_ids[0])\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "injured-suffering",
   "metadata": {
    "gradient": {}
   },
   "outputs": [],
   "source": [
    "def encode_input_str(text, target_lang, tokenizer, seq_len,\n",
    "                     lang_token_map=LANG_TOKEN_MAPPING):\n",
    "  target_lang_token = lang_token_map[target_lang]\n",
    "\n",
    "  # Tokenize and add special tokens\n",
    "  input_ids = tokenizer.encode(\n",
    "      text = target_lang_token + text,\n",
    "      return_tensors = 'pt',\n",
    "      padding = 'max_length',\n",
    "      truncation = True,\n",
    "      max_length = seq_len)\n",
    "\n",
    "  return input_ids[0]\n",
    "  \n",
    "def encode_target_str(text, tokenizer, seq_len,\n",
    "                      lang_token_map=LANG_TOKEN_MAPPING):\n",
    "  token_ids = tokenizer.encode(\n",
    "      text = text,\n",
    "      return_tensors = 'pt',\n",
    "      padding = 'max_length',\n",
    "      truncation = True,\n",
    "      max_length = seq_len)\n",
    "  \n",
    "  return token_ids[0]\n",
    "\n",
    "def format_translation_data(translations, lang_token_map,\n",
    "                            tokenizer, seq_len=128):\n",
    "  # Choose a random 2 languages for in i/o\n",
    "  langs = list(lang_token_map.keys())\n",
    "  input_lang, target_lang = np.random.choice(langs, size=2, replace=False)\n",
    "\n",
    "  # Get the translations for the batch\n",
    "  input_text = translations[input_lang]\n",
    "  target_text = translations[target_lang]\n",
    "\n",
    "  if input_text is None or target_text is None:\n",
    "    return None\n",
    "\n",
    "  input_token_ids = encode_input_str(\n",
    "      input_text, target_lang, tokenizer, seq_len, lang_token_map)\n",
    "  \n",
    "  target_token_ids = encode_target_str(\n",
    "      target_text, tokenizer, seq_len, lang_token_map)\n",
    "\n",
    "  return input_token_ids, target_token_ids\n",
    "\n",
    "def transform_batch(batch, lang_token_map, tokenizer):\n",
    "  inputs = []\n",
    "  targets = []\n",
    "  for translation_set in batch['translation']:\n",
    "    formatted_data = format_translation_data(\n",
    "        translation_set, lang_token_map, tokenizer, max_seq_len)\n",
    "    \n",
    "    if formatted_data is None:\n",
    "      continue\n",
    "    \n",
    "    input_ids, target_ids = formatted_data\n",
    "    inputs.append(input_ids.unsqueeze(0))\n",
    "    targets.append(target_ids.unsqueeze(0))\n",
    "    \n",
    "  batch_input_ids = torch.cat(inputs).cuda()\n",
    "  batch_target_ids = torch.cat(targets).cuda()\n",
    "\n",
    "  return batch_input_ids, batch_target_ids\n",
    "\n",
    "def get_data_generator(dataset, lang_token_map, tokenizer, batch_size=32):\n",
    "  dataset = dataset.shuffle()\n",
    "  for i in range(0, len(dataset), batch_size):\n",
    "    raw_batch = dataset[i:i+batch_size]\n",
    "    yield transform_batch(raw_batch, lang_token_map, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "criminal-model",
   "metadata": {
    "gradient": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<hi> ▁Other , ▁Private ▁Use </s> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "▁ अ न ् य , ▁ न िज ़ ी ▁ उ प य ो ग </s> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "Input shape: torch.Size([8, 20])\n",
      "Output shape: torch.Size([8, 20])\n"
     ]
    }
   ],
   "source": [
    "# Testing `data_transform`\n",
    "in_ids, out_ids = format_translation_data(\n",
    "    train_dataset[0]['translation'], LANG_TOKEN_MAPPING, tokenizer)\n",
    "\n",
    "print(' '.join(tokenizer.convert_ids_to_tokens(in_ids)))\n",
    "print(' '.join(tokenizer.convert_ids_to_tokens(out_ids)))\n",
    "\n",
    "# Testing data generator\n",
    "data_gen = get_data_generator(train_dataset, LANG_TOKEN_MAPPING, tokenizer, 8)\n",
    "data_batch = next(data_gen)\n",
    "print('Input shape:', data_batch[0].shape)\n",
    "print('Output shape:', data_batch[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "favorite-pantyhose",
   "metadata": {
    "gradient": {}
   },
   "outputs": [],
   "source": [
    "n_epochs = 1\n",
    "batch_size = 16\n",
    "print_freq = 50\n",
    "checkpoint_freq = 1000\n",
    "lr = 5e-4\n",
    "n_batches = int(np.ceil(len(train_dataset) / batch_size))\n",
    "total_steps = n_epochs * n_batches\n",
    "n_warmup_steps = int(total_steps * 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "clear-syndication",
   "metadata": {
    "gradient": {}
   },
   "outputs": [],
   "source": [
    "optimizer = AdamW(model.parameters(), lr=lr)\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer, n_warmup_steps, total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "saving-convergence",
   "metadata": {
    "gradient": {}
   },
   "outputs": [],
   "source": [
    "losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "tropical-butler",
   "metadata": {
    "gradient": {}
   },
   "outputs": [],
   "source": [
    "def eval_model(model, gdataset, max_iters=8):\n",
    "  test_generator = get_data_generator(gdataset, LANG_TOKEN_MAPPING,\n",
    "                                      tokenizer, batch_size)\n",
    "  eval_losses = []\n",
    "  for i, (input_batch, label_batch) in enumerate(test_generator):\n",
    "    if i >= max_iters:\n",
    "      break\n",
    "\n",
    "    model_out = model.forward(\n",
    "        input_ids = input_batch,\n",
    "        labels = label_batch)\n",
    "    eval_losses.append(model_out.loss.item())\n",
    "\n",
    "  return np.mean(eval_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "accredited-amber",
   "metadata": {
    "gradient": {}
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-22-7d82fc3c8b16>:7: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  in tqdm_notebook(enumerate(data_generator), total=n_batches):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "682230c54ae34544827a1d0a03b6b928",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=33395.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | Step: 50 | Avg. loss: 5.223 | lr: 7.507507507507507e-05\n",
      "Epoch: 1 | Step: 100 | Avg. loss: 3.268 | lr: 0.00015015015015015014\n",
      "Epoch: 1 | Step: 150 | Avg. loss: 2.883 | lr: 0.00022522522522522523\n",
      "Epoch: 1 | Step: 200 | Avg. loss: 2.696 | lr: 0.0003003003003003003\n",
      "Epoch: 1 | Step: 250 | Avg. loss: 2.586 | lr: 0.00037537537537537537\n",
      "Epoch: 1 | Step: 300 | Avg. loss: 2.575 | lr: 0.00045045045045045046\n",
      "Epoch: 1 | Step: 350 | Avg. loss: 2.606 | lr: 0.0004997429072651382\n",
      "Epoch: 1 | Step: 400 | Avg. loss: 2.600 | lr: 0.0004989867521626036\n",
      "Epoch: 1 | Step: 450 | Avg. loss: 2.379 | lr: 0.0004982305970600689\n",
      "Epoch: 1 | Step: 500 | Avg. loss: 2.413 | lr: 0.0004974744419575343\n",
      "Epoch: 1 | Step: 550 | Avg. loss: 2.427 | lr: 0.0004967182868549997\n",
      "Epoch: 1 | Step: 600 | Avg. loss: 2.299 | lr: 0.000495962131752465\n",
      "Epoch: 1 | Step: 650 | Avg. loss: 2.321 | lr: 0.0004952059766499305\n",
      "Epoch: 1 | Step: 700 | Avg. loss: 2.318 | lr: 0.0004944498215473959\n",
      "Epoch: 1 | Step: 750 | Avg. loss: 2.283 | lr: 0.0004936936664448612\n",
      "Epoch: 1 | Step: 800 | Avg. loss: 2.315 | lr: 0.0004929375113423266\n",
      "Epoch: 1 | Step: 850 | Avg. loss: 2.222 | lr: 0.000492181356239792\n",
      "Epoch: 1 | Step: 900 | Avg. loss: 2.172 | lr: 0.0004914252011372573\n",
      "Epoch: 1 | Step: 950 | Avg. loss: 2.229 | lr: 0.0004906690460347227\n",
      "Epoch: 1 | Step: 1000 | Avg. loss: 2.178 | lr: 0.000489912890932188\n",
      "Saving model with test loss of 2.461\n",
      "Epoch: 1 | Step: 1050 | Avg. loss: 2.142 | lr: 0.0004891567358296533\n",
      "Epoch: 1 | Step: 1100 | Avg. loss: 2.220 | lr: 0.0004884005807271188\n",
      "Epoch: 1 | Step: 1150 | Avg. loss: 2.239 | lr: 0.0004876444256245841\n",
      "Epoch: 1 | Step: 1200 | Avg. loss: 2.148 | lr: 0.00048688827052204947\n",
      "Epoch: 1 | Step: 1250 | Avg. loss: 2.326 | lr: 0.00048613211541951483\n",
      "Epoch: 1 | Step: 1300 | Avg. loss: 2.246 | lr: 0.0004853759603169802\n",
      "Epoch: 1 | Step: 1350 | Avg. loss: 2.250 | lr: 0.0004846198052144456\n",
      "Epoch: 1 | Step: 1400 | Avg. loss: 2.235 | lr: 0.000483863650111911\n",
      "Epoch: 1 | Step: 1450 | Avg. loss: 2.179 | lr: 0.00048310749500937635\n",
      "Epoch: 1 | Step: 1500 | Avg. loss: 2.211 | lr: 0.0004823513399068417\n",
      "Epoch: 1 | Step: 1550 | Avg. loss: 2.210 | lr: 0.0004815951848043071\n",
      "Epoch: 1 | Step: 1600 | Avg. loss: 2.128 | lr: 0.00048083902970177245\n",
      "Epoch: 1 | Step: 1650 | Avg. loss: 2.124 | lr: 0.0004800828745992378\n",
      "Epoch: 1 | Step: 1700 | Avg. loss: 2.148 | lr: 0.0004793267194967032\n",
      "Epoch: 1 | Step: 1750 | Avg. loss: 2.222 | lr: 0.0004785705643941685\n",
      "Epoch: 1 | Step: 1800 | Avg. loss: 2.193 | lr: 0.0004778144092916339\n",
      "Epoch: 1 | Step: 1850 | Avg. loss: 2.161 | lr: 0.0004770582541890993\n",
      "Epoch: 1 | Step: 1900 | Avg. loss: 2.090 | lr: 0.00047630209908656464\n",
      "Epoch: 1 | Step: 1950 | Avg. loss: 2.099 | lr: 0.00047554594398403\n",
      "Epoch: 1 | Step: 2000 | Avg. loss: 2.150 | lr: 0.0004747897888814954\n",
      "Saving model with test loss of 2.316\n",
      "Epoch: 1 | Step: 2050 | Avg. loss: 2.156 | lr: 0.00047403363377896074\n",
      "Epoch: 1 | Step: 2100 | Avg. loss: 2.182 | lr: 0.0004732774786764261\n",
      "Epoch: 1 | Step: 2150 | Avg. loss: 2.092 | lr: 0.0004725213235738915\n",
      "Epoch: 1 | Step: 2200 | Avg. loss: 2.160 | lr: 0.00047176516847135684\n",
      "Epoch: 1 | Step: 2250 | Avg. loss: 2.073 | lr: 0.00047100901336882226\n",
      "Epoch: 1 | Step: 2300 | Avg. loss: 2.097 | lr: 0.0004702528582662876\n",
      "Epoch: 1 | Step: 2350 | Avg. loss: 2.064 | lr: 0.000469496703163753\n",
      "Epoch: 1 | Step: 2400 | Avg. loss: 2.135 | lr: 0.00046874054806121836\n",
      "Epoch: 1 | Step: 2450 | Avg. loss: 2.148 | lr: 0.00046798439295868367\n",
      "Epoch: 1 | Step: 2500 | Avg. loss: 2.244 | lr: 0.00046722823785614904\n",
      "Epoch: 1 | Step: 2550 | Avg. loss: 2.345 | lr: 0.0004664720827536144\n",
      "Epoch: 1 | Step: 2600 | Avg. loss: 2.359 | lr: 0.00046571592765107977\n",
      "Epoch: 1 | Step: 2650 | Avg. loss: 2.219 | lr: 0.00046495977254854513\n",
      "Epoch: 1 | Step: 2700 | Avg. loss: 2.267 | lr: 0.00046420361744601055\n",
      "Epoch: 1 | Step: 2750 | Avg. loss: 2.171 | lr: 0.0004634474623434759\n",
      "Epoch: 1 | Step: 2800 | Avg. loss: 2.268 | lr: 0.0004626913072409413\n",
      "Epoch: 1 | Step: 2850 | Avg. loss: 2.141 | lr: 0.00046193515213840665\n",
      "Epoch: 1 | Step: 2900 | Avg. loss: 2.239 | lr: 0.000461178997035872\n",
      "Epoch: 1 | Step: 2950 | Avg. loss: 2.299 | lr: 0.0004604228419333374\n",
      "Epoch: 1 | Step: 3000 | Avg. loss: 2.238 | lr: 0.00045966668683080275\n",
      "Saving model with test loss of 2.243\n",
      "Epoch: 1 | Step: 3050 | Avg. loss: 2.158 | lr: 0.0004589105317282681\n",
      "Epoch: 1 | Step: 3100 | Avg. loss: 2.176 | lr: 0.0004581543766257335\n",
      "Epoch: 1 | Step: 3150 | Avg. loss: 2.220 | lr: 0.00045739822152319885\n",
      "Epoch: 1 | Step: 3200 | Avg. loss: 2.194 | lr: 0.0004566420664206642\n",
      "Epoch: 1 | Step: 3250 | Avg. loss: 2.198 | lr: 0.0004558859113181296\n",
      "Epoch: 1 | Step: 3300 | Avg. loss: 2.204 | lr: 0.00045512975621559494\n",
      "Epoch: 1 | Step: 3350 | Avg. loss: 2.253 | lr: 0.0004543736011130603\n",
      "Epoch: 1 | Step: 3400 | Avg. loss: 2.213 | lr: 0.0004536174460105257\n",
      "Epoch: 1 | Step: 3450 | Avg. loss: 2.076 | lr: 0.00045286129090799104\n",
      "Epoch: 1 | Step: 3500 | Avg. loss: 2.167 | lr: 0.0004521051358054564\n",
      "Epoch: 1 | Step: 3550 | Avg. loss: 2.076 | lr: 0.0004513489807029218\n",
      "Epoch: 1 | Step: 3600 | Avg. loss: 2.206 | lr: 0.0004505928256003872\n",
      "Epoch: 1 | Step: 3650 | Avg. loss: 2.205 | lr: 0.00044983667049785256\n",
      "Epoch: 1 | Step: 3700 | Avg. loss: 2.175 | lr: 0.0004490805153953179\n",
      "Epoch: 1 | Step: 3750 | Avg. loss: 2.204 | lr: 0.0004483243602927833\n",
      "Epoch: 1 | Step: 3800 | Avg. loss: 2.120 | lr: 0.00044756820519024866\n",
      "Epoch: 1 | Step: 3850 | Avg. loss: 2.177 | lr: 0.000446812050087714\n",
      "Epoch: 1 | Step: 3900 | Avg. loss: 2.160 | lr: 0.00044605589498517933\n",
      "Epoch: 1 | Step: 3950 | Avg. loss: 2.094 | lr: 0.0004452997398826447\n",
      "Epoch: 1 | Step: 4000 | Avg. loss: 2.116 | lr: 0.00044454358478011007\n",
      "Saving model with test loss of 2.292\n",
      "Epoch: 1 | Step: 4050 | Avg. loss: 2.069 | lr: 0.0004437874296775755\n",
      "Epoch: 1 | Step: 4100 | Avg. loss: 2.135 | lr: 0.00044303127457504085\n",
      "Epoch: 1 | Step: 4150 | Avg. loss: 2.077 | lr: 0.0004422751194725062\n",
      "Epoch: 1 | Step: 4200 | Avg. loss: 2.109 | lr: 0.0004415189643699716\n",
      "Epoch: 1 | Step: 4250 | Avg. loss: 2.248 | lr: 0.00044076280926743695\n",
      "Epoch: 1 | Step: 4300 | Avg. loss: 2.100 | lr: 0.0004400066541649023\n",
      "Epoch: 1 | Step: 4350 | Avg. loss: 2.129 | lr: 0.0004392504990623677\n",
      "Epoch: 1 | Step: 4400 | Avg. loss: 2.130 | lr: 0.00043849434395983305\n",
      "Epoch: 1 | Step: 4450 | Avg. loss: 2.131 | lr: 0.0004377381888572984\n",
      "Epoch: 1 | Step: 4500 | Avg. loss: 2.139 | lr: 0.00043698203375476383\n",
      "Epoch: 1 | Step: 4550 | Avg. loss: 2.146 | lr: 0.0004362258786522292\n",
      "Epoch: 1 | Step: 4600 | Avg. loss: 2.102 | lr: 0.0004354697235496945\n",
      "Epoch: 1 | Step: 4650 | Avg. loss: 2.037 | lr: 0.0004347135684471599\n",
      "Epoch: 1 | Step: 4700 | Avg. loss: 2.105 | lr: 0.00043395741334462524\n",
      "Epoch: 1 | Step: 4750 | Avg. loss: 2.077 | lr: 0.0004332012582420906\n",
      "Epoch: 1 | Step: 4800 | Avg. loss: 2.127 | lr: 0.000432445103139556\n",
      "Epoch: 1 | Step: 4850 | Avg. loss: 2.152 | lr: 0.00043168894803702134\n",
      "Epoch: 1 | Step: 4900 | Avg. loss: 2.081 | lr: 0.0004309327929344867\n",
      "Epoch: 1 | Step: 4950 | Avg. loss: 2.161 | lr: 0.00043017663783195213\n",
      "Epoch: 1 | Step: 5000 | Avg. loss: 2.053 | lr: 0.0004294204827294175\n",
      "Saving model with test loss of 2.267\n",
      "Epoch: 1 | Step: 5050 | Avg. loss: 2.085 | lr: 0.00042866432762688286\n",
      "Epoch: 1 | Step: 5100 | Avg. loss: 2.105 | lr: 0.0004279081725243482\n",
      "Epoch: 1 | Step: 5150 | Avg. loss: 2.025 | lr: 0.0004271520174218136\n",
      "Epoch: 1 | Step: 5200 | Avg. loss: 2.167 | lr: 0.00042639586231927896\n",
      "Epoch: 1 | Step: 5250 | Avg. loss: 2.029 | lr: 0.0004256397072167443\n",
      "Epoch: 1 | Step: 5300 | Avg. loss: 2.192 | lr: 0.00042488355211420963\n",
      "Epoch: 1 | Step: 5350 | Avg. loss: 2.124 | lr: 0.000424127397011675\n",
      "Epoch: 1 | Step: 5400 | Avg. loss: 2.089 | lr: 0.00042337124190914037\n",
      "Epoch: 1 | Step: 5450 | Avg. loss: 2.176 | lr: 0.0004226150868066058\n",
      "Epoch: 1 | Step: 5500 | Avg. loss: 2.109 | lr: 0.00042185893170407115\n",
      "Epoch: 1 | Step: 5550 | Avg. loss: 2.097 | lr: 0.0004211027766015365\n",
      "Epoch: 1 | Step: 5600 | Avg. loss: 2.079 | lr: 0.0004203466214990019\n",
      "Epoch: 1 | Step: 5650 | Avg. loss: 2.089 | lr: 0.00041959046639646725\n",
      "Epoch: 1 | Step: 5700 | Avg. loss: 2.086 | lr: 0.0004188343112939326\n",
      "Epoch: 1 | Step: 5750 | Avg. loss: 2.066 | lr: 0.000418078156191398\n",
      "Epoch: 1 | Step: 5800 | Avg. loss: 2.109 | lr: 0.00041732200108886335\n",
      "Epoch: 1 | Step: 5850 | Avg. loss: 2.090 | lr: 0.0004165658459863287\n",
      "Epoch: 1 | Step: 5900 | Avg. loss: 2.152 | lr: 0.00041580969088379413\n",
      "Epoch: 1 | Step: 5950 | Avg. loss: 2.130 | lr: 0.0004150535357812595\n",
      "Epoch: 1 | Step: 6000 | Avg. loss: 1.970 | lr: 0.00041429738067872487\n",
      "Saving model with test loss of 2.190\n",
      "Epoch: 1 | Step: 6050 | Avg. loss: 1.984 | lr: 0.0004135412255761902\n",
      "Epoch: 1 | Step: 6100 | Avg. loss: 2.000 | lr: 0.00041278507047365554\n",
      "Epoch: 1 | Step: 6150 | Avg. loss: 2.161 | lr: 0.0004120289153711209\n",
      "Epoch: 1 | Step: 6200 | Avg. loss: 2.031 | lr: 0.0004112727602685863\n",
      "Epoch: 1 | Step: 6250 | Avg. loss: 2.069 | lr: 0.00041051660516605164\n",
      "Epoch: 1 | Step: 6300 | Avg. loss: 1.992 | lr: 0.000409760450063517\n",
      "Epoch: 1 | Step: 6350 | Avg. loss: 1.987 | lr: 0.0004090042949609824\n",
      "Epoch: 1 | Step: 6400 | Avg. loss: 2.058 | lr: 0.0004082481398584478\n",
      "Epoch: 1 | Step: 6450 | Avg. loss: 2.044 | lr: 0.00040749198475591316\n",
      "Epoch: 1 | Step: 6500 | Avg. loss: 2.067 | lr: 0.0004067358296533785\n",
      "Epoch: 1 | Step: 6550 | Avg. loss: 2.101 | lr: 0.0004059796745508439\n",
      "Epoch: 1 | Step: 6600 | Avg. loss: 2.048 | lr: 0.00040522351944830926\n",
      "Epoch: 1 | Step: 6650 | Avg. loss: 2.141 | lr: 0.0004044673643457746\n",
      "Epoch: 1 | Step: 6700 | Avg. loss: 2.035 | lr: 0.00040371120924324\n",
      "Epoch: 1 | Step: 6750 | Avg. loss: 2.129 | lr: 0.0004029550541407053\n",
      "Epoch: 1 | Step: 6800 | Avg. loss: 2.180 | lr: 0.0004021988990381707\n",
      "Epoch: 1 | Step: 6850 | Avg. loss: 2.051 | lr: 0.0004014427439356361\n",
      "Epoch: 1 | Step: 6900 | Avg. loss: 2.087 | lr: 0.00040068658883310145\n",
      "Epoch: 1 | Step: 6950 | Avg. loss: 2.058 | lr: 0.0003999304337305668\n",
      "Epoch: 1 | Step: 7000 | Avg. loss: 2.059 | lr: 0.0003991742786280322\n",
      "Saving model with test loss of 2.227\n",
      "Epoch: 1 | Step: 7050 | Avg. loss: 2.025 | lr: 0.00039841812352549755\n",
      "Epoch: 1 | Step: 7100 | Avg. loss: 2.098 | lr: 0.0003976619684229629\n",
      "Epoch: 1 | Step: 7150 | Avg. loss: 2.070 | lr: 0.0003969058133204283\n",
      "Epoch: 1 | Step: 7200 | Avg. loss: 2.066 | lr: 0.00039614965821789365\n",
      "Epoch: 1 | Step: 7250 | Avg. loss: 2.087 | lr: 0.00039539350311535907\n",
      "Epoch: 1 | Step: 7300 | Avg. loss: 1.998 | lr: 0.00039463734801282443\n",
      "Epoch: 1 | Step: 7350 | Avg. loss: 2.083 | lr: 0.0003938811929102898\n",
      "Epoch: 1 | Step: 7400 | Avg. loss: 2.072 | lr: 0.00039312503780775516\n",
      "Epoch: 1 | Step: 7450 | Avg. loss: 2.081 | lr: 0.0003923688827052205\n",
      "Epoch: 1 | Step: 7500 | Avg. loss: 2.104 | lr: 0.00039161272760268584\n",
      "Epoch: 1 | Step: 7550 | Avg. loss: 2.133 | lr: 0.0003908565725001512\n",
      "Epoch: 1 | Step: 7600 | Avg. loss: 2.098 | lr: 0.0003901004173976166\n",
      "Epoch: 1 | Step: 7650 | Avg. loss: 2.055 | lr: 0.00038934426229508194\n",
      "Epoch: 1 | Step: 7700 | Avg. loss: 2.002 | lr: 0.00038858810719254736\n",
      "Epoch: 1 | Step: 7750 | Avg. loss: 2.021 | lr: 0.0003878319520900127\n",
      "Epoch: 1 | Step: 7800 | Avg. loss: 1.930 | lr: 0.0003870757969874781\n",
      "Epoch: 1 | Step: 7850 | Avg. loss: 2.109 | lr: 0.00038631964188494346\n",
      "Epoch: 1 | Step: 7900 | Avg. loss: 2.016 | lr: 0.0003855634867824088\n",
      "Epoch: 1 | Step: 7950 | Avg. loss: 2.007 | lr: 0.0003848073316798742\n",
      "Epoch: 1 | Step: 8000 | Avg. loss: 2.012 | lr: 0.00038405117657733956\n",
      "Saving model with test loss of 2.497\n",
      "Epoch: 1 | Step: 8050 | Avg. loss: 2.074 | lr: 0.0003832950214748049\n",
      "Epoch: 1 | Step: 8100 | Avg. loss: 2.111 | lr: 0.0003825388663722703\n",
      "Epoch: 1 | Step: 8150 | Avg. loss: 2.042 | lr: 0.0003817827112697357\n",
      "Epoch: 1 | Step: 8200 | Avg. loss: 2.079 | lr: 0.000381026556167201\n",
      "Epoch: 1 | Step: 8250 | Avg. loss: 2.100 | lr: 0.0003802704010646664\n",
      "Epoch: 1 | Step: 8300 | Avg. loss: 2.069 | lr: 0.00037951424596213175\n",
      "Epoch: 1 | Step: 8350 | Avg. loss: 2.076 | lr: 0.0003787580908595971\n",
      "Epoch: 1 | Step: 8400 | Avg. loss: 2.185 | lr: 0.0003780019357570625\n",
      "Epoch: 1 | Step: 8450 | Avg. loss: 2.172 | lr: 0.00037724578065452785\n",
      "Epoch: 1 | Step: 8500 | Avg. loss: 2.081 | lr: 0.0003764896255519932\n",
      "Epoch: 1 | Step: 8550 | Avg. loss: 2.125 | lr: 0.0003757334704494586\n",
      "Epoch: 1 | Step: 8600 | Avg. loss: 2.072 | lr: 0.000374977315346924\n",
      "Epoch: 1 | Step: 8650 | Avg. loss: 2.123 | lr: 0.00037422116024438937\n",
      "Epoch: 1 | Step: 8700 | Avg. loss: 2.072 | lr: 0.00037346500514185473\n",
      "Epoch: 1 | Step: 8750 | Avg. loss: 2.060 | lr: 0.0003727088500393201\n",
      "Epoch: 1 | Step: 8800 | Avg. loss: 2.075 | lr: 0.00037195269493678546\n",
      "Epoch: 1 | Step: 8850 | Avg. loss: 2.024 | lr: 0.00037119653983425083\n",
      "Epoch: 1 | Step: 8900 | Avg. loss: 2.057 | lr: 0.00037044038473171614\n",
      "Epoch: 1 | Step: 8950 | Avg. loss: 2.317 | lr: 0.0003696842296291815\n",
      "Epoch: 1 | Step: 9000 | Avg. loss: 2.084 | lr: 0.0003689280745266469\n",
      "Saving model with test loss of 2.259\n",
      "Epoch: 1 | Step: 9050 | Avg. loss: 2.010 | lr: 0.0003681719194241123\n",
      "Epoch: 1 | Step: 9100 | Avg. loss: 2.096 | lr: 0.00036741576432157766\n",
      "Epoch: 1 | Step: 9150 | Avg. loss: 2.069 | lr: 0.000366659609219043\n",
      "Epoch: 1 | Step: 9200 | Avg. loss: 2.125 | lr: 0.0003659034541165084\n",
      "Epoch: 1 | Step: 9250 | Avg. loss: 2.024 | lr: 0.00036514729901397376\n",
      "Epoch: 1 | Step: 9300 | Avg. loss: 2.073 | lr: 0.0003643911439114391\n",
      "Epoch: 1 | Step: 9350 | Avg. loss: 2.036 | lr: 0.0003636349888089045\n",
      "Epoch: 1 | Step: 9400 | Avg. loss: 1.996 | lr: 0.00036287883370636985\n",
      "Epoch: 1 | Step: 9450 | Avg. loss: 2.063 | lr: 0.0003621226786038352\n",
      "Epoch: 1 | Step: 9500 | Avg. loss: 1.974 | lr: 0.00036136652350130064\n",
      "Epoch: 1 | Step: 9550 | Avg. loss: 2.032 | lr: 0.000360610368398766\n",
      "Epoch: 1 | Step: 9600 | Avg. loss: 2.021 | lr: 0.0003598542132962313\n",
      "Epoch: 1 | Step: 9650 | Avg. loss: 1.978 | lr: 0.0003590980581936967\n",
      "Epoch: 1 | Step: 9700 | Avg. loss: 1.988 | lr: 0.00035834190309116205\n",
      "Epoch: 1 | Step: 9750 | Avg. loss: 2.039 | lr: 0.0003575857479886274\n",
      "Epoch: 1 | Step: 9800 | Avg. loss: 2.003 | lr: 0.0003568295928860928\n",
      "Epoch: 1 | Step: 9850 | Avg. loss: 1.954 | lr: 0.00035607343778355815\n",
      "Epoch: 1 | Step: 9900 | Avg. loss: 1.987 | lr: 0.0003553172826810235\n",
      "Epoch: 1 | Step: 9950 | Avg. loss: 2.054 | lr: 0.00035456112757848893\n",
      "Epoch: 1 | Step: 10000 | Avg. loss: 1.981 | lr: 0.0003538049724759543\n",
      "Saving model with test loss of 2.241\n",
      "Epoch: 1 | Step: 10050 | Avg. loss: 1.933 | lr: 0.00035304881737341967\n",
      "Epoch: 1 | Step: 10100 | Avg. loss: 2.026 | lr: 0.00035229266227088503\n",
      "Epoch: 1 | Step: 10150 | Avg. loss: 1.985 | lr: 0.0003515365071683504\n",
      "Epoch: 1 | Step: 10200 | Avg. loss: 1.994 | lr: 0.00035078035206581576\n",
      "Epoch: 1 | Step: 10250 | Avg. loss: 2.032 | lr: 0.00035002419696328113\n",
      "Epoch: 1 | Step: 10300 | Avg. loss: 1.956 | lr: 0.0003492680418607465\n",
      "Epoch: 1 | Step: 10350 | Avg. loss: 1.901 | lr: 0.0003485118867582118\n",
      "Epoch: 1 | Step: 10400 | Avg. loss: 1.943 | lr: 0.00034775573165567717\n",
      "Epoch: 1 | Step: 10450 | Avg. loss: 2.005 | lr: 0.0003469995765531426\n",
      "Epoch: 1 | Step: 10500 | Avg. loss: 1.945 | lr: 0.00034624342145060796\n",
      "Epoch: 1 | Step: 10550 | Avg. loss: 2.030 | lr: 0.0003454872663480733\n",
      "Epoch: 1 | Step: 10600 | Avg. loss: 2.046 | lr: 0.0003447311112455387\n",
      "Epoch: 1 | Step: 10650 | Avg. loss: 1.949 | lr: 0.00034397495614300406\n",
      "Epoch: 1 | Step: 10700 | Avg. loss: 2.036 | lr: 0.0003432188010404694\n",
      "Epoch: 1 | Step: 10750 | Avg. loss: 1.985 | lr: 0.0003424626459379348\n",
      "Epoch: 1 | Step: 10800 | Avg. loss: 1.905 | lr: 0.00034170649083540015\n",
      "Epoch: 1 | Step: 10850 | Avg. loss: 1.929 | lr: 0.0003409503357328655\n",
      "Epoch: 1 | Step: 10900 | Avg. loss: 1.902 | lr: 0.00034019418063033094\n",
      "Epoch: 1 | Step: 10950 | Avg. loss: 1.995 | lr: 0.0003394380255277963\n",
      "Epoch: 1 | Step: 11000 | Avg. loss: 2.039 | lr: 0.00033868187042526167\n",
      "Saving model with test loss of 2.121\n",
      "Epoch: 1 | Step: 11050 | Avg. loss: 1.929 | lr: 0.000337925715322727\n",
      "Epoch: 1 | Step: 11100 | Avg. loss: 1.999 | lr: 0.00033716956022019235\n",
      "Epoch: 1 | Step: 11150 | Avg. loss: 1.964 | lr: 0.0003364134051176577\n",
      "Epoch: 1 | Step: 11200 | Avg. loss: 1.950 | lr: 0.0003356572500151231\n",
      "Epoch: 1 | Step: 11250 | Avg. loss: 1.926 | lr: 0.00033490109491258845\n",
      "Epoch: 1 | Step: 11300 | Avg. loss: 1.926 | lr: 0.0003341449398100538\n",
      "Epoch: 1 | Step: 11350 | Avg. loss: 1.927 | lr: 0.00033338878470751923\n",
      "Epoch: 1 | Step: 11400 | Avg. loss: 1.909 | lr: 0.0003326326296049846\n",
      "Epoch: 1 | Step: 11450 | Avg. loss: 1.906 | lr: 0.00033187647450244996\n",
      "Epoch: 1 | Step: 11500 | Avg. loss: 2.053 | lr: 0.00033112031939991533\n",
      "Epoch: 1 | Step: 11550 | Avg. loss: 1.905 | lr: 0.0003303641642973807\n",
      "Epoch: 1 | Step: 11600 | Avg. loss: 1.972 | lr: 0.00032960800919484606\n",
      "Epoch: 1 | Step: 11650 | Avg. loss: 1.945 | lr: 0.00032885185409231143\n",
      "Epoch: 1 | Step: 11700 | Avg. loss: 1.924 | lr: 0.0003280956989897768\n",
      "Epoch: 1 | Step: 11750 | Avg. loss: 2.039 | lr: 0.0003273395438872421\n",
      "Epoch: 1 | Step: 11800 | Avg. loss: 1.930 | lr: 0.0003265833887847075\n",
      "Epoch: 1 | Step: 11850 | Avg. loss: 1.942 | lr: 0.0003258272336821729\n",
      "Epoch: 1 | Step: 11900 | Avg. loss: 1.947 | lr: 0.00032507107857963826\n",
      "Epoch: 1 | Step: 11950 | Avg. loss: 1.954 | lr: 0.0003243149234771036\n",
      "Epoch: 1 | Step: 12000 | Avg. loss: 1.965 | lr: 0.000323558768374569\n",
      "Saving model with test loss of 2.122\n",
      "Epoch: 1 | Step: 12050 | Avg. loss: 1.868 | lr: 0.00032280261327203436\n",
      "Epoch: 1 | Step: 12100 | Avg. loss: 1.956 | lr: 0.0003220464581694997\n",
      "Epoch: 1 | Step: 12150 | Avg. loss: 2.064 | lr: 0.0003212903030669651\n",
      "Epoch: 1 | Step: 12200 | Avg. loss: 2.009 | lr: 0.00032053414796443045\n",
      "Epoch: 1 | Step: 12250 | Avg. loss: 1.938 | lr: 0.0003197779928618959\n",
      "Epoch: 1 | Step: 12300 | Avg. loss: 1.962 | lr: 0.00031902183775936124\n",
      "Epoch: 1 | Step: 12350 | Avg. loss: 1.892 | lr: 0.0003182656826568266\n",
      "Epoch: 1 | Step: 12400 | Avg. loss: 1.935 | lr: 0.00031750952755429197\n",
      "Epoch: 1 | Step: 12450 | Avg. loss: 1.910 | lr: 0.00031675337245175734\n",
      "Epoch: 1 | Step: 12500 | Avg. loss: 1.916 | lr: 0.00031599721734922265\n",
      "Epoch: 1 | Step: 12550 | Avg. loss: 1.894 | lr: 0.000315241062246688\n",
      "Epoch: 1 | Step: 12600 | Avg. loss: 1.905 | lr: 0.0003144849071441534\n",
      "Epoch: 1 | Step: 12650 | Avg. loss: 1.936 | lr: 0.00031372875204161875\n",
      "Epoch: 1 | Step: 12700 | Avg. loss: 1.981 | lr: 0.00031297259693908417\n",
      "Epoch: 1 | Step: 12750 | Avg. loss: 1.949 | lr: 0.00031221644183654953\n",
      "Epoch: 1 | Step: 12800 | Avg. loss: 1.983 | lr: 0.0003114602867340149\n",
      "Epoch: 1 | Step: 12850 | Avg. loss: 1.948 | lr: 0.00031070413163148026\n",
      "Epoch: 1 | Step: 12900 | Avg. loss: 1.988 | lr: 0.00030994797652894563\n",
      "Epoch: 1 | Step: 12950 | Avg. loss: 1.971 | lr: 0.000309191821426411\n",
      "Epoch: 1 | Step: 13000 | Avg. loss: 1.983 | lr: 0.00030843566632387636\n",
      "Saving model with test loss of 2.115\n",
      "Epoch: 1 | Step: 13050 | Avg. loss: 1.948 | lr: 0.00030767951122134173\n",
      "Epoch: 1 | Step: 13100 | Avg. loss: 1.905 | lr: 0.0003069233561188071\n",
      "Epoch: 1 | Step: 13150 | Avg. loss: 1.931 | lr: 0.0003061672010162725\n",
      "Epoch: 1 | Step: 13200 | Avg. loss: 1.882 | lr: 0.0003054110459137378\n",
      "Epoch: 1 | Step: 13250 | Avg. loss: 1.999 | lr: 0.0003046548908112032\n",
      "Epoch: 1 | Step: 13300 | Avg. loss: 2.002 | lr: 0.00030389873570866856\n",
      "Epoch: 1 | Step: 13350 | Avg. loss: 1.943 | lr: 0.0003031425806061339\n",
      "Epoch: 1 | Step: 13400 | Avg. loss: 1.979 | lr: 0.0003023864255035993\n",
      "Epoch: 1 | Step: 13450 | Avg. loss: 1.794 | lr: 0.00030163027040106465\n",
      "Epoch: 1 | Step: 13500 | Avg. loss: 1.941 | lr: 0.00030087411529853\n",
      "Epoch: 1 | Step: 13550 | Avg. loss: 1.923 | lr: 0.0003001179601959954\n",
      "Epoch: 1 | Step: 13600 | Avg. loss: 1.940 | lr: 0.0002993618050934608\n",
      "Epoch: 1 | Step: 13650 | Avg. loss: 1.983 | lr: 0.00029860564999092617\n",
      "Epoch: 1 | Step: 13700 | Avg. loss: 1.945 | lr: 0.00029784949488839154\n",
      "Epoch: 1 | Step: 13750 | Avg. loss: 1.823 | lr: 0.0002970933397858569\n",
      "Epoch: 1 | Step: 13800 | Avg. loss: 1.870 | lr: 0.00029633718468332227\n",
      "Epoch: 1 | Step: 13850 | Avg. loss: 1.975 | lr: 0.00029558102958078764\n",
      "Epoch: 1 | Step: 13900 | Avg. loss: 1.929 | lr: 0.00029482487447825295\n",
      "Epoch: 1 | Step: 13950 | Avg. loss: 1.868 | lr: 0.0002940687193757183\n",
      "Epoch: 1 | Step: 14000 | Avg. loss: 1.895 | lr: 0.0002933125642731837\n",
      "Saving model with test loss of 2.289\n",
      "Epoch: 1 | Step: 14050 | Avg. loss: 1.928 | lr: 0.0002925564091706491\n",
      "Epoch: 1 | Step: 14100 | Avg. loss: 1.870 | lr: 0.00029180025406811447\n",
      "Epoch: 1 | Step: 14150 | Avg. loss: 1.905 | lr: 0.00029104409896557983\n",
      "Epoch: 1 | Step: 14200 | Avg. loss: 1.922 | lr: 0.0002902879438630452\n",
      "Epoch: 1 | Step: 14250 | Avg. loss: 1.848 | lr: 0.00028953178876051056\n",
      "Epoch: 1 | Step: 14300 | Avg. loss: 1.919 | lr: 0.00028877563365797593\n",
      "Epoch: 1 | Step: 14350 | Avg. loss: 1.869 | lr: 0.0002880194785554413\n",
      "Epoch: 1 | Step: 14400 | Avg. loss: 1.894 | lr: 0.00028726332345290666\n",
      "Epoch: 1 | Step: 14450 | Avg. loss: 1.919 | lr: 0.000286507168350372\n",
      "Epoch: 1 | Step: 14500 | Avg. loss: 1.963 | lr: 0.00028575101324783745\n",
      "Epoch: 1 | Step: 14550 | Avg. loss: 1.945 | lr: 0.0002849948581453028\n",
      "Epoch: 1 | Step: 14600 | Avg. loss: 1.949 | lr: 0.0002842387030427682\n",
      "Epoch: 1 | Step: 14650 | Avg. loss: 1.926 | lr: 0.0002834825479402335\n",
      "Epoch: 1 | Step: 14700 | Avg. loss: 1.889 | lr: 0.00028272639283769886\n",
      "Epoch: 1 | Step: 14750 | Avg. loss: 1.931 | lr: 0.0002819702377351642\n",
      "Epoch: 1 | Step: 14800 | Avg. loss: 1.883 | lr: 0.0002812140826326296\n",
      "Epoch: 1 | Step: 14850 | Avg. loss: 1.903 | lr: 0.00028045792753009495\n",
      "Epoch: 1 | Step: 14900 | Avg. loss: 1.966 | lr: 0.0002797017724275603\n",
      "Epoch: 1 | Step: 14950 | Avg. loss: 1.936 | lr: 0.00027894561732502574\n",
      "Epoch: 1 | Step: 15000 | Avg. loss: 1.887 | lr: 0.0002781894622224911\n",
      "Saving model with test loss of 1.912\n",
      "Epoch: 1 | Step: 15050 | Avg. loss: 1.840 | lr: 0.00027743330711995647\n",
      "Epoch: 1 | Step: 15100 | Avg. loss: 1.915 | lr: 0.00027667715201742184\n",
      "Epoch: 1 | Step: 15150 | Avg. loss: 1.903 | lr: 0.0002759209969148872\n",
      "Epoch: 1 | Step: 15200 | Avg. loss: 1.941 | lr: 0.00027516484181235257\n",
      "Epoch: 1 | Step: 15250 | Avg. loss: 1.898 | lr: 0.00027440868670981794\n",
      "Epoch: 1 | Step: 15300 | Avg. loss: 1.902 | lr: 0.0002736525316072833\n",
      "Epoch: 1 | Step: 15350 | Avg. loss: 1.849 | lr: 0.0002728963765047486\n",
      "Epoch: 1 | Step: 15400 | Avg. loss: 1.911 | lr: 0.000272140221402214\n",
      "Epoch: 1 | Step: 15450 | Avg. loss: 1.869 | lr: 0.0002713840662996794\n",
      "Epoch: 1 | Step: 15500 | Avg. loss: 1.874 | lr: 0.00027062791119714476\n",
      "Epoch: 1 | Step: 15550 | Avg. loss: 1.937 | lr: 0.00026987175609461013\n",
      "Epoch: 1 | Step: 15600 | Avg. loss: 1.867 | lr: 0.0002691156009920755\n",
      "Epoch: 1 | Step: 15650 | Avg. loss: 1.882 | lr: 0.00026835944588954086\n",
      "Epoch: 1 | Step: 15700 | Avg. loss: 1.979 | lr: 0.00026760329078700623\n",
      "Epoch: 1 | Step: 15750 | Avg. loss: 1.878 | lr: 0.0002668471356844716\n",
      "Epoch: 1 | Step: 15800 | Avg. loss: 1.885 | lr: 0.00026609098058193696\n",
      "Epoch: 1 | Step: 15850 | Avg. loss: 1.852 | lr: 0.0002653348254794023\n",
      "Epoch: 1 | Step: 15900 | Avg. loss: 1.903 | lr: 0.00026457867037686775\n",
      "Epoch: 1 | Step: 15950 | Avg. loss: 1.890 | lr: 0.0002638225152743331\n",
      "Epoch: 1 | Step: 16000 | Avg. loss: 1.962 | lr: 0.0002630663601717985\n",
      "Saving model with test loss of 1.950\n",
      "Epoch: 1 | Step: 16050 | Avg. loss: 2.035 | lr: 0.0002623102050692638\n",
      "Epoch: 1 | Step: 16100 | Avg. loss: 1.898 | lr: 0.00026155404996672916\n",
      "Epoch: 1 | Step: 16150 | Avg. loss: 1.978 | lr: 0.0002607978948641945\n",
      "Epoch: 1 | Step: 16200 | Avg. loss: 1.839 | lr: 0.0002600417397616599\n",
      "Epoch: 1 | Step: 16250 | Avg. loss: 1.913 | lr: 0.00025928558465912525\n",
      "Epoch: 1 | Step: 16300 | Avg. loss: 1.815 | lr: 0.0002585294295565906\n",
      "Epoch: 1 | Step: 16350 | Avg. loss: 1.925 | lr: 0.00025777327445405604\n",
      "Epoch: 1 | Step: 16400 | Avg. loss: 1.853 | lr: 0.0002570171193515214\n",
      "Epoch: 1 | Step: 16450 | Avg. loss: 1.899 | lr: 0.00025626096424898677\n",
      "Epoch: 1 | Step: 16500 | Avg. loss: 1.879 | lr: 0.00025550480914645214\n",
      "Epoch: 1 | Step: 16550 | Avg. loss: 1.885 | lr: 0.0002547486540439175\n",
      "Epoch: 1 | Step: 16600 | Avg. loss: 1.909 | lr: 0.00025399249894138287\n",
      "Epoch: 1 | Step: 16650 | Avg. loss: 1.855 | lr: 0.00025323634383884823\n",
      "Epoch: 1 | Step: 16700 | Avg. loss: 1.872 | lr: 0.0002524801887363136\n",
      "Epoch: 1 | Step: 16750 | Avg. loss: 1.804 | lr: 0.00025172403363377897\n",
      "Epoch: 1 | Step: 16800 | Avg. loss: 1.894 | lr: 0.00025096787853124433\n",
      "Epoch: 1 | Step: 16850 | Avg. loss: 1.856 | lr: 0.0002502117234287097\n",
      "Epoch: 1 | Step: 16900 | Avg. loss: 1.852 | lr: 0.00024945556832617506\n",
      "Epoch: 1 | Step: 16950 | Avg. loss: 1.845 | lr: 0.00024869941322364043\n",
      "Epoch: 1 | Step: 17000 | Avg. loss: 1.838 | lr: 0.0002479432581211058\n",
      "Saving model with test loss of 2.016\n",
      "Epoch: 1 | Step: 17050 | Avg. loss: 1.872 | lr: 0.00024718710301857116\n",
      "Epoch: 1 | Step: 17100 | Avg. loss: 1.844 | lr: 0.00024643094791603653\n",
      "Epoch: 1 | Step: 17150 | Avg. loss: 1.756 | lr: 0.0002456747928135019\n",
      "Epoch: 1 | Step: 17200 | Avg. loss: 1.866 | lr: 0.00024491863771096726\n",
      "Epoch: 1 | Step: 17250 | Avg. loss: 1.796 | lr: 0.0002441624826084327\n",
      "Epoch: 1 | Step: 17300 | Avg. loss: 1.843 | lr: 0.00024340632750589802\n",
      "Epoch: 1 | Step: 17350 | Avg. loss: 1.802 | lr: 0.00024265017240336338\n",
      "Epoch: 1 | Step: 17400 | Avg. loss: 1.898 | lr: 0.00024189401730082875\n",
      "Epoch: 1 | Step: 17450 | Avg. loss: 1.892 | lr: 0.00024113786219829412\n",
      "Epoch: 1 | Step: 17500 | Avg. loss: 1.896 | lr: 0.00024038170709575948\n",
      "Epoch: 1 | Step: 17550 | Avg. loss: 1.827 | lr: 0.00023962555199322485\n",
      "Epoch: 1 | Step: 17600 | Avg. loss: 1.912 | lr: 0.00023886939689069024\n",
      "Epoch: 1 | Step: 17650 | Avg. loss: 1.737 | lr: 0.0002381132417881556\n",
      "Epoch: 1 | Step: 17700 | Avg. loss: 1.870 | lr: 0.00023735708668562095\n",
      "Epoch: 1 | Step: 17750 | Avg. loss: 1.794 | lr: 0.0002366009315830863\n",
      "Epoch: 1 | Step: 17800 | Avg. loss: 1.885 | lr: 0.0002358447764805517\n",
      "Epoch: 1 | Step: 17850 | Avg. loss: 1.796 | lr: 0.00023508862137801707\n",
      "Epoch: 1 | Step: 17900 | Avg. loss: 1.865 | lr: 0.00023433246627548244\n",
      "Epoch: 1 | Step: 17950 | Avg. loss: 1.901 | lr: 0.0002335763111729478\n",
      "Epoch: 1 | Step: 18000 | Avg. loss: 1.784 | lr: 0.00023282015607041317\n",
      "Saving model with test loss of 2.127\n",
      "Epoch: 1 | Step: 18050 | Avg. loss: 1.831 | lr: 0.00023206400096787853\n",
      "Epoch: 1 | Step: 18100 | Avg. loss: 1.789 | lr: 0.0002313078458653439\n",
      "Epoch: 1 | Step: 18150 | Avg. loss: 1.838 | lr: 0.00023055169076280927\n",
      "Epoch: 1 | Step: 18200 | Avg. loss: 1.694 | lr: 0.00022979553566027463\n",
      "Epoch: 1 | Step: 18250 | Avg. loss: 1.826 | lr: 0.00022903938055774002\n",
      "Epoch: 1 | Step: 18300 | Avg. loss: 1.871 | lr: 0.0002282832254552054\n",
      "Epoch: 1 | Step: 18350 | Avg. loss: 1.832 | lr: 0.00022752707035267076\n",
      "Epoch: 1 | Step: 18400 | Avg. loss: 1.845 | lr: 0.0002267709152501361\n",
      "Epoch: 1 | Step: 18450 | Avg. loss: 1.881 | lr: 0.00022601476014760146\n",
      "Epoch: 1 | Step: 18500 | Avg. loss: 1.896 | lr: 0.00022525860504506685\n",
      "Epoch: 1 | Step: 18550 | Avg. loss: 1.806 | lr: 0.00022450244994253222\n",
      "Epoch: 1 | Step: 18600 | Avg. loss: 1.835 | lr: 0.00022374629483999759\n",
      "Epoch: 1 | Step: 18650 | Avg. loss: 1.805 | lr: 0.00022299013973746295\n",
      "Epoch: 1 | Step: 18700 | Avg. loss: 1.819 | lr: 0.00022223398463492835\n",
      "Epoch: 1 | Step: 18750 | Avg. loss: 1.876 | lr: 0.00022147782953239368\n",
      "Epoch: 1 | Step: 18800 | Avg. loss: 1.744 | lr: 0.00022072167442985905\n",
      "Epoch: 1 | Step: 18850 | Avg. loss: 1.791 | lr: 0.00021996551932732442\n",
      "Epoch: 1 | Step: 18900 | Avg. loss: 1.778 | lr: 0.00021920936422478978\n",
      "Epoch: 1 | Step: 18950 | Avg. loss: 1.815 | lr: 0.00021845320912225517\n",
      "Epoch: 1 | Step: 19000 | Avg. loss: 1.831 | lr: 0.00021769705401972054\n",
      "Saving model with test loss of 2.040\n",
      "Epoch: 1 | Step: 19050 | Avg. loss: 1.862 | lr: 0.0002169408989171859\n",
      "Epoch: 1 | Step: 19100 | Avg. loss: 1.851 | lr: 0.00021618474381465125\n",
      "Epoch: 1 | Step: 19150 | Avg. loss: 1.845 | lr: 0.0002154285887121166\n",
      "Epoch: 1 | Step: 19200 | Avg. loss: 1.782 | lr: 0.000214672433609582\n",
      "Epoch: 1 | Step: 19250 | Avg. loss: 1.818 | lr: 0.00021391627850704737\n",
      "Epoch: 1 | Step: 19300 | Avg. loss: 1.774 | lr: 0.00021316012340451274\n",
      "Epoch: 1 | Step: 19350 | Avg. loss: 1.840 | lr: 0.0002124039683019781\n",
      "Epoch: 1 | Step: 19400 | Avg. loss: 1.762 | lr: 0.0002116478131994435\n",
      "Epoch: 1 | Step: 19450 | Avg. loss: 1.777 | lr: 0.00021089165809690886\n",
      "Epoch: 1 | Step: 19500 | Avg. loss: 1.688 | lr: 0.0002101355029943742\n",
      "Epoch: 1 | Step: 19550 | Avg. loss: 1.693 | lr: 0.00020937934789183957\n",
      "Epoch: 1 | Step: 19600 | Avg. loss: 1.739 | lr: 0.00020862319278930493\n",
      "Epoch: 1 | Step: 19650 | Avg. loss: 1.802 | lr: 0.00020786703768677032\n",
      "Epoch: 1 | Step: 19700 | Avg. loss: 1.843 | lr: 0.0002071108825842357\n",
      "Epoch: 1 | Step: 19750 | Avg. loss: 1.789 | lr: 0.00020635472748170106\n",
      "Epoch: 1 | Step: 19800 | Avg. loss: 1.761 | lr: 0.00020559857237916642\n",
      "Epoch: 1 | Step: 19850 | Avg. loss: 1.719 | lr: 0.0002048424172766318\n",
      "Epoch: 1 | Step: 19900 | Avg. loss: 1.857 | lr: 0.00020408626217409715\n",
      "Epoch: 1 | Step: 19950 | Avg. loss: 1.776 | lr: 0.00020333010707156252\n",
      "Epoch: 1 | Step: 20000 | Avg. loss: 1.801 | lr: 0.00020257395196902789\n",
      "Saving model with test loss of 1.963\n",
      "Epoch: 1 | Step: 20050 | Avg. loss: 1.768 | lr: 0.00020181779686649325\n",
      "Epoch: 1 | Step: 20100 | Avg. loss: 1.820 | lr: 0.00020106164176395864\n",
      "Epoch: 1 | Step: 20150 | Avg. loss: 1.805 | lr: 0.000200305486661424\n",
      "Epoch: 1 | Step: 20200 | Avg. loss: 1.787 | lr: 0.00019954933155888935\n",
      "Epoch: 1 | Step: 20250 | Avg. loss: 1.752 | lr: 0.00019879317645635471\n",
      "Epoch: 1 | Step: 20300 | Avg. loss: 1.805 | lr: 0.0001980370213538201\n",
      "Epoch: 1 | Step: 20350 | Avg. loss: 1.672 | lr: 0.00019728086625128547\n",
      "Epoch: 1 | Step: 20400 | Avg. loss: 1.919 | lr: 0.00019652471114875084\n",
      "Epoch: 1 | Step: 20450 | Avg. loss: 1.797 | lr: 0.0001957685560462162\n",
      "Epoch: 1 | Step: 20500 | Avg. loss: 1.709 | lr: 0.00019501240094368157\n",
      "Epoch: 1 | Step: 20550 | Avg. loss: 1.787 | lr: 0.00019425624584114694\n",
      "Epoch: 1 | Step: 20600 | Avg. loss: 1.715 | lr: 0.0001935000907386123\n",
      "Epoch: 1 | Step: 20650 | Avg. loss: 1.845 | lr: 0.00019274393563607767\n",
      "Epoch: 1 | Step: 20700 | Avg. loss: 1.677 | lr: 0.00019198778053354304\n",
      "Epoch: 1 | Step: 20750 | Avg. loss: 1.788 | lr: 0.00019123162543100843\n",
      "Epoch: 1 | Step: 20800 | Avg. loss: 1.763 | lr: 0.0001904754703284738\n",
      "Epoch: 1 | Step: 20850 | Avg. loss: 1.767 | lr: 0.00018971931522593916\n",
      "Epoch: 1 | Step: 20900 | Avg. loss: 1.786 | lr: 0.0001889631601234045\n",
      "Epoch: 1 | Step: 20950 | Avg. loss: 1.786 | lr: 0.00018820700502086986\n",
      "Epoch: 1 | Step: 21000 | Avg. loss: 1.760 | lr: 0.00018745084991833526\n",
      "Saving model with test loss of 2.089\n",
      "Epoch: 1 | Step: 21050 | Avg. loss: 1.813 | lr: 0.00018669469481580062\n",
      "Epoch: 1 | Step: 21100 | Avg. loss: 1.767 | lr: 0.000185938539713266\n",
      "Epoch: 1 | Step: 21150 | Avg. loss: 1.765 | lr: 0.00018518238461073136\n",
      "Epoch: 1 | Step: 21200 | Avg. loss: 1.820 | lr: 0.00018442622950819675\n",
      "Epoch: 1 | Step: 21250 | Avg. loss: 1.796 | lr: 0.0001836700744056621\n",
      "Epoch: 1 | Step: 21300 | Avg. loss: 1.753 | lr: 0.00018291391930312745\n",
      "Epoch: 1 | Step: 21350 | Avg. loss: 1.798 | lr: 0.00018215776420059282\n",
      "Epoch: 1 | Step: 21400 | Avg. loss: 1.766 | lr: 0.00018140160909805818\n",
      "Epoch: 1 | Step: 21450 | Avg. loss: 1.759 | lr: 0.00018064545399552358\n",
      "Epoch: 1 | Step: 21500 | Avg. loss: 1.762 | lr: 0.00017988929889298894\n",
      "Epoch: 1 | Step: 21550 | Avg. loss: 1.729 | lr: 0.0001791331437904543\n",
      "Epoch: 1 | Step: 21600 | Avg. loss: 1.796 | lr: 0.00017837698868791968\n",
      "Epoch: 1 | Step: 21650 | Avg. loss: 1.782 | lr: 0.00017762083358538501\n",
      "Epoch: 1 | Step: 21700 | Avg. loss: 1.790 | lr: 0.0001768646784828504\n",
      "Epoch: 1 | Step: 21750 | Avg. loss: 1.753 | lr: 0.00017610852338031577\n",
      "Epoch: 1 | Step: 21800 | Avg. loss: 1.776 | lr: 0.00017535236827778114\n",
      "Epoch: 1 | Step: 21850 | Avg. loss: 1.687 | lr: 0.0001745962131752465\n",
      "Epoch: 1 | Step: 21900 | Avg. loss: 1.817 | lr: 0.0001738400580727119\n",
      "Epoch: 1 | Step: 21950 | Avg. loss: 1.706 | lr: 0.00017308390297017726\n",
      "Epoch: 1 | Step: 22000 | Avg. loss: 1.744 | lr: 0.0001723277478676426\n",
      "Saving model with test loss of 2.101\n",
      "Epoch: 1 | Step: 22050 | Avg. loss: 1.733 | lr: 0.00017157159276510797\n",
      "Epoch: 1 | Step: 22100 | Avg. loss: 1.799 | lr: 0.00017081543766257333\n",
      "Epoch: 1 | Step: 22150 | Avg. loss: 1.892 | lr: 0.00017005928256003873\n",
      "Epoch: 1 | Step: 22200 | Avg. loss: 1.780 | lr: 0.0001693031274575041\n",
      "Epoch: 1 | Step: 22250 | Avg. loss: 1.713 | lr: 0.00016854697235496946\n",
      "Epoch: 1 | Step: 22300 | Avg. loss: 1.739 | lr: 0.00016779081725243483\n",
      "Epoch: 1 | Step: 22350 | Avg. loss: 1.739 | lr: 0.0001670346621499002\n",
      "Epoch: 1 | Step: 22400 | Avg. loss: 1.758 | lr: 0.00016627850704736556\n",
      "Epoch: 1 | Step: 22450 | Avg. loss: 1.790 | lr: 0.00016552235194483092\n",
      "Epoch: 1 | Step: 22500 | Avg. loss: 1.766 | lr: 0.0001647661968422963\n",
      "Epoch: 1 | Step: 22550 | Avg. loss: 1.730 | lr: 0.00016401004173976165\n",
      "Epoch: 1 | Step: 22600 | Avg. loss: 1.833 | lr: 0.00016325388663722705\n",
      "Epoch: 1 | Step: 22650 | Avg. loss: 1.796 | lr: 0.0001624977315346924\n",
      "Epoch: 1 | Step: 22700 | Avg. loss: 1.769 | lr: 0.00016174157643215775\n",
      "Epoch: 1 | Step: 22750 | Avg. loss: 1.772 | lr: 0.00016098542132962312\n",
      "Epoch: 1 | Step: 22800 | Avg. loss: 1.793 | lr: 0.0001602292662270885\n",
      "Epoch: 1 | Step: 22850 | Avg. loss: 1.736 | lr: 0.00015947311112455388\n",
      "Epoch: 1 | Step: 22900 | Avg. loss: 1.772 | lr: 0.00015871695602201924\n",
      "Epoch: 1 | Step: 22950 | Avg. loss: 1.734 | lr: 0.0001579608009194846\n",
      "Epoch: 1 | Step: 23000 | Avg. loss: 1.695 | lr: 0.00015720464581694997\n",
      "Saving model with test loss of 2.129\n",
      "Epoch: 1 | Step: 23050 | Avg. loss: 1.737 | lr: 0.00015644849071441534\n",
      "Epoch: 1 | Step: 23100 | Avg. loss: 1.795 | lr: 0.0001556923356118807\n",
      "Epoch: 1 | Step: 23150 | Avg. loss: 1.805 | lr: 0.00015493618050934607\n",
      "Epoch: 1 | Step: 23200 | Avg. loss: 1.809 | lr: 0.00015418002540681144\n",
      "Epoch: 1 | Step: 23250 | Avg. loss: 1.777 | lr: 0.00015342387030427683\n",
      "Epoch: 1 | Step: 23300 | Avg. loss: 1.780 | lr: 0.0001526677152017422\n",
      "Epoch: 1 | Step: 23350 | Avg. loss: 1.758 | lr: 0.00015191156009920756\n",
      "Epoch: 1 | Step: 23400 | Avg. loss: 1.749 | lr: 0.0001511554049966729\n",
      "Epoch: 1 | Step: 23450 | Avg. loss: 1.810 | lr: 0.00015039924989413827\n",
      "Epoch: 1 | Step: 23500 | Avg. loss: 1.727 | lr: 0.00014964309479160366\n",
      "Epoch: 1 | Step: 23550 | Avg. loss: 1.748 | lr: 0.00014888693968906903\n",
      "Epoch: 1 | Step: 23600 | Avg. loss: 1.728 | lr: 0.0001481307845865344\n",
      "Epoch: 1 | Step: 23650 | Avg. loss: 1.695 | lr: 0.00014737462948399976\n",
      "Epoch: 1 | Step: 23700 | Avg. loss: 1.823 | lr: 0.00014661847438146515\n",
      "Epoch: 1 | Step: 23750 | Avg. loss: 1.765 | lr: 0.00014586231927893052\n",
      "Epoch: 1 | Step: 23800 | Avg. loss: 1.731 | lr: 0.00014510616417639586\n",
      "Epoch: 1 | Step: 23850 | Avg. loss: 1.734 | lr: 0.00014435000907386122\n",
      "Epoch: 1 | Step: 23900 | Avg. loss: 1.711 | lr: 0.0001435938539713266\n",
      "Epoch: 1 | Step: 23950 | Avg. loss: 1.649 | lr: 0.00014283769886879198\n",
      "Epoch: 1 | Step: 24000 | Avg. loss: 1.757 | lr: 0.00014208154376625735\n",
      "Saving model with test loss of 1.785\n",
      "Epoch: 1 | Step: 24050 | Avg. loss: 1.673 | lr: 0.0001413253886637227\n",
      "Epoch: 1 | Step: 24100 | Avg. loss: 1.754 | lr: 0.00014056923356118808\n",
      "Epoch: 1 | Step: 24150 | Avg. loss: 1.704 | lr: 0.00013981307845865342\n",
      "Epoch: 1 | Step: 24200 | Avg. loss: 1.676 | lr: 0.0001390569233561188\n",
      "Epoch: 1 | Step: 24250 | Avg. loss: 1.677 | lr: 0.00013830076825358418\n",
      "Epoch: 1 | Step: 24300 | Avg. loss: 1.704 | lr: 0.00013754461315104954\n",
      "Epoch: 1 | Step: 24350 | Avg. loss: 1.648 | lr: 0.0001367884580485149\n",
      "Epoch: 1 | Step: 24400 | Avg. loss: 1.643 | lr: 0.0001360323029459803\n",
      "Epoch: 1 | Step: 24450 | Avg. loss: 1.660 | lr: 0.00013527614784344567\n",
      "Epoch: 1 | Step: 24500 | Avg. loss: 1.699 | lr: 0.000134519992740911\n",
      "Epoch: 1 | Step: 24550 | Avg. loss: 1.720 | lr: 0.00013376383763837637\n",
      "Epoch: 1 | Step: 24600 | Avg. loss: 1.708 | lr: 0.00013300768253584174\n",
      "Epoch: 1 | Step: 24650 | Avg. loss: 1.736 | lr: 0.00013225152743330713\n",
      "Epoch: 1 | Step: 24700 | Avg. loss: 1.682 | lr: 0.0001314953723307725\n",
      "Epoch: 1 | Step: 24750 | Avg. loss: 1.773 | lr: 0.00013073921722823786\n",
      "Epoch: 1 | Step: 24800 | Avg. loss: 1.637 | lr: 0.00012998306212570323\n",
      "Epoch: 1 | Step: 24850 | Avg. loss: 1.768 | lr: 0.0001292269070231686\n",
      "Epoch: 1 | Step: 24900 | Avg. loss: 1.727 | lr: 0.00012847075192063396\n",
      "Epoch: 1 | Step: 24950 | Avg. loss: 1.681 | lr: 0.00012771459681809933\n",
      "Epoch: 1 | Step: 25000 | Avg. loss: 1.750 | lr: 0.0001269584417155647\n",
      "Saving model with test loss of 1.837\n",
      "Epoch: 1 | Step: 25050 | Avg. loss: 1.706 | lr: 0.00012620228661303006\n",
      "Epoch: 1 | Step: 25100 | Avg. loss: 1.654 | lr: 0.00012544613151049545\n",
      "Epoch: 1 | Step: 25150 | Avg. loss: 1.667 | lr: 0.0001246899764079608\n",
      "Epoch: 1 | Step: 25200 | Avg. loss: 1.649 | lr: 0.00012393382130542618\n",
      "Epoch: 1 | Step: 25250 | Avg. loss: 1.751 | lr: 0.00012317766620289155\n",
      "Epoch: 1 | Step: 25300 | Avg. loss: 1.705 | lr: 0.00012242151110035691\n",
      "Epoch: 1 | Step: 25350 | Avg. loss: 1.620 | lr: 0.00012166535599782228\n",
      "Epoch: 1 | Step: 25400 | Avg. loss: 1.731 | lr: 0.00012090920089528765\n",
      "Epoch: 1 | Step: 25450 | Avg. loss: 1.639 | lr: 0.00012015304579275301\n",
      "Epoch: 1 | Step: 25500 | Avg. loss: 1.790 | lr: 0.00011939689069021838\n",
      "Epoch: 1 | Step: 25550 | Avg. loss: 1.696 | lr: 0.00011864073558768374\n",
      "Epoch: 1 | Step: 25600 | Avg. loss: 1.639 | lr: 0.00011788458048514912\n",
      "Epoch: 1 | Step: 25650 | Avg. loss: 1.665 | lr: 0.00011712842538261448\n",
      "Epoch: 1 | Step: 25700 | Avg. loss: 1.680 | lr: 0.00011637227028007986\n",
      "Epoch: 1 | Step: 25750 | Avg. loss: 1.688 | lr: 0.00011561611517754522\n",
      "Epoch: 1 | Step: 25800 | Avg. loss: 1.758 | lr: 0.0001148599600750106\n",
      "Epoch: 1 | Step: 25850 | Avg. loss: 1.692 | lr: 0.00011410380497247595\n",
      "Epoch: 1 | Step: 25900 | Avg. loss: 1.721 | lr: 0.00011334764986994132\n",
      "Epoch: 1 | Step: 25950 | Avg. loss: 1.716 | lr: 0.0001125914947674067\n",
      "Epoch: 1 | Step: 26000 | Avg. loss: 1.648 | lr: 0.00011183533966487205\n",
      "Saving model with test loss of 1.964\n",
      "Epoch: 1 | Step: 26050 | Avg. loss: 1.695 | lr: 0.00011107918456233743\n",
      "Epoch: 1 | Step: 26100 | Avg. loss: 1.659 | lr: 0.0001103230294598028\n",
      "Epoch: 1 | Step: 26150 | Avg. loss: 1.704 | lr: 0.00010956687435726818\n",
      "Epoch: 1 | Step: 26200 | Avg. loss: 1.681 | lr: 0.00010881071925473353\n",
      "Epoch: 1 | Step: 26250 | Avg. loss: 1.700 | lr: 0.00010805456415219891\n",
      "Epoch: 1 | Step: 26300 | Avg. loss: 1.668 | lr: 0.00010729840904966427\n",
      "Epoch: 1 | Step: 26350 | Avg. loss: 1.636 | lr: 0.00010654225394712964\n",
      "Epoch: 1 | Step: 26400 | Avg. loss: 1.666 | lr: 0.000105786098844595\n",
      "Epoch: 1 | Step: 26450 | Avg. loss: 1.759 | lr: 0.00010502994374206037\n",
      "Epoch: 1 | Step: 26500 | Avg. loss: 1.628 | lr: 0.00010427378863952575\n",
      "Epoch: 1 | Step: 26550 | Avg. loss: 1.683 | lr: 0.0001035176335369911\n",
      "Epoch: 1 | Step: 26600 | Avg. loss: 1.684 | lr: 0.00010276147843445648\n",
      "Epoch: 1 | Step: 26650 | Avg. loss: 1.709 | lr: 0.00010200532333192185\n",
      "Epoch: 1 | Step: 26700 | Avg. loss: 1.668 | lr: 0.00010124916822938721\n",
      "Epoch: 1 | Step: 26750 | Avg. loss: 1.661 | lr: 0.00010049301312685258\n",
      "Epoch: 1 | Step: 26800 | Avg. loss: 1.664 | lr: 9.973685802431795e-05\n",
      "Epoch: 1 | Step: 26850 | Avg. loss: 1.661 | lr: 9.898070292178332e-05\n",
      "Epoch: 1 | Step: 26900 | Avg. loss: 1.653 | lr: 9.822454781924868e-05\n",
      "Epoch: 1 | Step: 26950 | Avg. loss: 1.659 | lr: 9.746839271671406e-05\n",
      "Epoch: 1 | Step: 27000 | Avg. loss: 1.699 | lr: 9.671223761417942e-05\n",
      "Saving model with test loss of 2.028\n",
      "Epoch: 1 | Step: 27050 | Avg. loss: 1.625 | lr: 9.59560825116448e-05\n",
      "Epoch: 1 | Step: 27100 | Avg. loss: 1.584 | lr: 9.519992740911015e-05\n",
      "Epoch: 1 | Step: 27150 | Avg. loss: 1.662 | lr: 9.444377230657552e-05\n",
      "Epoch: 1 | Step: 27200 | Avg. loss: 1.635 | lr: 9.36876172040409e-05\n",
      "Epoch: 1 | Step: 27250 | Avg. loss: 1.712 | lr: 9.293146210150625e-05\n",
      "Epoch: 1 | Step: 27300 | Avg. loss: 1.725 | lr: 9.217530699897163e-05\n",
      "Epoch: 1 | Step: 27350 | Avg. loss: 1.665 | lr: 9.1419151896437e-05\n",
      "Epoch: 1 | Step: 27400 | Avg. loss: 1.661 | lr: 9.066299679390238e-05\n",
      "Epoch: 1 | Step: 27450 | Avg. loss: 1.761 | lr: 8.990684169136773e-05\n",
      "Epoch: 1 | Step: 27500 | Avg. loss: 1.685 | lr: 8.915068658883311e-05\n",
      "Epoch: 1 | Step: 27550 | Avg. loss: 1.715 | lr: 8.839453148629847e-05\n",
      "Epoch: 1 | Step: 27600 | Avg. loss: 1.719 | lr: 8.763837638376384e-05\n",
      "Epoch: 1 | Step: 27650 | Avg. loss: 1.589 | lr: 8.68822212812292e-05\n",
      "Epoch: 1 | Step: 27700 | Avg. loss: 1.700 | lr: 8.612606617869457e-05\n",
      "Epoch: 1 | Step: 27750 | Avg. loss: 1.603 | lr: 8.536991107615995e-05\n",
      "Epoch: 1 | Step: 27800 | Avg. loss: 1.670 | lr: 8.46137559736253e-05\n",
      "Epoch: 1 | Step: 27850 | Avg. loss: 1.626 | lr: 8.385760087109068e-05\n",
      "Epoch: 1 | Step: 27900 | Avg. loss: 1.681 | lr: 8.310144576855605e-05\n",
      "Epoch: 1 | Step: 27950 | Avg. loss: 1.616 | lr: 8.234529066602142e-05\n",
      "Epoch: 1 | Step: 28000 | Avg. loss: 1.674 | lr: 8.158913556348678e-05\n",
      "Saving model with test loss of 2.015\n",
      "Epoch: 1 | Step: 28050 | Avg. loss: 1.572 | lr: 8.083298046095215e-05\n",
      "Epoch: 1 | Step: 28100 | Avg. loss: 1.634 | lr: 8.007682535841753e-05\n",
      "Epoch: 1 | Step: 28150 | Avg. loss: 1.647 | lr: 7.932067025588288e-05\n",
      "Epoch: 1 | Step: 28200 | Avg. loss: 1.601 | lr: 7.856451515334826e-05\n",
      "Epoch: 1 | Step: 28250 | Avg. loss: 1.585 | lr: 7.780836005081362e-05\n",
      "Epoch: 1 | Step: 28300 | Avg. loss: 1.694 | lr: 7.7052204948279e-05\n",
      "Epoch: 1 | Step: 28350 | Avg. loss: 1.662 | lr: 7.629604984574436e-05\n",
      "Epoch: 1 | Step: 28400 | Avg. loss: 1.641 | lr: 7.553989474320972e-05\n",
      "Epoch: 1 | Step: 28450 | Avg. loss: 1.626 | lr: 7.47837396406751e-05\n",
      "Epoch: 1 | Step: 28500 | Avg. loss: 1.585 | lr: 7.402758453814047e-05\n",
      "Epoch: 1 | Step: 28550 | Avg. loss: 1.686 | lr: 7.327142943560583e-05\n",
      "Epoch: 1 | Step: 28600 | Avg. loss: 1.676 | lr: 7.25152743330712e-05\n",
      "Epoch: 1 | Step: 28650 | Avg. loss: 1.650 | lr: 7.175911923053658e-05\n",
      "Epoch: 1 | Step: 28700 | Avg. loss: 1.624 | lr: 7.100296412800193e-05\n",
      "Epoch: 1 | Step: 28750 | Avg. loss: 1.656 | lr: 7.024680902546731e-05\n",
      "Epoch: 1 | Step: 28800 | Avg. loss: 1.609 | lr: 6.949065392293268e-05\n",
      "Epoch: 1 | Step: 28850 | Avg. loss: 1.639 | lr: 6.873449882039804e-05\n",
      "Epoch: 1 | Step: 28900 | Avg. loss: 1.592 | lr: 6.797834371786341e-05\n",
      "Epoch: 1 | Step: 28950 | Avg. loss: 1.666 | lr: 6.722218861532877e-05\n",
      "Epoch: 1 | Step: 29000 | Avg. loss: 1.617 | lr: 6.646603351279415e-05\n",
      "Saving model with test loss of 1.980\n",
      "Epoch: 1 | Step: 29050 | Avg. loss: 1.612 | lr: 6.57098784102595e-05\n",
      "Epoch: 1 | Step: 29100 | Avg. loss: 1.573 | lr: 6.495372330772489e-05\n",
      "Epoch: 1 | Step: 29150 | Avg. loss: 1.640 | lr: 6.419756820519025e-05\n",
      "Epoch: 1 | Step: 29200 | Avg. loss: 1.622 | lr: 6.344141310265562e-05\n",
      "Epoch: 1 | Step: 29250 | Avg. loss: 1.676 | lr: 6.268525800012098e-05\n",
      "Epoch: 1 | Step: 29300 | Avg. loss: 1.656 | lr: 6.192910289758635e-05\n",
      "Epoch: 1 | Step: 29350 | Avg. loss: 1.640 | lr: 6.117294779505173e-05\n",
      "Epoch: 1 | Step: 29400 | Avg. loss: 1.648 | lr: 6.041679269251709e-05\n",
      "Epoch: 1 | Step: 29450 | Avg. loss: 1.585 | lr: 5.966063758998245e-05\n",
      "Epoch: 1 | Step: 29500 | Avg. loss: 1.580 | lr: 5.8904482487447826e-05\n",
      "Epoch: 1 | Step: 29550 | Avg. loss: 1.636 | lr: 5.814832738491319e-05\n",
      "Epoch: 1 | Step: 29600 | Avg. loss: 1.683 | lr: 5.7392172282378564e-05\n",
      "Epoch: 1 | Step: 29650 | Avg. loss: 1.617 | lr: 5.663601717984393e-05\n",
      "Epoch: 1 | Step: 29700 | Avg. loss: 1.593 | lr: 5.58798620773093e-05\n",
      "Epoch: 1 | Step: 29750 | Avg. loss: 1.616 | lr: 5.512370697477467e-05\n",
      "Epoch: 1 | Step: 29800 | Avg. loss: 1.631 | lr: 5.436755187224004e-05\n",
      "Epoch: 1 | Step: 29850 | Avg. loss: 1.612 | lr: 5.36113967697054e-05\n",
      "Epoch: 1 | Step: 29900 | Avg. loss: 1.596 | lr: 5.285524166717077e-05\n",
      "Epoch: 1 | Step: 29950 | Avg. loss: 1.604 | lr: 5.209908656463614e-05\n",
      "Epoch: 1 | Step: 30000 | Avg. loss: 1.514 | lr: 5.1342931462101505e-05\n",
      "Saving model with test loss of 2.096\n",
      "Epoch: 1 | Step: 30050 | Avg. loss: 1.572 | lr: 5.058677635956688e-05\n",
      "Epoch: 1 | Step: 30100 | Avg. loss: 1.570 | lr: 4.9830621257032244e-05\n",
      "Epoch: 1 | Step: 30150 | Avg. loss: 1.582 | lr: 4.9074466154497616e-05\n",
      "Epoch: 1 | Step: 30200 | Avg. loss: 1.633 | lr: 4.831831105196298e-05\n",
      "Epoch: 1 | Step: 30250 | Avg. loss: 1.660 | lr: 4.756215594942834e-05\n",
      "Epoch: 1 | Step: 30300 | Avg. loss: 1.612 | lr: 4.6806000846893714e-05\n",
      "Epoch: 1 | Step: 30350 | Avg. loss: 1.647 | lr: 4.604984574435908e-05\n",
      "Epoch: 1 | Step: 30400 | Avg. loss: 1.634 | lr: 4.529369064182445e-05\n",
      "Epoch: 1 | Step: 30450 | Avg. loss: 1.618 | lr: 4.453753553928982e-05\n",
      "Epoch: 1 | Step: 30500 | Avg. loss: 1.616 | lr: 4.378138043675519e-05\n",
      "Epoch: 1 | Step: 30550 | Avg. loss: 1.636 | lr: 4.302522533422056e-05\n",
      "Epoch: 1 | Step: 30600 | Avg. loss: 1.612 | lr: 4.226907023168593e-05\n",
      "Epoch: 1 | Step: 30650 | Avg. loss: 1.677 | lr: 4.151291512915129e-05\n",
      "Epoch: 1 | Step: 30700 | Avg. loss: 1.620 | lr: 4.0756760026616655e-05\n",
      "Epoch: 1 | Step: 30750 | Avg. loss: 1.589 | lr: 4.000060492408203e-05\n",
      "Epoch: 1 | Step: 30800 | Avg. loss: 1.562 | lr: 3.9244449821547393e-05\n",
      "Epoch: 1 | Step: 30850 | Avg. loss: 1.590 | lr: 3.8488294719012766e-05\n",
      "Epoch: 1 | Step: 30900 | Avg. loss: 1.605 | lr: 3.773213961647813e-05\n",
      "Epoch: 1 | Step: 30950 | Avg. loss: 1.659 | lr: 3.6975984513943505e-05\n",
      "Epoch: 1 | Step: 31000 | Avg. loss: 1.613 | lr: 3.621982941140887e-05\n",
      "Saving model with test loss of 1.865\n",
      "Epoch: 1 | Step: 31050 | Avg. loss: 1.598 | lr: 3.546367430887424e-05\n",
      "Epoch: 1 | Step: 31100 | Avg. loss: 1.629 | lr: 3.47075192063396e-05\n",
      "Epoch: 1 | Step: 31150 | Avg. loss: 1.555 | lr: 3.395136410380497e-05\n",
      "Epoch: 1 | Step: 31200 | Avg. loss: 1.657 | lr: 3.319520900127034e-05\n",
      "Epoch: 1 | Step: 31250 | Avg. loss: 1.487 | lr: 3.243905389873571e-05\n",
      "Epoch: 1 | Step: 31300 | Avg. loss: 1.542 | lr: 3.168289879620108e-05\n",
      "Epoch: 1 | Step: 31350 | Avg. loss: 1.611 | lr: 3.0926743693666445e-05\n",
      "Epoch: 1 | Step: 31400 | Avg. loss: 1.576 | lr: 3.017058859113181e-05\n",
      "Epoch: 1 | Step: 31450 | Avg. loss: 1.489 | lr: 2.941443348859718e-05\n",
      "Epoch: 1 | Step: 31500 | Avg. loss: 1.609 | lr: 2.865827838606255e-05\n",
      "Epoch: 1 | Step: 31550 | Avg. loss: 1.575 | lr: 2.790212328352792e-05\n",
      "Epoch: 1 | Step: 31600 | Avg. loss: 1.615 | lr: 2.7145968180993285e-05\n",
      "Epoch: 1 | Step: 31650 | Avg. loss: 1.582 | lr: 2.6389813078458654e-05\n",
      "Epoch: 1 | Step: 31700 | Avg. loss: 1.592 | lr: 2.5633657975924024e-05\n",
      "Epoch: 1 | Step: 31750 | Avg. loss: 1.641 | lr: 2.4877502873389393e-05\n",
      "Epoch: 1 | Step: 31800 | Avg. loss: 1.518 | lr: 2.4121347770854755e-05\n",
      "Epoch: 1 | Step: 31850 | Avg. loss: 1.540 | lr: 2.3365192668320125e-05\n",
      "Epoch: 1 | Step: 31900 | Avg. loss: 1.607 | lr: 2.2609037565785494e-05\n",
      "Epoch: 1 | Step: 31950 | Avg. loss: 1.567 | lr: 2.1852882463250863e-05\n",
      "Epoch: 1 | Step: 32000 | Avg. loss: 1.603 | lr: 2.109672736071623e-05\n",
      "Saving model with test loss of 1.947\n",
      "Epoch: 1 | Step: 32050 | Avg. loss: 1.589 | lr: 2.03405722581816e-05\n",
      "Epoch: 1 | Step: 32100 | Avg. loss: 1.631 | lr: 1.9584417155646968e-05\n",
      "Epoch: 1 | Step: 32150 | Avg. loss: 1.661 | lr: 1.8828262053112337e-05\n",
      "Epoch: 1 | Step: 32200 | Avg. loss: 1.573 | lr: 1.8072106950577703e-05\n",
      "Epoch: 1 | Step: 32250 | Avg. loss: 1.607 | lr: 1.731595184804307e-05\n",
      "Epoch: 1 | Step: 32300 | Avg. loss: 1.629 | lr: 1.6559796745508438e-05\n",
      "Epoch: 1 | Step: 32350 | Avg. loss: 1.546 | lr: 1.5803641642973807e-05\n",
      "Epoch: 1 | Step: 32400 | Avg. loss: 1.578 | lr: 1.5047486540439175e-05\n",
      "Epoch: 1 | Step: 32450 | Avg. loss: 1.604 | lr: 1.4291331437904544e-05\n",
      "Epoch: 1 | Step: 32500 | Avg. loss: 1.558 | lr: 1.3535176335369912e-05\n",
      "Epoch: 1 | Step: 32550 | Avg. loss: 1.586 | lr: 1.277902123283528e-05\n",
      "Epoch: 1 | Step: 32600 | Avg. loss: 1.594 | lr: 1.2022866130300647e-05\n",
      "Epoch: 1 | Step: 32650 | Avg. loss: 1.598 | lr: 1.1266711027766016e-05\n",
      "Epoch: 1 | Step: 32700 | Avg. loss: 1.556 | lr: 1.0510555925231384e-05\n",
      "Epoch: 1 | Step: 32750 | Avg. loss: 1.598 | lr: 9.754400822696751e-06\n",
      "Epoch: 1 | Step: 32800 | Avg. loss: 1.569 | lr: 8.998245720162119e-06\n",
      "Epoch: 1 | Step: 32850 | Avg. loss: 1.537 | lr: 8.242090617627488e-06\n",
      "Epoch: 1 | Step: 32900 | Avg. loss: 1.560 | lr: 7.485935515092856e-06\n",
      "Epoch: 1 | Step: 32950 | Avg. loss: 1.558 | lr: 6.729780412558224e-06\n",
      "Epoch: 1 | Step: 33000 | Avg. loss: 1.527 | lr: 5.973625310023592e-06\n",
      "Saving model with test loss of 1.923\n",
      "Epoch: 1 | Step: 33050 | Avg. loss: 1.562 | lr: 5.21747020748896e-06\n",
      "Epoch: 1 | Step: 33100 | Avg. loss: 1.633 | lr: 4.461315104954329e-06\n",
      "Epoch: 1 | Step: 33150 | Avg. loss: 1.518 | lr: 3.7051600024196964e-06\n",
      "Epoch: 1 | Step: 33200 | Avg. loss: 1.506 | lr: 2.9490048998850644e-06\n",
      "Epoch: 1 | Step: 33250 | Avg. loss: 1.591 | lr: 2.1928497973504325e-06\n",
      "Epoch: 1 | Step: 33300 | Avg. loss: 1.577 | lr: 1.4366946948158007e-06\n",
      "Epoch: 1 | Step: 33350 | Avg. loss: 1.617 | lr: 6.805395922811688e-07\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch_idx in range(n_epochs):\n",
    "  # Randomize data order\n",
    "  data_generator = get_data_generator(train_dataset, LANG_TOKEN_MAPPING,\n",
    "                                      tokenizer, batch_size)\n",
    "                \n",
    "  for batch_idx, (input_batch, label_batch) \\\n",
    "      in tqdm_notebook(enumerate(data_generator), total=n_batches):\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Forward pass\n",
    "    model_out = model.forward(\n",
    "        input_ids = input_batch,\n",
    "        labels = label_batch)\n",
    "\n",
    "    # Calculate loss and update weights\n",
    "    loss = model_out.loss\n",
    "    losses.append(loss.item())\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "\n",
    "    # Print training update info\n",
    "    if (batch_idx + 1) % print_freq == 0:\n",
    "      avg_loss = np.mean(losses[-print_freq:])\n",
    "      print('Epoch: {} | Step: {} | Avg. loss: {:.3f} | lr: {}'.format(\n",
    "          epoch_idx+1, batch_idx+1, avg_loss, scheduler.get_last_lr()[0]))\n",
    "      \n",
    "    if (batch_idx + 1) % checkpoint_freq == 0:\n",
    "      test_loss = eval_model(model, test_dataset)\n",
    "      print('Saving model with test loss of {:.3f}'.format(test_loss))\n",
    "      torch.save(model.state_dict(), model_path)\n",
    "\n",
    "torch.save(model.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "unlimited-fighter",
   "metadata": {
    "gradient": {}
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(model_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "antique-vegetation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f207091c070>]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "window_size = 50\n",
    "smoothed_losses = []\n",
    "for i in range(len(losses)-window_size):\n",
    "  smoothed_losses.append(np.mean(losses[i:i+window_size]))\n",
    "\n",
    "plt.plot(smoothed_losses[100:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "pediatric-austria",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw input text: Give shots of injections or pills, but he must be alright soon.\n",
      "Truncated input text: <hi>▁Give▁shots of▁injections or▁pills,▁but▁he▁must be▁alright▁soon.<pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n"
     ]
    }
   ],
   "source": [
    "test_sentence = test_dataset[0]['translation']['en']\n",
    "print('Raw input text:', test_sentence)\n",
    "\n",
    "input_ids = encode_input_str(\n",
    "    text = test_sentence,\n",
    "    target_lang = 'hi',\n",
    "    tokenizer = tokenizer,\n",
    "    seq_len = model.config.max_length,\n",
    "    lang_token_map = LANG_TOKEN_MAPPING)\n",
    "input_ids = input_ids.unsqueeze(0).cuda()\n",
    "\n",
    "print('Truncated input text:', tokenizer.convert_tokens_to_string(\n",
    "    tokenizer.convert_ids_to_tokens(input_ids[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "outdoor-allah",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "क्या आप निश्चित हैं कि \n",
      "मैं तुम्हारे लिए ा\n",
      "मैं तुम्हारे लिए एक\n"
     ]
    }
   ],
   "source": [
    "output_tokens = model.generate(input_ids, num_beams=10, num_return_sequences=3)\n",
    "# print(output_tokens)\n",
    "for token_set in output_tokens:\n",
    "  print(tokenizer.decode(token_set, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "minor-acoustic",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A surfboarder ran into a shark  ->  ▁All▁right.\n"
     ]
    }
   ],
   "source": [
    "output_language = 'en' #@param [\"en\", \"de\"]\n",
    "#@title Slick Blue Translate\n",
    "input_text = 'A surfboarder ran into a shark' #@param {type:\"string\"}\n",
    "\n",
    "input_ids = encode_input_str(\n",
    "    text = input_text,\n",
    "    target_lang = output_language,\n",
    "    tokenizer = tokenizer,\n",
    "    seq_len = model.config.max_length,\n",
    "    lang_token_map = LANG_TOKEN_MAPPING)\n",
    "input_ids = input_ids.unsqueeze(0).cuda()\n",
    "\n",
    "output_tokens = model.generate(input_ids, num_beams=20, length_penalty=0.2)\n",
    "print(input_text + '  ->  ' + \\\n",
    "      tokenizer.decode(output_tokens[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "civic-boxing",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, load_metric\n",
    "\n",
    "metric = load_metric(\"bleu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "resident-setup",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLeu = 32.16\n"
     ]
    }
   ],
   "source": [
    "print(f'Bleu = {model.compute_metrics*100:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "metropolitan-phase",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "robust-danish",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
