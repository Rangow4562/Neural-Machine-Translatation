{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "SMT_English_to_Spanish.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "2cVVv4e0VoF0"
      },
      "source": [
        "import pickle"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aKVfJuwSxY-w"
      },
      "source": [
        "tokenized_stores = {'en_train': [], 'en_dev': [], 'en_test': [], 'se_train': [], 'se_dev': [], 'se_test': []}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5OCBe2XixY--"
      },
      "source": [
        "\n",
        "for key in tokenized_stores:\n",
        "    file_name = \"/content/\" + str(key)[3:] + \".\" + str(key)[0:2]\n",
        "    load = open(file_name)\n",
        "    sentences = load.read().split('\\n')\n",
        "    \n",
        "    for sentence in sentences:\n",
        "        token_store = sentence.split(' ')\n",
        "        tokenized_stores[key].append(token_store)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4SWltGqbxY_H",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b306d8c-cc5f-4ad4-dba0-13762f91c412"
      },
      "source": [
        "print(tokenized_stores['se_train'][2])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['He', 'quedado', 'conmovido', 'por', 'esta', 'conferencia,', 'y', 'deseo', 'agradecer', 'a', 'todos', 'ustedes', 'sus', 'amables', 'comentarios', 'acerca', 'de', 'lo', 'que', 'tenía', 'que', 'decir', 'la', 'otra', 'noche.', '']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dIZNBPNAxY_S"
      },
      "source": [
        "train_size = len(tokenized_stores['en_train'])\n",
        "dev_size = len(tokenized_stores['en_dev'])\n",
        "test_size = len(tokenized_stores['en_test'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6VMMiv4BxY_c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f41b30c8-59bc-4cc1-ce43-8abcd4692e16"
      },
      "source": [
        "en_words = {}\n",
        "es_words = {}\n",
        "\n",
        "for key in tokenized_stores:\n",
        "    if str(key)[0] == 'e':\n",
        "        # creating en_words\n",
        "        for sentence in tokenized_stores[key]:\n",
        "            for word in sentence:\n",
        "                if word in en_words:\n",
        "                    en_words[word] += 1\n",
        "                else:\n",
        "                    en_words[word] = 1\n",
        "    else:\n",
        "        # creating es_words\n",
        "        for sentence in tokenized_stores[key]:\n",
        "            for word in sentence:\n",
        "                if word in es_words:\n",
        "                    es_words[word] += 1\n",
        "                else:\n",
        "                    es_words[word] = 1\n",
        "                    \n",
        "en_vocab = len(en_words)\n",
        "es_vocab = len(es_words)\n",
        "print(\"Number of Unique Words:\")\n",
        "print(\"> English:\", str(en_vocab))\n",
        "print(\"> Spanish:\", str(es_vocab))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of Unique Words:\n",
            "> English: 48811\n",
            "> Spanish: 61372\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fCA1636BxY_k"
      },
      "source": [
        "\n",
        "t = {}\n",
        "\n",
        "uniform = 1 / (en_vocab * es_vocab)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sKUGRo00xY_p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "254a8f16-09c2-4289-a7c9-9aaee34108d7"
      },
      "source": [
        "n_iters = 0\n",
        "max_iters = 25\n",
        "\n",
        "fine_tune = 1\n",
        "has_converged = False\n",
        "\n",
        "while n_iters < max_iters and has_converged == False:\n",
        "    has_converged = True\n",
        "    max_change = -1\n",
        "\n",
        "    n_iters += 1 \n",
        "    count = {}\n",
        "    total = {}\n",
        "    for index in range(train_size):\n",
        "        s_total = {}\n",
        "        for en_word in tokenized_stores['en_train'][index]:\n",
        "            s_total[en_word] = 0\n",
        "            for es_word in tokenized_stores['se_train'][index]:\n",
        "                if (en_word, es_word) not in t:\n",
        "                    t[(en_word, es_word)] = uniform\n",
        "                s_total[en_word] += t[(en_word, es_word)]\n",
        "\n",
        "        for en_word in tokenized_stores['en_train'][index]:\n",
        "            for es_word in tokenized_stores['se_train'][index]:\n",
        "                if (en_word, es_word) not in count:\n",
        "                    count[(en_word, es_word)] = 0\n",
        "                count[(en_word, es_word)] += (t[(en_word, es_word)] / s_total[en_word])\n",
        "\n",
        "                if es_word not in total:\n",
        "                    total[es_word] = 0\n",
        "                total[es_word] += (t[(en_word, es_word)] / s_total[en_word])\n",
        "\n",
        "\n",
        "\n",
        "    if fine_tune == 0:\n",
        "      updated = {}\n",
        "   \n",
        "      for index in range(train_size):\n",
        "          for es_word in tokenized_stores['se_train'][index]:\n",
        "              for en_word in tokenized_stores['en_train'][index]:\n",
        "                  if (en_word, es_word) in updated:\n",
        "                      continue\n",
        "                  updated[(en_word, es_word)] = 1\n",
        "                  if abs(t[(en_word, es_word)] - count[(en_word, es_word)] / total[es_word]) > 0.01:\n",
        "                      has_converged = False\n",
        "                      max_change = max(max_change, abs(t[(en_word, es_word)] - count[(en_word, es_word)] / total[es_word]))\n",
        "                  t[(en_word, es_word)] = count[(en_word, es_word)] / total[es_word]\n",
        "\n",
        "    elif fine_tune == 1:\n",
        " \n",
        "      max_words = 1000\n",
        "      n_es_words = 0\n",
        "      updates = 0\n",
        "\n",
        "      for es_word_tuples in sorted(es_words.items(), key = lambda k:(k[1], k[0]), reverse = True):\n",
        "          es_word = es_word_tuples[0]\n",
        "          n_es_words += 1\n",
        "          if n_es_words > max_words:\n",
        "              break\n",
        "          n_en_words = 0\n",
        "          for en_word_tuples in sorted(en_words.items(), key = lambda k:(k[1], k[0]), reverse = True):\n",
        "              en_word = en_word_tuples[0]\n",
        "              n_en_words += 1\n",
        "              if n_en_words > max_words:\n",
        "                  break\n",
        "              if (en_word, es_word) not in count or es_word not in total:\n",
        "                  continue\n",
        "               \n",
        "              else:\n",
        "                  if abs(t[(en_word, es_word)] - count[(en_word, es_word)] / total[es_word]) > 0.005:\n",
        "                      has_converged = False\n",
        "                      max_change = max(max_change, abs(t[(en_word, es_word)] - count[(en_word, es_word)] / total[es_word]))\n",
        "                  t[(en_word, es_word)] = count[(en_word, es_word)] / total[es_word]\n",
        "\n",
        "    print(\"Iteration \" + str(n_iters) + \" Completed, Maximum Change: \" + str(max_change))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration 1 Completed, Maximum Change: 0.2768782008740783\n",
            "Iteration 2 Completed, Maximum Change: 0.34932410271091824\n",
            "Iteration 3 Completed, Maximum Change: 0.20264198358496976\n",
            "Iteration 4 Completed, Maximum Change: 0.12245388811795044\n",
            "Iteration 5 Completed, Maximum Change: 0.08040468373817894\n",
            "Iteration 6 Completed, Maximum Change: 0.05104219402918564\n",
            "Iteration 7 Completed, Maximum Change: 0.03274035743078252\n",
            "Iteration 8 Completed, Maximum Change: 0.022929256490210936\n",
            "Iteration 9 Completed, Maximum Change: 0.017229226299106948\n",
            "Iteration 10 Completed, Maximum Change: 0.013231263522106351\n",
            "Iteration 11 Completed, Maximum Change: 0.010354735163378592\n",
            "Iteration 12 Completed, Maximum Change: 0.008236117038911805\n",
            "Iteration 13 Completed, Maximum Change: 0.006643797653858252\n",
            "Iteration 14 Completed, Maximum Change: 0.005425935978163321\n",
            "Iteration 15 Completed, Maximum Change: 0.005217421559786033\n",
            "Iteration 16 Completed, Maximum Change: -1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6yaalpW64cA-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "25dbebd0-8f6d-4ba9-a97a-8d4c2e731287"
      },
      "source": [
        "\n",
        "limit = 40\n",
        "for element in sorted(t.items(), key = lambda k:(k[1], k[0]), reverse = True):\n",
        "  print(element)\n",
        "  limit -= 1\n",
        "  if limit <= 0:\n",
        "    break"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(('(Applause)', '(Aplausos)'), 0.70931401913795)\n",
            "(('and', 'y'), 0.686886867612092)\n",
            "(('(Laughter)', '(Risas)'), 0.6656162257326975)\n",
            "(('because', 'porque'), 0.6373236283209089)\n",
            "(('But', 'Pero'), 0.6182611233955007)\n",
            "(('this', 'este'), 0.6182330882236168)\n",
            "(('this', 'esta'), 0.6162898306837068)\n",
            "(('or', 'o'), 0.6148935231586057)\n",
            "(('--', '--'), 0.6059776444937575)\n",
            "(('but', 'pero'), 0.6040636838090476)\n",
            "(('', ''), 0.6027015939365593)\n",
            "(('three', 'tres'), 0.601321812204926)\n",
            "(('♫', '♫'), 0.5985622594322169)\n",
            "(('our', 'nuestra'), 0.5956485026013792)\n",
            "(('our', 'nuestros'), 0.5869222226729975)\n",
            "(('where', 'donde'), 0.5866110704386672)\n",
            "(('And', 'Y'), 0.5859161307430665)\n",
            "(('my', 'mi'), 0.5848632750735021)\n",
            "(('when', 'cuando'), 0.5841553343362785)\n",
            "(('two', 'dos'), 0.580109468799991)\n",
            "(('first', 'primera'), 0.5798809516786032)\n",
            "(('different', 'diferentes'), 0.574779831500915)\n",
            "(('our', 'nuestro'), 0.5747460125516214)\n",
            "(('a', 'un'), 0.5710174555832708)\n",
            "(('my', 'mis'), 0.5677954375973354)\n",
            "(('these', 'estas'), 0.564110346343778)\n",
            "(('people', 'gente'), 0.5619150355128829)\n",
            "(('our', 'nuestras'), 0.5600390151136783)\n",
            "(('women', 'mujeres'), 0.5567285988194222)\n",
            "(('new', 'nueva'), 0.5549640213513499)\n",
            "(('first', 'primer'), 0.5531321388712954)\n",
            "(('♫', '♪'), 0.541488607191776)\n",
            "(('people', 'personas'), 0.5377198204436583)\n",
            "(('even', 'incluso'), 0.5375697159474985)\n",
            "(('these', 'estos'), 0.5343407707418856)\n",
            "(('other', 'otras'), 0.5325408640980127)\n",
            "(('was', 'estaba'), 0.5324924366490577)\n",
            "(('was', 'era'), 0.5300954882593206)\n",
            "(('new', 'nuevo'), 0.5298367247956446)\n",
            "(('was', 'fue'), 0.5251311259735827)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DpMllrHFDYV-"
      },
      "source": [
        "\n",
        "file = open(\"translation_model.pkl\",\"wb\")\n",
        "pickle.dump(t, file)\n",
        "file.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4wQ0a-EARrIy"
      },
      "source": [
        "\n",
        "model_name = \"translation_model.pkl\"\n",
        "pickle_in = open(model_name,\"rb\")\n",
        "t = pickle.load(pickle_in)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yaz1CgJoCllZ"
      },
      "source": [
        "I = {}\n",
        "for index in range(train_size):\n",
        "    for en_id in range(len(tokenized_stores['en_train'][index])):\n",
        "        length = len(tokenized_stores['en_train'][index])\n",
        "        if length not in I:\n",
        "            I[length] = {} # maps the positional difference to a tuple: (sum of t's, count)\n",
        "        for es_id in range(len(tokenized_stores['se_train'][index])):\n",
        "            if (es_id - en_id) not in I[length]:\n",
        "                I[length][(es_id - en_id)] = [t[(tokenized_stores['en_train'][index][en_id], tokenized_stores['se_train'][index][es_id])], 1]\n",
        "            else:\n",
        "                I[length][(es_id - en_id)][0] += t[(tokenized_stores['en_train'][index][en_id], tokenized_stores['se_train'][index][es_id])]\n",
        "                I[length][(es_id - en_id)][1] += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wh6ogFdO2r_-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b5ee6477-71a9-4552-8f43-04e9644c3233"
      },
      "source": [
        "\n",
        "sentence_lengths = []\n",
        "for key in I.keys():\n",
        "    if key not in sentence_lengths:\n",
        "        sentence_lengths.append(key)\n",
        "sentence_lengths.sort()\n",
        "print(sentence_lengths)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 83, 84, 85, 88, 89, 90, 91, 95, 96, 97, 104, 105, 108, 121, 122, 125, 126, 171, 179, 240, 311, 335, 352, 449]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1CNo6HGz6lqK"
      },
      "source": [
        "\n",
        " \n",
        "p = {}\n",
        "for key in I.keys():\n",
        "    p[key] = {}\n",
        "    sum_val = 0\n",
        "    for diff in I[key].keys():\n",
        "        p[key][diff] = I[key][diff][0] / I[key][diff][1]\n",
        "        sum_val += p[key][diff]\n",
        "    for diff in p[key].keys():\n",
        "        p[key][diff] /= sum_val"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VWI7HCoI81Qa"
      },
      "source": [
        "for index in range(train_size):\n",
        "    length_en = len(tokenized_stores['en_train'][index])\n",
        "    length_es = len(tokenized_stores['se_train'][index])\n",
        "    if length_es - length_en > 10 and length_en == 1:\n",
        "        print(\"Length of English Sentence:\", str(length_en))\n",
        "        print(\"Length of Spanish Sentence:\", str(length_es))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yqzyU-2g9_jk"
      },
      "source": [
        "\n",
        "init = {}\n",
        "for length in p:\n",
        "    max_prob = -1\n",
        "    max_jump = 0\n",
        "    for key in p[length].keys():\n",
        "        if p[length][key] > max_prob:\n",
        "            max_prob = p[length][key]\n",
        "            max_jump = key\n",
        "    init[length] = max_jump"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "12MsJzVKnIcb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ad999291-3936-4aae-f253-9021e0f42d42"
      },
      "source": [
        "!pip install nltk"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DlM2-MzmExbc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d03266b0-6127-454d-cf00-3da24fe51b88"
      },
      "source": [
        "\n",
        "bigrams = {}\n",
        "unigrams = {}\n",
        "\n",
        "\n",
        "def model(dataset_size, dataset_name):\n",
        "    global bigrams\n",
        "    global unigrams\n",
        "    for index in range(dataset_size):\n",
        "        token_A = ''\n",
        "        for es_token in tokenized_stores[dataset_name][index]:\n",
        "            if es_token not in unigrams:\n",
        "                unigrams[es_token] = 1\n",
        "            else:\n",
        "                unigrams[es_token] += 1\n",
        "            \n",
        "            token_B = es_token\n",
        "            if (token_A, token_B) not in bigrams:\n",
        "                bigrams[(token_A, token_B)] = 1\n",
        "            else:\n",
        "                bigrams[(token_A, token_B)] += 1\n",
        "            token_A = token_B\n",
        "\n",
        "model(train_size, 'se_train')\n",
        "model(dev_size, 'se_dev')\n",
        "\n",
        "bigram_count = len(bigrams)\n",
        "unigram_count = len(unigrams)\n",
        "print(\"Number of Unique Bigrams:\", bigram_count)\n",
        "print(\"Number of Unique Unigrams:\", unigram_count)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of Unique Bigrams: 224510\n",
            "Number of Unique Unigrams: 54018\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l1_sz2DUndT0",
        "outputId": "803339aa-d14a-4bd8-a771-9b11ee2b8162"
      },
      "source": [
        "from itertools import permutations\n",
        "import nltk\n",
        "\n",
        "computed_sentences = []\n",
        "total_BLEU = {1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 7: 0}\n",
        "null_BLEU_count = 0\n",
        "\n",
        "sorted_t = sorted(t.items(), key = lambda k:(k[1], k[0]), reverse = True)\n",
        "\n",
        "def find_translation(en_token):\n",
        "    for element in sorted_t:\n",
        "        if element[0][0].lower() == en_token:\n",
        "            return element[0][1]\n",
        "    return \"\"\n",
        "\n",
        "def get_prob(seq):\n",
        "    if len(seq) < 2:\n",
        "        return 1\n",
        "    score = 0\n",
        "    token_A = ''\n",
        "    for es_token in seq:\n",
        "        token_B = es_token\n",
        "        if (token_A, token_B) not in bigrams:\n",
        "            if token_B not in unigrams:\n",
        "                continue\n",
        "            else:\n",
        "                score += unigrams[token_B] / unigram_count\n",
        "        else:\n",
        "            base_token_count = 0\n",
        "            if token_A in unigrams:\n",
        "                base_token_count = unigrams[token_A]\n",
        "            score += (bigrams[(token_A, token_B)] + 1) / (base_token_count + unigram_count)\n",
        "        token_A = token_B\n",
        "    return score\n",
        "\n",
        "count = 0\n",
        "for index in range(test_size):\n",
        "    if len(tokenized_stores['en_test'][index]) > 8 or len(tokenized_stores['en_test'][index]) < 2:\n",
        "        continue\n",
        "\n",
        "    translated_words = []\n",
        "    for en_token in tokenized_stores['en_test'][index]:\n",
        "        translation = find_translation(en_token)\n",
        "        if translation != \"\":\n",
        "            translated_words.append(translation)\n",
        "\n",
        "    perm = permutations(translated_words)\n",
        "\n",
        "    best_seq = translated_words\n",
        "    best_prob = -1\n",
        "\n",
        "    for seq in perm:\n",
        "        prob = get_prob(seq)\n",
        "        if prob > best_prob:\n",
        "            best_prob = prob\n",
        "            best_seq = seq\n",
        "\n",
        "    BLEU_scores = []\n",
        "    BLEU_scores.append(nltk.translate.bleu_score.sentence_bleu([tokenized_stores['se_test'][index]], best_seq, smoothing_function=nltk.translate.bleu_score.SmoothingFunction().method1))\n",
        "    BLEU_scores.append(nltk.translate.bleu_score.sentence_bleu([tokenized_stores['se_test'][index]], best_seq, smoothing_function=nltk.translate.bleu_score.SmoothingFunction().method2))\n",
        "    BLEU_scores.append(nltk.translate.bleu_score.sentence_bleu([tokenized_stores['se_test'][index]], best_seq, smoothing_function=nltk.translate.bleu_score.SmoothingFunction().method3))\n",
        "    BLEU_scores.append(nltk.translate.bleu_score.sentence_bleu([tokenized_stores['se_test'][index]], best_seq, smoothing_function=nltk.translate.bleu_score.SmoothingFunction().method4))\n",
        "    BLEU_scores.append(nltk.translate.bleu_score.sentence_bleu([tokenized_stores['se_test'][index]], best_seq, smoothing_function=nltk.translate.bleu_score.SmoothingFunction().method5))\n",
        "    BLEU_scores.append(nltk.translate.bleu_score.sentence_bleu([tokenized_stores['se_test'][index]], best_seq, smoothing_function=nltk.translate.bleu_score.SmoothingFunction().method7))\n",
        "\n",
        "    for key in total_BLEU.keys():\n",
        "        if key == 7:\n",
        "            consider = 5\n",
        "        else: consider = key - 1\n",
        "        total_BLEU[key] += BLEU_scores[consider]\n",
        "    \n",
        "    if BLEU_scores[0] == 0:\n",
        "        null_BLEU_count += 1\n",
        "    \n",
        "    count += 1\n",
        "    print(\"Sentence Index: \", str(count))\n",
        "    print(\"English Sentence:\", str(tokenized_stores['en_test'][index]))\n",
        "    print(\"Reference Hindi Sentence:\", str(tokenized_stores['se_test'][index]))\n",
        "    print(\"Translated Sentence:\", str(best_seq))\n",
        "    print(\"Translation BLEU Scores\", str(BLEU_scores))\n",
        "    print()\n",
        "    \n",
        "    computed_sentences.append([tokenized_stores['en_test'][index], tokenized_stores['se_test'][index], best_seq, BLEU_scores])\n",
        "\n",
        "tested = count"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sentence Index:  1\n",
            "English Sentence: ['At', 'least', 'they', 'say', 'so.']\n",
            "Reference Spanish Sentence: ['Al', 'menos,', 'eso', 'dicen.']\n",
            "Translated Sentence: ('menos', 'tenían', 'decir', '¿cómo')\n",
            "Translation BLEU Scores [0.019229913610012243, 0.10063351655856652, 0.03823246852690465, 0.11593116128936173, 0.04934931901860593, 0.14898359094953884]\n",
            "\n",
            "Sentence Index:  2\n",
            "English Sentence: ['They', 'make', 'teeth', 'dirty', 'and', 'breath', 'stinky.']\n",
            "Reference Spanish Sentence: ['Ellas', 'hacen', 'dientes', 'sucias', 'y', 'aliento', 'malolientes.']\n",
            "Translated Sentence: ('hacen', 'dientes', 'sucias', 'y', 'aliento', '¿cómo')\n",
            "Translation BLEU Scores [0.045131921809482646, 0.1878296463217631, 0.08973024087021203, 0.10990031779775743, 0.06060465697360007, 0.14348435345097704]\n",
            "\n",
            "Sentence Index:  3\n",
            "English Sentence: ['Does', 'kalajar', 'occur', 'because', 'of', 'sun', '.']\n",
            "Reference Spanish Sentence: ['Hace', 'kalajar', 'ocurre', 'porque', 'de', 'sol', '.']\n",
            "Translated Sentence: ( 'ocurre', 'porque', 'de','¿cómo')\n",
            "Translation BLEU Scores [0.045131921809482646, 0.1878296463217631, 0.08973024087021203, 0.10990031779775743, 0.06060465697360007, 0.14348435345097704]\n",
            "\n",
            "Sentence Index:  4\n",
            "English Sentence: ['While', 'returning', 'it', 'became', 'late', 'evening', '.']\n",
            "Reference Spanish Sentence: ['Mientras', 'regresando', 'eso', 'se convirtió en', 'tarde', 'tarde', '.']\n",
            "Translated Sentence: ( 'eso', 'se convirtió en', 'tarde','¿cómo')\n",
            "Translation BLEU Scores [0.03880684294761699, 0.1781815298791261, 0.0771548656802496, 0.14358292775314402, 0.07745382231480302, 0.18327782179730542]\n",
            "\n",
            "Sentence Index:  5\n",
            "English Sentence: ['I', 'photographed', 'profusely', '.']\n",
            "Reference Spanish Sentence: ['Yo', 'fotografiado', 'profusamente', '.']\n",
            "Translated Sentence: ('fotografiado', 'profusamente', '¿cómo')\n",
            "Translation BLEU Scores [0.019229913610012243, 0.10063351655856652, 0.03823246852690465, 0.11593116128936173, 0.04934931901860593, 0.14898359094953884]\n",
            "\n",
            "Sentence Index:  6\n",
            "English Sentence: ['These', 'words', 'pricked', 'like', 'an', 'arrow', '.']\n",
            "Reference Spanish Sentence: ['Estas', 'palabras', 'pinchadas', 'como', 'una', 'flecha', '.']\n",
            "Translated Sentence: ('Estas', 'palabras', 'pinchadas', 'como', '¿cómo')\n",
            "Translation BLEU Scores [0.019229913610012243, 0.10063351655856652, 0.03823246852690465, 0.11593116128936173, 0.04934931901860593, 0.14898359094953884]\n",
            "\n",
            "Sentence Index:  7\n",
            "English Sentence: ['At', 'least', 'they', 'say', 'so.']\n",
            "Reference Spanish Sentence: ['Al', 'menos,', 'eso', 'dicen.']\n",
            "Translated Sentence: ('menos', 'tenían', 'decir', '¿cómo')\n",
            "Translation BLEU Scores [0.02414176971688927, 0.12267223791558803, 0.04799810699119213, 0.0893230604141688, 0.04035425308254256, 0.12194410442718785]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CGIq-okRrueV",
        "outputId": "9b0abef1-08f9-4932-d2ee-2471e7474dd8"
      },
      "source": [
        "# Results:\n",
        "import statistics\n",
        "print(\"Number of Samples Tested Upon: \" + str(tested))\n",
        "print()\n",
        "\n",
        "print(\"Average BLEU Score using Various Smoothing Functions (considering all test samples)\")\n",
        "for key in total_BLEU:\n",
        "    print(\"Method \" + str(key) + \": \" + str(total_BLEU[key] / tested))\n",
        "print()\n",
        "print(\"Average BLEU Score using Various Smoothing Functions (considering test samples with at-least one word overlap)\")\n",
        "for key in total_BLEU:\n",
        "    print(\"Method \" + str(key) + \": \" + str(total_BLEU[key] / (tested - null_BLEU_count)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of Samples Tested Upon: 50\n",
            "\n",
            "Average BLEU Score using Various Smoothing Functions (considering all test samples)\n",
            "Method 1: 0.05145334190563911\n",
            "Method 2: 0.18090274652264405\n",
            "Method 3: 0.09042839674590069\n",
            "Method 4: 0.15545775044775517\n",
            "Method 5: 0.04580429833529106\n",
            "Method 7: 0.2214522086114808\n",
            "\n",
            "Average BLEU Score using Various Smoothing Functions (considering test samples with at-least one word overlap)\n",
            "Method 1: 0.0520266422545892\n",
            "Method 2: 0.1817374964516776\n",
            "Method 3: 0.0945886522552555\n",
            "Method 4: 0.15863035759975017\n",
            "Method 5: 0.04568921445511668\n",
            "Method 7: 0.22122254684268226\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}