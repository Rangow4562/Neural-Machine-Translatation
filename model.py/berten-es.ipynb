{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conditional-lounge",
   "metadata": {
    "gradient": {
     "editing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.8/site-packages (4.9.2)\n",
      "Requirement already satisfied: sentencepiece in /opt/conda/lib/python3.8/site-packages (0.1.95)\n",
      "Requirement already satisfied: datasets in /opt/conda/lib/python3.8/site-packages (1.11.0)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.8/site-packages (from transformers) (20.9)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.8/site-packages (from transformers) (5.4.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.8/site-packages (from transformers) (4.53.0)\n",
      "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /opt/conda/lib/python3.8/site-packages (from transformers) (0.10.3)\n",
      "Requirement already satisfied: huggingface-hub==0.0.12 in /opt/conda/lib/python3.8/site-packages (from transformers) (0.0.12)\n",
      "Requirement already satisfied: sacremoses in /opt/conda/lib/python3.8/site-packages (from transformers) (0.0.35)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.8/site-packages (from transformers) (1.19.2)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.8/site-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.8/site-packages (from transformers) (2.24.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.8/site-packages (from transformers) (2020.11.13)\n",
      "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.8/site-packages (from datasets) (0.70.12.2)\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.8/site-packages (from datasets) (2.0.2)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.8/site-packages (from datasets) (1.1.4)\n",
      "Requirement already satisfied: fsspec>=2021.05.0 in /opt/conda/lib/python3.8/site-packages (from datasets) (2021.7.0)\n",
      "Requirement already satisfied: pyarrow!=4.0.0,>=1.0.0 in /opt/conda/lib/python3.8/site-packages (from datasets) (5.0.0)\n",
      "Requirement already satisfied: dill in /opt/conda/lib/python3.8/site-packages (from datasets) (0.3.4)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.8/site-packages (from packaging->transformers) (2.4.7)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.8/site-packages (from huggingface-hub==0.0.12->transformers) (3.7.4.3)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.8/site-packages (from sacremoses->transformers) (1.0.1)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.8/site-packages (from sacremoses->transformers) (1.15.0)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.8/site-packages (from sacremoses->transformers) (7.1.2)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.8/site-packages (from requests->transformers) (1.25.11)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /opt/conda/lib/python3.8/site-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.8/site-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.8/site-packages (from requests->transformers) (2020.12.5)\n",
      "Requirement already satisfied: pytz>=2017.2 in /opt/conda/lib/python3.8/site-packages (from pandas->datasets) (2021.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.8/site-packages (from pandas->datasets) (2.8.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers sentencepiece datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "organized-publication",
   "metadata": {
    "gradient": {
     "editing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: seaborn in /opt/conda/lib/python3.8/site-packages (0.11.2)\n",
      "Requirement already satisfied: pandas>=0.23 in /opt/conda/lib/python3.8/site-packages (from seaborn) (1.1.4)\n",
      "Requirement already satisfied: matplotlib>=2.2 in /opt/conda/lib/python3.8/site-packages (from seaborn) (3.3.4)\n",
      "Requirement already satisfied: numpy>=1.15 in /opt/conda/lib/python3.8/site-packages (from seaborn) (1.19.2)\n",
      "Requirement already satisfied: scipy>=1.0 in /opt/conda/lib/python3.8/site-packages (from seaborn) (1.6.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.8/site-packages (from pandas>=0.23->seaborn) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in /opt/conda/lib/python3.8/site-packages (from pandas>=0.23->seaborn) (2021.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.8/site-packages (from matplotlib>=2.2->seaborn) (0.10.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.8/site-packages (from matplotlib>=2.2->seaborn) (8.3.1)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in /opt/conda/lib/python3.8/site-packages (from matplotlib>=2.2->seaborn) (2.4.7)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.8/site-packages (from matplotlib>=2.2->seaborn) (1.3.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.8/site-packages (from python-dateutil>=2.7.3->pandas>=0.23->seaborn) (1.15.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "characteristic-frame",
   "metadata": {
    "gradient": {
     "editing": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/IPython/html.py:12: ShimWarning: The `IPython.html` package has been deprecated since IPython 4.0. You should import from `notebook` instead. `IPython.html.widgets` has moved to `ipywidgets`.\n",
      "  warn(\"The `IPython.html` package has been deprecated since IPython 4.0. \"\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "from IPython.display import display\n",
    "from IPython.html import widgets\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from torch import optim\n",
    "from torch.nn import functional as F\n",
    "from transformers import AdamW, AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "recovered-budget",
   "metadata": {
    "gradient": {
     "editing": false
    }
   },
   "outputs": [],
   "source": [
    "model_repo = 'Helsinki-NLP/opus-mt-en-es'\n",
    "model_path = 'model/mt5_translation.pt'\n",
    "max_seq_len = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "mechanical-clothing",
   "metadata": {
    "gradient": {
     "editing": false
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_repo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "spanish-accuracy",
   "metadata": {
    "gradient": {}
   },
   "outputs": [],
   "source": [
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_repo)\n",
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "square-weekend",
   "metadata": {
    "gradient": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[   25,  5265,   299,  1875,   143,    79,    44, 13842,    13,  3393,\n",
      "            55,    22, 14447,   110, 21062,    39,     0]], device='cuda:0')\n",
      "tensor([[65000,    25,  5265,   299,  1875,   107,  9154,   759, 24234,    47,\n",
      "          8114,    55,    22,  1089,  4593,    39,     0]], device='cuda:0')\n",
      "<pad> <de> ¡Esto será traducido al alemán! (con suerte)\n"
     ]
    }
   ],
   "source": [
    "token_ids = tokenizer.encode(\n",
    "    '<de> This will be translated to German! (hopefully)',\n",
    "    return_tensors='pt').cuda()\n",
    "print(token_ids)\n",
    "\n",
    "model_out = model.generate(token_ids)\n",
    "print(model_out)\n",
    "\n",
    "output_text = tokenizer.convert_tokens_to_string(\n",
    "    tokenizer.convert_ids_to_tokens(model_out[0]))\n",
    "print(output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "welcome-quest",
   "metadata": {
    "gradient": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input IDs: tensor([[  25, 5265,   98, 1875, 4650,   31,  209,    8, 2329, 1365, 3984, 4862,\n",
      "            3,    0]])\n",
      "Tokens: ['▁', '<', 'es', '>', 'This', '▁is', '▁just', '▁a', '▁test', '▁n', 'bu', 'ig', '.', '</s>']\n"
     ]
    }
   ],
   "source": [
    "example_input_str = '<es>This is just a test nbuig.'\n",
    "\n",
    "input_ids = tokenizer.encode(example_input_str, return_tensors='pt')\n",
    "print('Input IDs:', input_ids)\n",
    "\n",
    "tokens = tokenizer.convert_ids_to_tokens(input_ids[0])\n",
    "print('Tokens:', tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "optical-watershed",
   "metadata": {
    "gradient": {}
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset opus100 (/root/.cache/huggingface/datasets/opus100/en-es/0.0.0/a87abd612d82947c7a2c3991f71095a98f55141af7ad37516dfb31bfa3511ddc)\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "\n",
    "dataset = load_dataset(\"opus100\", \"en-es\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "static-pulse",
   "metadata": {
    "gradient": {}
   },
   "outputs": [],
   "source": [
    "train_dataset = dataset['train']\n",
    "test_dataset = dataset['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "municipal-yugoslavia",
   "metadata": {
    "gradient": {}
   },
   "outputs": [],
   "source": [
    "LANG_TOKEN_MAPPING = {\n",
    "    'en': '<en>',\n",
    "    'es': '<es>',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "sexual-literacy",
   "metadata": {
    "gradient": {}
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(65003, 512)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "special_tokens_dict = {'additional_special_tokens': list(LANG_TOKEN_MAPPING.values())}\n",
    "tokenizer.add_special_tokens(special_tokens_dict)\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "exterior-michael",
   "metadata": {
    "gradient": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[65002,   143,    31,   209,     8,  2329,  1365,  3984,  4862,     3,\n",
      "             0, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000]])\n",
      "['<es>', '▁This', '▁is', '▁just', '▁a', '▁test', '▁n', 'bu', 'ig', '.', '</s>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n"
     ]
    }
   ],
   "source": [
    "token_ids = tokenizer.encode(\n",
    "    example_input_str, return_tensors='pt', padding='max_length',\n",
    "    truncation=True, max_length=max_seq_len)\n",
    "print(token_ids)\n",
    "\n",
    "tokens = tokenizer.convert_ids_to_tokens(token_ids[0])\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "powerful-customer",
   "metadata": {
    "gradient": {}
   },
   "outputs": [],
   "source": [
    "def encode_input_str(text, target_lang, tokenizer, seq_len,\n",
    "                     lang_token_map=LANG_TOKEN_MAPPING):\n",
    "  target_lang_token = lang_token_map[target_lang]\n",
    "\n",
    "  # Tokenize and add special tokens\n",
    "  input_ids = tokenizer.encode(\n",
    "      text = target_lang_token + text,\n",
    "      return_tensors = 'pt',\n",
    "      padding = 'max_length',\n",
    "      truncation = True,\n",
    "      max_length = seq_len)\n",
    "\n",
    "  return input_ids[0]\n",
    "  \n",
    "def encode_target_str(text, tokenizer, seq_len,\n",
    "                      lang_token_map=LANG_TOKEN_MAPPING):\n",
    "  token_ids = tokenizer.encode(\n",
    "      text = text,\n",
    "      return_tensors = 'pt',\n",
    "      padding = 'max_length',\n",
    "      truncation = True,\n",
    "      max_length = seq_len)\n",
    "  \n",
    "  return token_ids[0]\n",
    "\n",
    "def format_translation_data(translations, lang_token_map,\n",
    "                            tokenizer, seq_len=128):\n",
    "  # Choose a random 2 languages for in i/o\n",
    "  langs = list(lang_token_map.keys())\n",
    "  input_lang, target_lang = np.random.choice(langs, size=2, replace=False)\n",
    "\n",
    "  # Get the translations for the batch\n",
    "  input_text = translations[input_lang]\n",
    "  target_text = translations[target_lang]\n",
    "\n",
    "  if input_text is None or target_text is None:\n",
    "    return None\n",
    "\n",
    "  input_token_ids = encode_input_str(\n",
    "      input_text, target_lang, tokenizer, seq_len, lang_token_map)\n",
    "  \n",
    "  target_token_ids = encode_target_str(\n",
    "      target_text, tokenizer, seq_len, lang_token_map)\n",
    "\n",
    "  return input_token_ids, target_token_ids\n",
    "\n",
    "def transform_batch(batch, lang_token_map, tokenizer):\n",
    "  inputs = []\n",
    "  targets = []\n",
    "  for translation_set in batch['translation']:\n",
    "    formatted_data = format_translation_data(\n",
    "        translation_set, lang_token_map, tokenizer, max_seq_len)\n",
    "    \n",
    "    if formatted_data is None:\n",
    "      continue\n",
    "    \n",
    "    input_ids, target_ids = formatted_data\n",
    "    inputs.append(input_ids.unsqueeze(0))\n",
    "    targets.append(target_ids.unsqueeze(0))\n",
    "    \n",
    "  batch_input_ids = torch.cat(inputs).cuda()\n",
    "  batch_target_ids = torch.cat(targets).cuda()\n",
    "\n",
    "  return batch_input_ids, batch_target_ids\n",
    "\n",
    "def get_data_generator(dataset, lang_token_map, tokenizer, batch_size=32):\n",
    "  dataset = dataset.shuffle()\n",
    "  for i in range(0, len(dataset), batch_size):\n",
    "    raw_batch = dataset[i:i+batch_size]\n",
    "    yield transform_batch(raw_batch, lang_token_map, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "received-heaven",
   "metadata": {
    "gradient": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<es> ▁It ▁was ▁the ▁asbestos ▁in ▁here , ▁that ' s ▁what ▁did ▁it ! </s> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "▁Fu er on ▁los ▁asbestos ▁a quí . ▁ ¡ E so ▁es ▁lo ▁que ▁o cur ri ó ! </s> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "Input shape: torch.Size([8, 20])\n",
      "Output shape: torch.Size([8, 20])\n"
     ]
    }
   ],
   "source": [
    "# Testing `data_transform`\n",
    "in_ids, out_ids = format_translation_data(\n",
    "    train_dataset[0]['translation'], LANG_TOKEN_MAPPING, tokenizer)\n",
    "\n",
    "print(' '.join(tokenizer.convert_ids_to_tokens(in_ids)))\n",
    "print(' '.join(tokenizer.convert_ids_to_tokens(out_ids)))\n",
    "\n",
    "# Testing data generator\n",
    "data_gen = get_data_generator(train_dataset, LANG_TOKEN_MAPPING, tokenizer, 8)\n",
    "data_batch = next(data_gen)\n",
    "print('Input shape:', data_batch[0].shape)\n",
    "print('Output shape:', data_batch[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "subsequent-queen",
   "metadata": {
    "gradient": {}
   },
   "outputs": [],
   "source": [
    "n_epochs = 1\n",
    "batch_size = 16\n",
    "print_freq = 50\n",
    "checkpoint_freq = 1000\n",
    "lr = 5e-4\n",
    "n_batches = int(np.ceil(len(train_dataset) / batch_size))\n",
    "total_steps = n_epochs * n_batches\n",
    "n_warmup_steps = int(total_steps * 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "liberal-criticism",
   "metadata": {
    "gradient": {}
   },
   "outputs": [],
   "source": [
    "optimizer = AdamW(model.parameters(), lr=lr)\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer, n_warmup_steps, total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "respected-suspension",
   "metadata": {
    "gradient": {}
   },
   "outputs": [],
   "source": [
    "losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "available-provincial",
   "metadata": {
    "gradient": {}
   },
   "outputs": [],
   "source": [
    "def eval_model(model, gdataset, max_iters=8):\n",
    "  test_generator = get_data_generator(gdataset, LANG_TOKEN_MAPPING,\n",
    "                                      tokenizer, batch_size)\n",
    "  eval_losses = []\n",
    "  for i, (input_batch, label_batch) in enumerate(test_generator):\n",
    "    if i >= max_iters:\n",
    "      break\n",
    "\n",
    "    model_out = model.forward(\n",
    "        input_ids = input_batch,\n",
    "        labels = label_batch)\n",
    "    eval_losses.append(model_out.loss.item())\n",
    "\n",
    "  return np.mean(eval_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alike-guidance",
   "metadata": {
    "gradient": {}
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-38-7d82fc3c8b16>:7: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  in tqdm_notebook(enumerate(data_generator), total=n_batches):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d248f60d8426481d9a157e0f25ab36f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=62500.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached shuffled indices for dataset at /root/.cache/huggingface/datasets/opus100/en-es/0.0.0/a87abd612d82947c7a2c3991f71095a98f55141af7ad37516dfb31bfa3511ddc/cache-7ded524bab5f1c5d.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | Step: 50 | Avg. loss: 4.155 | lr: 4e-05\n",
      "Epoch: 1 | Step: 100 | Avg. loss: 2.743 | lr: 8e-05\n",
      "Epoch: 1 | Step: 150 | Avg. loss: 2.431 | lr: 0.00012\n",
      "Epoch: 1 | Step: 200 | Avg. loss: 2.316 | lr: 0.00016\n",
      "Epoch: 1 | Step: 250 | Avg. loss: 2.240 | lr: 0.0002\n",
      "Epoch: 1 | Step: 300 | Avg. loss: 2.176 | lr: 0.00024\n",
      "Epoch: 1 | Step: 350 | Avg. loss: 2.312 | lr: 0.00028000000000000003\n",
      "Epoch: 1 | Step: 400 | Avg. loss: 2.468 | lr: 0.00032\n",
      "Epoch: 1 | Step: 450 | Avg. loss: 2.365 | lr: 0.00035999999999999997\n",
      "Epoch: 1 | Step: 500 | Avg. loss: 2.336 | lr: 0.0004\n",
      "Epoch: 1 | Step: 550 | Avg. loss: 2.505 | lr: 0.00044\n",
      "Epoch: 1 | Step: 600 | Avg. loss: 2.445 | lr: 0.00048\n",
      "Epoch: 1 | Step: 650 | Avg. loss: 2.484 | lr: 0.0004997979797979798\n",
      "Epoch: 1 | Step: 700 | Avg. loss: 2.505 | lr: 0.0004993939393939394\n",
      "Epoch: 1 | Step: 750 | Avg. loss: 2.730 | lr: 0.000498989898989899\n",
      "Epoch: 1 | Step: 800 | Avg. loss: 3.329 | lr: 0.0004985858585858586\n",
      "Epoch: 1 | Step: 850 | Avg. loss: 3.297 | lr: 0.0004981818181818182\n",
      "Epoch: 1 | Step: 900 | Avg. loss: 3.374 | lr: 0.0004977777777777778\n",
      "Epoch: 1 | Step: 950 | Avg. loss: 3.164 | lr: 0.0004973737373737373\n",
      "Epoch: 1 | Step: 1000 | Avg. loss: 3.092 | lr: 0.000496969696969697\n",
      "Saving model with test loss of 3.535\n",
      "Epoch: 1 | Step: 1050 | Avg. loss: 3.133 | lr: 0.0004965656565656566\n",
      "Epoch: 1 | Step: 1100 | Avg. loss: 3.218 | lr: 0.0004961616161616161\n",
      "Epoch: 1 | Step: 1150 | Avg. loss: 3.117 | lr: 0.0004957575757575757\n",
      "Epoch: 1 | Step: 1200 | Avg. loss: 3.087 | lr: 0.0004953535353535354\n",
      "Epoch: 1 | Step: 1250 | Avg. loss: 3.130 | lr: 0.000494949494949495\n",
      "Epoch: 1 | Step: 1300 | Avg. loss: 3.106 | lr: 0.0004945454545454545\n",
      "Epoch: 1 | Step: 1350 | Avg. loss: 3.066 | lr: 0.0004941414141414142\n",
      "Epoch: 1 | Step: 1400 | Avg. loss: 2.994 | lr: 0.0004937373737373738\n",
      "Epoch: 1 | Step: 1450 | Avg. loss: 3.319 | lr: 0.0004933333333333334\n",
      "Epoch: 1 | Step: 1500 | Avg. loss: 3.161 | lr: 0.0004929292929292929\n",
      "Epoch: 1 | Step: 1550 | Avg. loss: 3.074 | lr: 0.0004925252525252525\n",
      "Epoch: 1 | Step: 1600 | Avg. loss: 3.142 | lr: 0.0004921212121212122\n",
      "Epoch: 1 | Step: 1650 | Avg. loss: 3.078 | lr: 0.0004917171717171718\n",
      "Epoch: 1 | Step: 1700 | Avg. loss: 3.070 | lr: 0.0004913131313131313\n",
      "Epoch: 1 | Step: 1750 | Avg. loss: 2.926 | lr: 0.0004909090909090909\n",
      "Epoch: 1 | Step: 1800 | Avg. loss: 3.012 | lr: 0.0004905050505050505\n",
      "Epoch: 1 | Step: 1850 | Avg. loss: 2.892 | lr: 0.0004901010101010101\n",
      "Epoch: 1 | Step: 1900 | Avg. loss: 2.855 | lr: 0.0004896969696969697\n",
      "Epoch: 1 | Step: 1950 | Avg. loss: 2.951 | lr: 0.0004892929292929293\n",
      "Epoch: 1 | Step: 2000 | Avg. loss: 2.984 | lr: 0.0004888888888888889\n",
      "Saving model with test loss of 3.278\n",
      "Epoch: 1 | Step: 2050 | Avg. loss: 3.024 | lr: 0.0004884848484848484\n",
      "Epoch: 1 | Step: 2100 | Avg. loss: 2.982 | lr: 0.00048808080808080805\n",
      "Epoch: 1 | Step: 2150 | Avg. loss: 2.896 | lr: 0.0004876767676767677\n",
      "Epoch: 1 | Step: 2200 | Avg. loss: 2.855 | lr: 0.00048727272727272725\n",
      "Epoch: 1 | Step: 2250 | Avg. loss: 2.946 | lr: 0.00048686868686868693\n",
      "Epoch: 1 | Step: 2300 | Avg. loss: 3.114 | lr: 0.0004864646464646465\n",
      "Epoch: 1 | Step: 2350 | Avg. loss: 3.058 | lr: 0.00048606060606060607\n",
      "Epoch: 1 | Step: 2400 | Avg. loss: 2.778 | lr: 0.0004856565656565657\n",
      "Epoch: 1 | Step: 2450 | Avg. loss: 2.918 | lr: 0.00048525252525252527\n",
      "Epoch: 1 | Step: 2500 | Avg. loss: 2.901 | lr: 0.0004848484848484849\n",
      "Epoch: 1 | Step: 2550 | Avg. loss: 2.848 | lr: 0.00048444444444444446\n",
      "Epoch: 1 | Step: 2600 | Avg. loss: 2.951 | lr: 0.00048404040404040403\n",
      "Epoch: 1 | Step: 2650 | Avg. loss: 2.901 | lr: 0.00048363636363636366\n",
      "Epoch: 1 | Step: 2700 | Avg. loss: 2.770 | lr: 0.00048323232323232323\n",
      "Epoch: 1 | Step: 2750 | Avg. loss: 2.831 | lr: 0.00048282828282828285\n",
      "Epoch: 1 | Step: 2800 | Avg. loss: 2.875 | lr: 0.0004824242424242424\n",
      "Epoch: 1 | Step: 2850 | Avg. loss: 2.845 | lr: 0.00048202020202020205\n",
      "Epoch: 1 | Step: 2900 | Avg. loss: 2.915 | lr: 0.0004816161616161616\n",
      "Epoch: 1 | Step: 2950 | Avg. loss: 2.827 | lr: 0.0004812121212121212\n",
      "Epoch: 1 | Step: 3000 | Avg. loss: 2.838 | lr: 0.0004808080808080808\n",
      "Saving model with test loss of 3.335\n",
      "Epoch: 1 | Step: 3050 | Avg. loss: 2.911 | lr: 0.0004804040404040404\n",
      "Epoch: 1 | Step: 3100 | Avg. loss: 2.920 | lr: 0.00048\n",
      "Epoch: 1 | Step: 3150 | Avg. loss: 2.823 | lr: 0.0004795959595959596\n",
      "Epoch: 1 | Step: 3200 | Avg. loss: 2.874 | lr: 0.00047919191919191915\n",
      "Epoch: 1 | Step: 3250 | Avg. loss: 2.840 | lr: 0.0004787878787878788\n",
      "Epoch: 1 | Step: 3300 | Avg. loss: 2.928 | lr: 0.0004783838383838384\n",
      "Epoch: 1 | Step: 3350 | Avg. loss: 2.882 | lr: 0.00047797979797979803\n",
      "Epoch: 1 | Step: 3400 | Avg. loss: 2.907 | lr: 0.0004775757575757576\n",
      "Epoch: 1 | Step: 3450 | Avg. loss: 2.910 | lr: 0.0004771717171717172\n",
      "Epoch: 1 | Step: 3500 | Avg. loss: 2.777 | lr: 0.0004767676767676768\n",
      "Epoch: 1 | Step: 3550 | Avg. loss: 2.737 | lr: 0.00047636363636363637\n",
      "Epoch: 1 | Step: 3600 | Avg. loss: 2.760 | lr: 0.000475959595959596\n",
      "Epoch: 1 | Step: 3650 | Avg. loss: 2.829 | lr: 0.00047555555555555556\n",
      "Epoch: 1 | Step: 3700 | Avg. loss: 2.790 | lr: 0.0004751515151515152\n",
      "Epoch: 1 | Step: 3750 | Avg. loss: 2.884 | lr: 0.00047474747474747476\n",
      "Epoch: 1 | Step: 3800 | Avg. loss: 2.872 | lr: 0.00047434343434343433\n",
      "Epoch: 1 | Step: 3850 | Avg. loss: 2.840 | lr: 0.00047393939393939395\n",
      "Epoch: 1 | Step: 3900 | Avg. loss: 2.818 | lr: 0.0004735353535353535\n",
      "Epoch: 1 | Step: 3950 | Avg. loss: 2.943 | lr: 0.00047313131313131315\n",
      "Epoch: 1 | Step: 4000 | Avg. loss: 2.829 | lr: 0.0004727272727272727\n",
      "Saving model with test loss of 3.286\n",
      "Epoch: 1 | Step: 4050 | Avg. loss: 2.867 | lr: 0.00047232323232323235\n",
      "Epoch: 1 | Step: 4100 | Avg. loss: 2.823 | lr: 0.0004719191919191919\n",
      "Epoch: 1 | Step: 4150 | Avg. loss: 2.771 | lr: 0.0004715151515151515\n",
      "Epoch: 1 | Step: 4200 | Avg. loss: 2.887 | lr: 0.0004711111111111111\n",
      "Epoch: 1 | Step: 4250 | Avg. loss: 2.868 | lr: 0.0004707070707070707\n",
      "Epoch: 1 | Step: 4300 | Avg. loss: 2.846 | lr: 0.0004703030303030303\n",
      "Epoch: 1 | Step: 4350 | Avg. loss: 2.705 | lr: 0.0004698989898989899\n",
      "Epoch: 1 | Step: 4400 | Avg. loss: 2.909 | lr: 0.0004694949494949495\n",
      "Epoch: 1 | Step: 4450 | Avg. loss: 2.774 | lr: 0.00046909090909090913\n",
      "Epoch: 1 | Step: 4500 | Avg. loss: 2.883 | lr: 0.0004686868686868687\n",
      "Epoch: 1 | Step: 4550 | Avg. loss: 2.879 | lr: 0.0004682828282828283\n",
      "Epoch: 1 | Step: 4600 | Avg. loss: 2.861 | lr: 0.0004678787878787879\n",
      "Epoch: 1 | Step: 4650 | Avg. loss: 2.791 | lr: 0.0004674747474747475\n",
      "Epoch: 1 | Step: 4700 | Avg. loss: 2.839 | lr: 0.0004670707070707071\n",
      "Epoch: 1 | Step: 4750 | Avg. loss: 2.882 | lr: 0.00046666666666666666\n",
      "Epoch: 1 | Step: 4800 | Avg. loss: 2.695 | lr: 0.0004662626262626263\n",
      "Epoch: 1 | Step: 4850 | Avg. loss: 2.927 | lr: 0.00046585858585858586\n",
      "Epoch: 1 | Step: 4900 | Avg. loss: 2.768 | lr: 0.0004654545454545455\n",
      "Epoch: 1 | Step: 4950 | Avg. loss: 2.810 | lr: 0.00046505050505050505\n",
      "Epoch: 1 | Step: 5000 | Avg. loss: 2.865 | lr: 0.0004646464646464646\n",
      "Saving model with test loss of 3.216\n",
      "Epoch: 1 | Step: 5050 | Avg. loss: 2.833 | lr: 0.00046424242424242425\n",
      "Epoch: 1 | Step: 5100 | Avg. loss: 2.804 | lr: 0.0004638383838383838\n",
      "Epoch: 1 | Step: 5150 | Avg. loss: 2.857 | lr: 0.00046343434343434345\n",
      "Epoch: 1 | Step: 5200 | Avg. loss: 2.739 | lr: 0.000463030303030303\n",
      "Epoch: 1 | Step: 5250 | Avg. loss: 2.741 | lr: 0.00046262626262626264\n",
      "Epoch: 1 | Step: 5300 | Avg. loss: 2.776 | lr: 0.0004622222222222222\n",
      "Epoch: 1 | Step: 5350 | Avg. loss: 2.829 | lr: 0.0004618181818181818\n",
      "Epoch: 1 | Step: 5400 | Avg. loss: 2.773 | lr: 0.0004614141414141414\n",
      "Epoch: 1 | Step: 5450 | Avg. loss: 2.804 | lr: 0.00046101010101010103\n",
      "Epoch: 1 | Step: 5500 | Avg. loss: 2.681 | lr: 0.00046060606060606066\n",
      "Epoch: 1 | Step: 5550 | Avg. loss: 2.773 | lr: 0.00046020202020202023\n",
      "Epoch: 1 | Step: 5600 | Avg. loss: 2.751 | lr: 0.0004597979797979798\n",
      "Epoch: 1 | Step: 5650 | Avg. loss: 2.817 | lr: 0.0004593939393939394\n",
      "Epoch: 1 | Step: 5700 | Avg. loss: 2.909 | lr: 0.000458989898989899\n",
      "Epoch: 1 | Step: 5750 | Avg. loss: 2.805 | lr: 0.0004585858585858586\n",
      "Epoch: 1 | Step: 5800 | Avg. loss: 2.835 | lr: 0.0004581818181818182\n",
      "Epoch: 1 | Step: 5850 | Avg. loss: 2.882 | lr: 0.0004577777777777778\n",
      "Epoch: 1 | Step: 5900 | Avg. loss: 2.873 | lr: 0.0004573737373737374\n",
      "Epoch: 1 | Step: 5950 | Avg. loss: 2.833 | lr: 0.00045696969696969696\n",
      "Epoch: 1 | Step: 6000 | Avg. loss: 2.896 | lr: 0.0004565656565656566\n",
      "Saving model with test loss of 3.386\n",
      "Epoch: 1 | Step: 6050 | Avg. loss: 2.814 | lr: 0.00045616161616161615\n",
      "Epoch: 1 | Step: 6100 | Avg. loss: 2.794 | lr: 0.0004557575757575758\n",
      "Epoch: 1 | Step: 6150 | Avg. loss: 2.808 | lr: 0.00045535353535353535\n",
      "Epoch: 1 | Step: 6200 | Avg. loss: 2.811 | lr: 0.0004549494949494949\n",
      "Epoch: 1 | Step: 6250 | Avg. loss: 2.811 | lr: 0.00045454545454545455\n",
      "Epoch: 1 | Step: 6300 | Avg. loss: 2.819 | lr: 0.0004541414141414141\n",
      "Epoch: 1 | Step: 6350 | Avg. loss: 2.959 | lr: 0.00045373737373737374\n",
      "Epoch: 1 | Step: 6400 | Avg. loss: 4.743 | lr: 0.0004533333333333333\n",
      "Epoch: 1 | Step: 6450 | Avg. loss: 4.697 | lr: 0.00045292929292929294\n",
      "Epoch: 1 | Step: 6500 | Avg. loss: 4.479 | lr: 0.00045252525252525256\n",
      "Epoch: 1 | Step: 6550 | Avg. loss: 4.436 | lr: 0.00045212121212121213\n",
      "Epoch: 1 | Step: 6600 | Avg. loss: 4.549 | lr: 0.00045171717171717176\n",
      "Epoch: 1 | Step: 6650 | Avg. loss: 4.450 | lr: 0.00045131313131313133\n",
      "Epoch: 1 | Step: 6700 | Avg. loss: 4.470 | lr: 0.00045090909090909095\n",
      "Epoch: 1 | Step: 6750 | Avg. loss: 4.673 | lr: 0.0004505050505050505\n",
      "Epoch: 1 | Step: 6800 | Avg. loss: 4.403 | lr: 0.00045010101010101015\n",
      "Epoch: 1 | Step: 6850 | Avg. loss: 4.553 | lr: 0.0004496969696969697\n",
      "Epoch: 1 | Step: 6900 | Avg. loss: 4.395 | lr: 0.0004492929292929293\n",
      "Epoch: 1 | Step: 6950 | Avg. loss: 4.442 | lr: 0.0004488888888888889\n",
      "Epoch: 1 | Step: 7000 | Avg. loss: 4.418 | lr: 0.0004484848484848485\n",
      "Saving model with test loss of 4.825\n",
      "Epoch: 1 | Step: 7050 | Avg. loss: 4.475 | lr: 0.0004480808080808081\n",
      "Epoch: 1 | Step: 7100 | Avg. loss: 4.371 | lr: 0.0004476767676767677\n",
      "Epoch: 1 | Step: 7150 | Avg. loss: 4.522 | lr: 0.00044727272727272725\n",
      "Epoch: 1 | Step: 7200 | Avg. loss: 4.358 | lr: 0.0004468686868686869\n",
      "Epoch: 1 | Step: 7250 | Avg. loss: 4.466 | lr: 0.00044646464646464645\n",
      "Epoch: 1 | Step: 7300 | Avg. loss: 4.453 | lr: 0.0004460606060606061\n",
      "Epoch: 1 | Step: 7350 | Avg. loss: 4.550 | lr: 0.00044565656565656564\n",
      "Epoch: 1 | Step: 7400 | Avg. loss: 4.371 | lr: 0.0004452525252525252\n",
      "Epoch: 1 | Step: 7450 | Avg. loss: 4.385 | lr: 0.00044484848484848484\n",
      "Epoch: 1 | Step: 7500 | Avg. loss: 4.454 | lr: 0.0004444444444444444\n",
      "Epoch: 1 | Step: 7550 | Avg. loss: 4.440 | lr: 0.00044404040404040404\n",
      "Epoch: 1 | Step: 7600 | Avg. loss: 4.344 | lr: 0.00044363636363636366\n",
      "Epoch: 1 | Step: 7650 | Avg. loss: 4.493 | lr: 0.0004432323232323233\n",
      "Epoch: 1 | Step: 7700 | Avg. loss: 4.515 | lr: 0.00044282828282828286\n",
      "Epoch: 1 | Step: 7750 | Avg. loss: 4.491 | lr: 0.00044242424242424243\n",
      "Epoch: 1 | Step: 7800 | Avg. loss: 4.647 | lr: 0.00044202020202020205\n",
      "Epoch: 1 | Step: 7850 | Avg. loss: 5.169 | lr: 0.0004416161616161616\n",
      "Epoch: 1 | Step: 7900 | Avg. loss: 5.093 | lr: 0.00044121212121212125\n",
      "Epoch: 1 | Step: 7950 | Avg. loss: 5.160 | lr: 0.0004408080808080808\n",
      "Epoch: 1 | Step: 8000 | Avg. loss: 5.174 | lr: 0.00044040404040404044\n",
      "Saving model with test loss of 5.930\n",
      "Epoch: 1 | Step: 8050 | Avg. loss: 5.072 | lr: 0.00044\n",
      "Epoch: 1 | Step: 8100 | Avg. loss: 5.176 | lr: 0.0004395959595959596\n",
      "Epoch: 1 | Step: 8150 | Avg. loss: 5.262 | lr: 0.0004391919191919192\n",
      "Epoch: 1 | Step: 8200 | Avg. loss: 5.105 | lr: 0.0004387878787878788\n",
      "Epoch: 1 | Step: 8250 | Avg. loss: 5.200 | lr: 0.0004383838383838384\n",
      "Epoch: 1 | Step: 8300 | Avg. loss: 4.989 | lr: 0.000437979797979798\n",
      "Epoch: 1 | Step: 8350 | Avg. loss: 5.053 | lr: 0.00043757575757575755\n",
      "Epoch: 1 | Step: 8400 | Avg. loss: 5.069 | lr: 0.0004371717171717172\n",
      "Epoch: 1 | Step: 8450 | Avg. loss: 5.102 | lr: 0.00043676767676767674\n",
      "Epoch: 1 | Step: 8500 | Avg. loss: 5.209 | lr: 0.00043636363636363637\n",
      "Epoch: 1 | Step: 8550 | Avg. loss: 5.111 | lr: 0.00043595959595959594\n",
      "Epoch: 1 | Step: 8600 | Avg. loss: 5.044 | lr: 0.0004355555555555555\n",
      "Epoch: 1 | Step: 8650 | Avg. loss: 5.071 | lr: 0.0004351515151515152\n",
      "Epoch: 1 | Step: 8700 | Avg. loss: 5.097 | lr: 0.00043474747474747476\n",
      "Epoch: 1 | Step: 8750 | Avg. loss: 5.112 | lr: 0.0004343434343434344\n",
      "Epoch: 1 | Step: 8800 | Avg. loss: 5.141 | lr: 0.00043393939393939396\n",
      "Epoch: 1 | Step: 8850 | Avg. loss: 5.172 | lr: 0.0004335353535353536\n",
      "Epoch: 1 | Step: 8900 | Avg. loss: 5.125 | lr: 0.00043313131313131315\n",
      "Epoch: 1 | Step: 8950 | Avg. loss: 4.999 | lr: 0.0004327272727272727\n",
      "Epoch: 1 | Step: 9000 | Avg. loss: 4.929 | lr: 0.00043232323232323235\n",
      "Saving model with test loss of 5.684\n",
      "Epoch: 1 | Step: 9050 | Avg. loss: 5.179 | lr: 0.0004319191919191919\n",
      "Epoch: 1 | Step: 9100 | Avg. loss: 5.210 | lr: 0.00043151515151515154\n",
      "Epoch: 1 | Step: 9150 | Avg. loss: 5.163 | lr: 0.0004311111111111111\n",
      "Epoch: 1 | Step: 9200 | Avg. loss: 5.103 | lr: 0.00043070707070707074\n",
      "Epoch: 1 | Step: 9250 | Avg. loss: 5.099 | lr: 0.0004303030303030303\n",
      "Epoch: 1 | Step: 9300 | Avg. loss: 5.277 | lr: 0.0004298989898989899\n",
      "Epoch: 1 | Step: 9350 | Avg. loss: 5.191 | lr: 0.0004294949494949495\n",
      "Epoch: 1 | Step: 9400 | Avg. loss: 5.121 | lr: 0.0004290909090909091\n",
      "Epoch: 1 | Step: 9450 | Avg. loss: 5.102 | lr: 0.0004286868686868687\n",
      "Epoch: 1 | Step: 9500 | Avg. loss: 5.188 | lr: 0.0004282828282828283\n",
      "Epoch: 1 | Step: 9550 | Avg. loss: 5.100 | lr: 0.00042787878787878784\n",
      "Epoch: 1 | Step: 9600 | Avg. loss: 5.116 | lr: 0.00042747474747474747\n",
      "Epoch: 1 | Step: 9650 | Avg. loss: 5.085 | lr: 0.00042707070707070704\n",
      "Epoch: 1 | Step: 9700 | Avg. loss: 5.198 | lr: 0.0004266666666666667\n",
      "Epoch: 1 | Step: 9750 | Avg. loss: 5.068 | lr: 0.0004262626262626263\n",
      "Epoch: 1 | Step: 9800 | Avg. loss: 5.176 | lr: 0.0004258585858585859\n",
      "Epoch: 1 | Step: 9850 | Avg. loss: 4.975 | lr: 0.0004254545454545455\n",
      "Epoch: 1 | Step: 9900 | Avg. loss: 5.061 | lr: 0.00042505050505050506\n",
      "Epoch: 1 | Step: 9950 | Avg. loss: 5.088 | lr: 0.0004246464646464647\n",
      "Epoch: 1 | Step: 10000 | Avg. loss: 5.106 | lr: 0.00042424242424242425\n",
      "Saving model with test loss of 5.731\n",
      "Epoch: 1 | Step: 10050 | Avg. loss: 4.996 | lr: 0.0004238383838383839\n",
      "Epoch: 1 | Step: 10100 | Avg. loss: 5.133 | lr: 0.00042343434343434345\n",
      "Epoch: 1 | Step: 10150 | Avg. loss: 4.994 | lr: 0.000423030303030303\n",
      "Epoch: 1 | Step: 10200 | Avg. loss: 5.098 | lr: 0.00042262626262626264\n",
      "Epoch: 1 | Step: 10250 | Avg. loss: 5.001 | lr: 0.0004222222222222222\n",
      "Epoch: 1 | Step: 10300 | Avg. loss: 4.983 | lr: 0.00042181818181818184\n",
      "Epoch: 1 | Step: 10350 | Avg. loss: 5.179 | lr: 0.0004214141414141414\n",
      "Epoch: 1 | Step: 10400 | Avg. loss: 5.054 | lr: 0.00042101010101010104\n",
      "Epoch: 1 | Step: 10450 | Avg. loss: 4.997 | lr: 0.0004206060606060606\n",
      "Epoch: 1 | Step: 10500 | Avg. loss: 5.219 | lr: 0.0004202020202020202\n",
      "Epoch: 1 | Step: 10550 | Avg. loss: 5.145 | lr: 0.0004197979797979798\n",
      "Epoch: 1 | Step: 10600 | Avg. loss: 5.107 | lr: 0.0004193939393939394\n",
      "Epoch: 1 | Step: 10650 | Avg. loss: 5.119 | lr: 0.000418989898989899\n",
      "Epoch: 1 | Step: 10700 | Avg. loss: 5.033 | lr: 0.00041858585858585857\n",
      "Epoch: 1 | Step: 10750 | Avg. loss: 5.009 | lr: 0.00041818181818181814\n",
      "Epoch: 1 | Step: 10800 | Avg. loss: 5.114 | lr: 0.0004177777777777778\n",
      "Epoch: 1 | Step: 10850 | Avg. loss: 5.000 | lr: 0.0004173737373737374\n",
      "Epoch: 1 | Step: 10900 | Avg. loss: 5.217 | lr: 0.000416969696969697\n",
      "Epoch: 1 | Step: 10950 | Avg. loss: 5.165 | lr: 0.0004165656565656566\n",
      "Epoch: 1 | Step: 11000 | Avg. loss: 5.156 | lr: 0.0004161616161616162\n",
      "Saving model with test loss of 5.877\n",
      "Epoch: 1 | Step: 11050 | Avg. loss: 5.110 | lr: 0.0004157575757575758\n",
      "Epoch: 1 | Step: 11100 | Avg. loss: 5.024 | lr: 0.00041535353535353535\n",
      "Epoch: 1 | Step: 11150 | Avg. loss: 5.253 | lr: 0.000414949494949495\n",
      "Epoch: 1 | Step: 11200 | Avg. loss: 5.081 | lr: 0.00041454545454545455\n",
      "Epoch: 1 | Step: 11250 | Avg. loss: 5.289 | lr: 0.0004141414141414142\n",
      "Epoch: 1 | Step: 11300 | Avg. loss: 5.205 | lr: 0.00041373737373737374\n",
      "Epoch: 1 | Step: 11350 | Avg. loss: 4.979 | lr: 0.0004133333333333333\n",
      "Epoch: 1 | Step: 11400 | Avg. loss: 5.002 | lr: 0.00041292929292929294\n",
      "Epoch: 1 | Step: 11450 | Avg. loss: 5.104 | lr: 0.0004125252525252525\n",
      "Epoch: 1 | Step: 11500 | Avg. loss: 5.146 | lr: 0.00041212121212121214\n",
      "Epoch: 1 | Step: 11550 | Avg. loss: 4.982 | lr: 0.0004117171717171717\n",
      "Epoch: 1 | Step: 11600 | Avg. loss: 5.042 | lr: 0.00041131313131313133\n",
      "Epoch: 1 | Step: 11650 | Avg. loss: 5.082 | lr: 0.0004109090909090909\n",
      "Epoch: 1 | Step: 11700 | Avg. loss: 4.991 | lr: 0.0004105050505050505\n",
      "Epoch: 1 | Step: 11750 | Avg. loss: 5.192 | lr: 0.0004101010101010101\n",
      "Epoch: 1 | Step: 11800 | Avg. loss: 5.117 | lr: 0.00040969696969696967\n",
      "Epoch: 1 | Step: 11850 | Avg. loss: 5.091 | lr: 0.00040929292929292935\n",
      "Epoch: 1 | Step: 11900 | Avg. loss: 5.108 | lr: 0.0004088888888888889\n",
      "Epoch: 1 | Step: 11950 | Avg. loss: 5.103 | lr: 0.0004084848484848485\n",
      "Epoch: 1 | Step: 12000 | Avg. loss: 5.035 | lr: 0.0004080808080808081\n",
      "Saving model with test loss of 5.803\n",
      "Epoch: 1 | Step: 12050 | Avg. loss: 5.085 | lr: 0.0004076767676767677\n",
      "Epoch: 1 | Step: 12100 | Avg. loss: 5.124 | lr: 0.0004072727272727273\n",
      "Epoch: 1 | Step: 12150 | Avg. loss: 5.013 | lr: 0.0004068686868686869\n",
      "Epoch: 1 | Step: 12200 | Avg. loss: 4.989 | lr: 0.0004064646464646465\n",
      "Epoch: 1 | Step: 12250 | Avg. loss: 5.024 | lr: 0.0004060606060606061\n",
      "Epoch: 1 | Step: 12300 | Avg. loss: 5.071 | lr: 0.00040565656565656565\n",
      "Epoch: 1 | Step: 12350 | Avg. loss: 5.138 | lr: 0.0004052525252525253\n",
      "Epoch: 1 | Step: 12400 | Avg. loss: 5.061 | lr: 0.00040484848484848484\n",
      "Epoch: 1 | Step: 12450 | Avg. loss: 5.180 | lr: 0.00040444444444444447\n",
      "Epoch: 1 | Step: 12500 | Avg. loss: 5.154 | lr: 0.00040404040404040404\n",
      "Epoch: 1 | Step: 12550 | Avg. loss: 5.080 | lr: 0.0004036363636363636\n",
      "Epoch: 1 | Step: 12600 | Avg. loss: 5.065 | lr: 0.00040323232323232324\n",
      "Epoch: 1 | Step: 12650 | Avg. loss: 5.142 | lr: 0.0004028282828282828\n",
      "Epoch: 1 | Step: 12700 | Avg. loss: 5.073 | lr: 0.00040242424242424243\n",
      "Epoch: 1 | Step: 12750 | Avg. loss: 5.097 | lr: 0.000402020202020202\n",
      "Epoch: 1 | Step: 12800 | Avg. loss: 5.110 | lr: 0.00040161616161616163\n",
      "Epoch: 1 | Step: 12850 | Avg. loss: 5.094 | lr: 0.0004012121212121212\n",
      "Epoch: 1 | Step: 12900 | Avg. loss: 5.103 | lr: 0.0004008080808080808\n",
      "Epoch: 1 | Step: 12950 | Avg. loss: 5.148 | lr: 0.00040040404040404045\n",
      "Epoch: 1 | Step: 13000 | Avg. loss: 5.162 | lr: 0.0004\n",
      "Saving model with test loss of 5.513\n",
      "Epoch: 1 | Step: 13050 | Avg. loss: 5.130 | lr: 0.00039959595959595964\n",
      "Epoch: 1 | Step: 13100 | Avg. loss: 5.038 | lr: 0.0003991919191919192\n",
      "Epoch: 1 | Step: 13150 | Avg. loss: 5.202 | lr: 0.0003987878787878788\n",
      "Epoch: 1 | Step: 13200 | Avg. loss: 5.075 | lr: 0.0003983838383838384\n",
      "Epoch: 1 | Step: 13250 | Avg. loss: 5.112 | lr: 0.000397979797979798\n",
      "Epoch: 1 | Step: 13300 | Avg. loss: 5.130 | lr: 0.0003975757575757576\n",
      "Epoch: 1 | Step: 13350 | Avg. loss: 4.956 | lr: 0.0003971717171717172\n",
      "Epoch: 1 | Step: 13400 | Avg. loss: 5.059 | lr: 0.0003967676767676768\n",
      "Epoch: 1 | Step: 13450 | Avg. loss: 5.148 | lr: 0.0003963636363636364\n",
      "Epoch: 1 | Step: 13500 | Avg. loss: 4.968 | lr: 0.00039595959595959594\n",
      "Epoch: 1 | Step: 13550 | Avg. loss: 5.037 | lr: 0.00039555555555555557\n",
      "Epoch: 1 | Step: 13600 | Avg. loss: 5.167 | lr: 0.00039515151515151514\n",
      "Epoch: 1 | Step: 13650 | Avg. loss: 5.188 | lr: 0.00039474747474747476\n",
      "Epoch: 1 | Step: 13700 | Avg. loss: 5.146 | lr: 0.00039434343434343434\n",
      "Epoch: 1 | Step: 13750 | Avg. loss: 4.946 | lr: 0.0003939393939393939\n",
      "Epoch: 1 | Step: 13800 | Avg. loss: 5.046 | lr: 0.00039353535353535353\n",
      "Epoch: 1 | Step: 13850 | Avg. loss: 5.064 | lr: 0.0003931313131313131\n",
      "Epoch: 1 | Step: 13900 | Avg. loss: 5.111 | lr: 0.00039272727272727273\n",
      "Epoch: 1 | Step: 13950 | Avg. loss: 5.168 | lr: 0.0003923232323232323\n",
      "Epoch: 1 | Step: 14000 | Avg. loss: 4.974 | lr: 0.000391919191919192\n",
      "Saving model with test loss of 5.616\n",
      "Epoch: 1 | Step: 14050 | Avg. loss: 5.000 | lr: 0.00039151515151515155\n",
      "Epoch: 1 | Step: 14100 | Avg. loss: 5.004 | lr: 0.0003911111111111111\n",
      "Epoch: 1 | Step: 14150 | Avg. loss: 5.149 | lr: 0.00039070707070707074\n",
      "Epoch: 1 | Step: 14200 | Avg. loss: 5.015 | lr: 0.0003903030303030303\n",
      "Epoch: 1 | Step: 14250 | Avg. loss: 5.213 | lr: 0.00038989898989898994\n",
      "Epoch: 1 | Step: 14300 | Avg. loss: 5.117 | lr: 0.0003894949494949495\n",
      "Epoch: 1 | Step: 14350 | Avg. loss: 5.096 | lr: 0.0003890909090909091\n",
      "Epoch: 1 | Step: 14400 | Avg. loss: 5.054 | lr: 0.0003886868686868687\n",
      "Epoch: 1 | Step: 14450 | Avg. loss: 5.063 | lr: 0.0003882828282828283\n",
      "Epoch: 1 | Step: 14500 | Avg. loss: 4.948 | lr: 0.0003878787878787879\n",
      "Epoch: 1 | Step: 14550 | Avg. loss: 4.990 | lr: 0.0003874747474747475\n",
      "Epoch: 1 | Step: 14600 | Avg. loss: 5.200 | lr: 0.0003870707070707071\n",
      "Epoch: 1 | Step: 14650 | Avg. loss: 5.145 | lr: 0.00038666666666666667\n",
      "Epoch: 1 | Step: 14700 | Avg. loss: 5.027 | lr: 0.00038626262626262624\n",
      "Epoch: 1 | Step: 14750 | Avg. loss: 4.967 | lr: 0.00038585858585858586\n",
      "Epoch: 1 | Step: 14800 | Avg. loss: 5.018 | lr: 0.00038545454545454544\n",
      "Epoch: 1 | Step: 14850 | Avg. loss: 4.970 | lr: 0.00038505050505050506\n",
      "Epoch: 1 | Step: 14900 | Avg. loss: 5.073 | lr: 0.00038464646464646463\n",
      "Epoch: 1 | Step: 14950 | Avg. loss: 5.089 | lr: 0.0003842424242424242\n",
      "Epoch: 1 | Step: 15000 | Avg. loss: 5.116 | lr: 0.00038383838383838383\n",
      "Saving model with test loss of 5.568\n",
      "Epoch: 1 | Step: 15050 | Avg. loss: 5.085 | lr: 0.00038343434343434345\n",
      "Epoch: 1 | Step: 15100 | Avg. loss: 5.081 | lr: 0.0003830303030303031\n",
      "Epoch: 1 | Step: 15150 | Avg. loss: 4.967 | lr: 0.00038262626262626265\n",
      "Epoch: 1 | Step: 15200 | Avg. loss: 5.009 | lr: 0.0003822222222222223\n",
      "Epoch: 1 | Step: 15250 | Avg. loss: 4.977 | lr: 0.00038181818181818184\n",
      "Epoch: 1 | Step: 15300 | Avg. loss: 5.052 | lr: 0.0003814141414141414\n",
      "Epoch: 1 | Step: 15350 | Avg. loss: 5.190 | lr: 0.00038101010101010104\n",
      "Epoch: 1 | Step: 15400 | Avg. loss: 5.087 | lr: 0.0003806060606060606\n",
      "Epoch: 1 | Step: 15450 | Avg. loss: 5.114 | lr: 0.00038020202020202024\n",
      "Epoch: 1 | Step: 15500 | Avg. loss: 5.081 | lr: 0.0003797979797979798\n",
      "Epoch: 1 | Step: 15550 | Avg. loss: 5.138 | lr: 0.0003793939393939394\n",
      "Epoch: 1 | Step: 15600 | Avg. loss: 5.064 | lr: 0.000378989898989899\n",
      "Epoch: 1 | Step: 15650 | Avg. loss: 5.137 | lr: 0.0003785858585858586\n",
      "Epoch: 1 | Step: 15700 | Avg. loss: 5.114 | lr: 0.0003781818181818182\n",
      "Epoch: 1 | Step: 15750 | Avg. loss: 5.148 | lr: 0.00037777777777777777\n",
      "Epoch: 1 | Step: 15800 | Avg. loss: 4.963 | lr: 0.0003773737373737374\n",
      "Epoch: 1 | Step: 15850 | Avg. loss: 5.013 | lr: 0.00037696969696969696\n",
      "Epoch: 1 | Step: 15900 | Avg. loss: 5.135 | lr: 0.00037656565656565654\n",
      "Epoch: 1 | Step: 15950 | Avg. loss: 5.066 | lr: 0.00037616161616161616\n",
      "Epoch: 1 | Step: 16000 | Avg. loss: 5.094 | lr: 0.00037575757575757573\n",
      "Saving model with test loss of 5.888\n",
      "Epoch: 1 | Step: 16050 | Avg. loss: 5.100 | lr: 0.00037535353535353536\n",
      "Epoch: 1 | Step: 16100 | Avg. loss: 5.146 | lr: 0.000374949494949495\n",
      "Epoch: 1 | Step: 16150 | Avg. loss: 5.006 | lr: 0.00037454545454545455\n",
      "Epoch: 1 | Step: 16200 | Avg. loss: 5.082 | lr: 0.0003741414141414142\n",
      "Epoch: 1 | Step: 16250 | Avg. loss: 5.118 | lr: 0.00037373737373737375\n",
      "Epoch: 1 | Step: 16300 | Avg. loss: 4.991 | lr: 0.0003733333333333334\n",
      "Epoch: 1 | Step: 16350 | Avg. loss: 5.144 | lr: 0.00037292929292929294\n",
      "Epoch: 1 | Step: 16400 | Avg. loss: 4.966 | lr: 0.00037252525252525257\n",
      "Epoch: 1 | Step: 16450 | Avg. loss: 4.949 | lr: 0.00037212121212121214\n",
      "Epoch: 1 | Step: 16500 | Avg. loss: 5.128 | lr: 0.0003717171717171717\n",
      "Epoch: 1 | Step: 16550 | Avg. loss: 5.198 | lr: 0.00037131313131313134\n",
      "Epoch: 1 | Step: 16600 | Avg. loss: 5.169 | lr: 0.0003709090909090909\n",
      "Epoch: 1 | Step: 16650 | Avg. loss: 5.170 | lr: 0.00037050505050505053\n",
      "Epoch: 1 | Step: 16700 | Avg. loss: 5.076 | lr: 0.0003701010101010101\n",
      "Epoch: 1 | Step: 16750 | Avg. loss: 4.959 | lr: 0.00036969696969696967\n",
      "Epoch: 1 | Step: 16800 | Avg. loss: 5.038 | lr: 0.0003692929292929293\n",
      "Epoch: 1 | Step: 16850 | Avg. loss: 5.178 | lr: 0.00036888888888888887\n",
      "Epoch: 1 | Step: 16900 | Avg. loss: 5.160 | lr: 0.0003684848484848485\n",
      "Epoch: 1 | Step: 16950 | Avg. loss: 5.077 | lr: 0.00036808080808080806\n",
      "Epoch: 1 | Step: 17000 | Avg. loss: 5.111 | lr: 0.0003676767676767677\n",
      "Saving model with test loss of 5.603\n",
      "Epoch: 1 | Step: 17050 | Avg. loss: 5.045 | lr: 0.00036727272727272726\n",
      "Epoch: 1 | Step: 17100 | Avg. loss: 5.168 | lr: 0.00036686868686868683\n",
      "Epoch: 1 | Step: 17150 | Avg. loss: 5.152 | lr: 0.00036646464646464646\n",
      "Epoch: 1 | Step: 17200 | Avg. loss: 5.087 | lr: 0.0003660606060606061\n",
      "Epoch: 1 | Step: 17250 | Avg. loss: 5.126 | lr: 0.0003656565656565657\n",
      "Epoch: 1 | Step: 17300 | Avg. loss: 5.068 | lr: 0.0003652525252525253\n",
      "Epoch: 1 | Step: 17350 | Avg. loss: 5.095 | lr: 0.00036484848484848485\n",
      "Epoch: 1 | Step: 17400 | Avg. loss: 4.975 | lr: 0.00036444444444444447\n",
      "Epoch: 1 | Step: 17450 | Avg. loss: 5.111 | lr: 0.00036404040404040404\n",
      "Epoch: 1 | Step: 17500 | Avg. loss: 5.011 | lr: 0.00036363636363636367\n",
      "Epoch: 1 | Step: 17550 | Avg. loss: 5.087 | lr: 0.00036323232323232324\n",
      "Epoch: 1 | Step: 17600 | Avg. loss: 5.068 | lr: 0.00036282828282828286\n",
      "Epoch: 1 | Step: 17650 | Avg. loss: 5.112 | lr: 0.00036242424242424244\n",
      "Epoch: 1 | Step: 17700 | Avg. loss: 5.027 | lr: 0.000362020202020202\n",
      "Epoch: 1 | Step: 17750 | Avg. loss: 4.981 | lr: 0.00036161616161616163\n",
      "Epoch: 1 | Step: 17800 | Avg. loss: 5.066 | lr: 0.0003612121212121212\n",
      "Epoch: 1 | Step: 17850 | Avg. loss: 5.047 | lr: 0.0003608080808080808\n",
      "Epoch: 1 | Step: 17900 | Avg. loss: 4.989 | lr: 0.0003604040404040404\n",
      "Epoch: 1 | Step: 17950 | Avg. loss: 5.043 | lr: 0.00035999999999999997\n",
      "Epoch: 1 | Step: 18000 | Avg. loss: 5.015 | lr: 0.0003595959595959596\n",
      "Saving model with test loss of 5.803\n",
      "Epoch: 1 | Step: 18050 | Avg. loss: 5.134 | lr: 0.00035919191919191916\n",
      "Epoch: 1 | Step: 18100 | Avg. loss: 5.029 | lr: 0.0003587878787878788\n",
      "Epoch: 1 | Step: 18150 | Avg. loss: 5.157 | lr: 0.00035838383838383836\n",
      "Epoch: 1 | Step: 18200 | Avg. loss: 5.058 | lr: 0.000357979797979798\n",
      "Epoch: 1 | Step: 18250 | Avg. loss: 5.058 | lr: 0.0003575757575757576\n",
      "Epoch: 1 | Step: 18300 | Avg. loss: 5.023 | lr: 0.0003571717171717172\n",
      "Epoch: 1 | Step: 18350 | Avg. loss: 5.199 | lr: 0.0003567676767676768\n",
      "Epoch: 1 | Step: 18400 | Avg. loss: 5.118 | lr: 0.0003563636363636364\n",
      "Epoch: 1 | Step: 18450 | Avg. loss: 4.969 | lr: 0.000355959595959596\n",
      "Epoch: 1 | Step: 18500 | Avg. loss: 5.046 | lr: 0.00035555555555555557\n",
      "Epoch: 1 | Step: 18550 | Avg. loss: 4.991 | lr: 0.00035515151515151514\n",
      "Epoch: 1 | Step: 18600 | Avg. loss: 5.161 | lr: 0.00035474747474747477\n",
      "Epoch: 1 | Step: 18650 | Avg. loss: 5.179 | lr: 0.00035434343434343434\n",
      "Epoch: 1 | Step: 18700 | Avg. loss: 5.132 | lr: 0.00035393939393939396\n",
      "Epoch: 1 | Step: 18750 | Avg. loss: 5.091 | lr: 0.00035353535353535354\n",
      "Epoch: 1 | Step: 18800 | Avg. loss: 5.086 | lr: 0.00035313131313131316\n",
      "Epoch: 1 | Step: 18850 | Avg. loss: 5.018 | lr: 0.00035272727272727273\n",
      "Epoch: 1 | Step: 18900 | Avg. loss: 5.129 | lr: 0.0003523232323232323\n",
      "Epoch: 1 | Step: 18950 | Avg. loss: 5.251 | lr: 0.0003519191919191919\n",
      "Epoch: 1 | Step: 19000 | Avg. loss: 5.113 | lr: 0.0003515151515151515\n",
      "Saving model with test loss of 5.659\n",
      "Epoch: 1 | Step: 19050 | Avg. loss: 5.100 | lr: 0.0003511111111111111\n",
      "Epoch: 1 | Step: 19100 | Avg. loss: 5.154 | lr: 0.0003507070707070707\n",
      "Epoch: 1 | Step: 19150 | Avg. loss: 5.070 | lr: 0.00035030303030303026\n",
      "Epoch: 1 | Step: 19200 | Avg. loss: 5.142 | lr: 0.0003498989898989899\n",
      "Epoch: 1 | Step: 19250 | Avg. loss: 5.068 | lr: 0.00034949494949494946\n",
      "Epoch: 1 | Step: 19300 | Avg. loss: 5.181 | lr: 0.00034909090909090914\n",
      "Epoch: 1 | Step: 19350 | Avg. loss: 5.036 | lr: 0.0003486868686868687\n",
      "Epoch: 1 | Step: 19400 | Avg. loss: 5.141 | lr: 0.00034828282828282834\n",
      "Epoch: 1 | Step: 19450 | Avg. loss: 5.061 | lr: 0.0003478787878787879\n",
      "Epoch: 1 | Step: 19500 | Avg. loss: 5.009 | lr: 0.0003474747474747475\n",
      "Epoch: 1 | Step: 19550 | Avg. loss: 5.126 | lr: 0.0003470707070707071\n",
      "Epoch: 1 | Step: 19600 | Avg. loss: 5.117 | lr: 0.00034666666666666667\n",
      "Epoch: 1 | Step: 19650 | Avg. loss: 5.173 | lr: 0.0003462626262626263\n",
      "Epoch: 1 | Step: 19700 | Avg. loss: 4.993 | lr: 0.00034585858585858587\n",
      "Epoch: 1 | Step: 19750 | Avg. loss: 5.115 | lr: 0.00034545454545454544\n",
      "Epoch: 1 | Step: 19800 | Avg. loss: 4.937 | lr: 0.00034505050505050506\n",
      "Epoch: 1 | Step: 19850 | Avg. loss: 5.169 | lr: 0.00034464646464646463\n",
      "Epoch: 1 | Step: 19900 | Avg. loss: 4.935 | lr: 0.00034424242424242426\n",
      "Epoch: 1 | Step: 19950 | Avg. loss: 5.145 | lr: 0.00034383838383838383\n",
      "Epoch: 1 | Step: 20000 | Avg. loss: 5.142 | lr: 0.00034343434343434346\n",
      "Saving model with test loss of 5.745\n",
      "Epoch: 1 | Step: 20050 | Avg. loss: 5.077 | lr: 0.000343030303030303\n",
      "Epoch: 1 | Step: 20100 | Avg. loss: 5.296 | lr: 0.0003426262626262626\n",
      "Epoch: 1 | Step: 20150 | Avg. loss: 5.114 | lr: 0.0003422222222222222\n",
      "Epoch: 1 | Step: 20200 | Avg. loss: 5.151 | lr: 0.0003418181818181818\n",
      "Epoch: 1 | Step: 20250 | Avg. loss: 5.148 | lr: 0.0003414141414141414\n",
      "Epoch: 1 | Step: 20300 | Avg. loss: 5.112 | lr: 0.000341010101010101\n",
      "Epoch: 1 | Step: 20350 | Avg. loss: 5.012 | lr: 0.00034060606060606056\n",
      "Epoch: 1 | Step: 20400 | Avg. loss: 5.245 | lr: 0.00034020202020202024\n",
      "Epoch: 1 | Step: 20450 | Avg. loss: 5.014 | lr: 0.0003397979797979798\n",
      "Epoch: 1 | Step: 20500 | Avg. loss: 5.108 | lr: 0.00033939393939393943\n",
      "Epoch: 1 | Step: 20550 | Avg. loss: 5.042 | lr: 0.000338989898989899\n",
      "Epoch: 1 | Step: 20600 | Avg. loss: 5.029 | lr: 0.00033858585858585863\n",
      "Epoch: 1 | Step: 20650 | Avg. loss: 5.054 | lr: 0.0003381818181818182\n",
      "Epoch: 1 | Step: 20700 | Avg. loss: 5.014 | lr: 0.00033777777777777777\n",
      "Epoch: 1 | Step: 20750 | Avg. loss: 4.987 | lr: 0.0003373737373737374\n",
      "Epoch: 1 | Step: 20800 | Avg. loss: 5.069 | lr: 0.00033696969696969697\n",
      "Epoch: 1 | Step: 20850 | Avg. loss: 5.157 | lr: 0.0003365656565656566\n",
      "Epoch: 1 | Step: 20900 | Avg. loss: 5.150 | lr: 0.00033616161616161616\n",
      "Epoch: 1 | Step: 20950 | Avg. loss: 5.050 | lr: 0.00033575757575757573\n",
      "Epoch: 1 | Step: 21000 | Avg. loss: 5.176 | lr: 0.00033535353535353536\n",
      "Saving model with test loss of 5.536\n",
      "Epoch: 1 | Step: 21050 | Avg. loss: 5.106 | lr: 0.00033494949494949493\n",
      "Epoch: 1 | Step: 21100 | Avg. loss: 4.954 | lr: 0.00033454545454545456\n",
      "Epoch: 1 | Step: 21150 | Avg. loss: 5.181 | lr: 0.0003341414141414141\n",
      "Epoch: 1 | Step: 21200 | Avg. loss: 5.151 | lr: 0.00033373737373737375\n",
      "Epoch: 1 | Step: 21250 | Avg. loss: 5.058 | lr: 0.0003333333333333333\n",
      "Epoch: 1 | Step: 21300 | Avg. loss: 5.028 | lr: 0.0003329292929292929\n",
      "Epoch: 1 | Step: 21350 | Avg. loss: 5.106 | lr: 0.0003325252525252525\n",
      "Epoch: 1 | Step: 21400 | Avg. loss: 5.107 | lr: 0.0003321212121212121\n",
      "Epoch: 1 | Step: 21450 | Avg. loss: 4.971 | lr: 0.00033171717171717177\n",
      "Epoch: 1 | Step: 21500 | Avg. loss: 4.983 | lr: 0.00033131313131313134\n",
      "Epoch: 1 | Step: 21550 | Avg. loss: 5.040 | lr: 0.00033090909090909096\n",
      "Epoch: 1 | Step: 21600 | Avg. loss: 4.967 | lr: 0.00033050505050505053\n",
      "Epoch: 1 | Step: 21650 | Avg. loss: 5.166 | lr: 0.0003301010101010101\n",
      "Epoch: 1 | Step: 21700 | Avg. loss: 5.143 | lr: 0.00032969696969696973\n",
      "Epoch: 1 | Step: 21750 | Avg. loss: 5.054 | lr: 0.0003292929292929293\n",
      "Epoch: 1 | Step: 21800 | Avg. loss: 5.114 | lr: 0.0003288888888888889\n",
      "Epoch: 1 | Step: 21850 | Avg. loss: 5.025 | lr: 0.0003284848484848485\n",
      "Epoch: 1 | Step: 21900 | Avg. loss: 5.169 | lr: 0.00032808080808080807\n",
      "Epoch: 1 | Step: 21950 | Avg. loss: 5.014 | lr: 0.0003276767676767677\n",
      "Epoch: 1 | Step: 22000 | Avg. loss: 4.992 | lr: 0.00032727272727272726\n",
      "Saving model with test loss of 5.353\n",
      "Epoch: 1 | Step: 22050 | Avg. loss: 5.060 | lr: 0.0003268686868686869\n",
      "Epoch: 1 | Step: 22100 | Avg. loss: 5.070 | lr: 0.00032646464646464646\n",
      "Epoch: 1 | Step: 22150 | Avg. loss: 5.114 | lr: 0.0003260606060606061\n",
      "Epoch: 1 | Step: 22200 | Avg. loss: 5.004 | lr: 0.00032565656565656566\n",
      "Epoch: 1 | Step: 22250 | Avg. loss: 5.094 | lr: 0.0003252525252525252\n",
      "Epoch: 1 | Step: 22300 | Avg. loss: 5.142 | lr: 0.00032484848484848485\n",
      "Epoch: 1 | Step: 22350 | Avg. loss: 5.123 | lr: 0.0003244444444444444\n",
      "Epoch: 1 | Step: 22400 | Avg. loss: 5.019 | lr: 0.00032404040404040405\n",
      "Epoch: 1 | Step: 22450 | Avg. loss: 5.142 | lr: 0.0003236363636363636\n",
      "Epoch: 1 | Step: 22500 | Avg. loss: 5.118 | lr: 0.00032323232323232324\n",
      "Epoch: 1 | Step: 22550 | Avg. loss: 4.989 | lr: 0.00032282828282828287\n",
      "Epoch: 1 | Step: 22600 | Avg. loss: 5.047 | lr: 0.00032242424242424244\n",
      "Epoch: 1 | Step: 22650 | Avg. loss: 5.096 | lr: 0.00032202020202020206\n",
      "Epoch: 1 | Step: 22700 | Avg. loss: 4.974 | lr: 0.00032161616161616163\n",
      "Epoch: 1 | Step: 22750 | Avg. loss: 5.020 | lr: 0.00032121212121212126\n",
      "Epoch: 1 | Step: 22800 | Avg. loss: 5.107 | lr: 0.00032080808080808083\n",
      "Epoch: 1 | Step: 22850 | Avg. loss: 5.068 | lr: 0.0003204040404040404\n",
      "Epoch: 1 | Step: 22900 | Avg. loss: 5.081 | lr: 0.00032\n",
      "Epoch: 1 | Step: 22950 | Avg. loss: 5.081 | lr: 0.0003195959595959596\n",
      "Epoch: 1 | Step: 23000 | Avg. loss: 5.086 | lr: 0.0003191919191919192\n",
      "Saving model with test loss of 5.980\n",
      "Epoch: 1 | Step: 23050 | Avg. loss: 5.088 | lr: 0.0003187878787878788\n",
      "Epoch: 1 | Step: 23100 | Avg. loss: 5.000 | lr: 0.00031838383838383836\n",
      "Epoch: 1 | Step: 23150 | Avg. loss: 5.018 | lr: 0.000317979797979798\n",
      "Epoch: 1 | Step: 23200 | Avg. loss: 5.060 | lr: 0.00031757575757575756\n",
      "Epoch: 1 | Step: 23250 | Avg. loss: 5.081 | lr: 0.0003171717171717172\n",
      "Epoch: 1 | Step: 23300 | Avg. loss: 5.104 | lr: 0.00031676767676767676\n",
      "Epoch: 1 | Step: 23350 | Avg. loss: 5.162 | lr: 0.0003163636363636364\n",
      "Epoch: 1 | Step: 23400 | Avg. loss: 5.130 | lr: 0.00031595959595959595\n",
      "Epoch: 1 | Step: 23450 | Avg. loss: 5.144 | lr: 0.0003155555555555555\n",
      "Epoch: 1 | Step: 23500 | Avg. loss: 5.203 | lr: 0.00031515151515151515\n",
      "Epoch: 1 | Step: 23550 | Avg. loss: 4.904 | lr: 0.0003147474747474747\n",
      "Epoch: 1 | Step: 23600 | Avg. loss: 5.170 | lr: 0.0003143434343434344\n",
      "Epoch: 1 | Step: 23650 | Avg. loss: 5.121 | lr: 0.00031393939393939397\n",
      "Epoch: 1 | Step: 23700 | Avg. loss: 4.928 | lr: 0.00031353535353535354\n",
      "Epoch: 1 | Step: 23750 | Avg. loss: 5.107 | lr: 0.00031313131313131316\n",
      "Epoch: 1 | Step: 23800 | Avg. loss: 5.020 | lr: 0.00031272727272727273\n",
      "Epoch: 1 | Step: 23850 | Avg. loss: 5.038 | lr: 0.00031232323232323236\n",
      "Epoch: 1 | Step: 23900 | Avg. loss: 5.047 | lr: 0.00031191919191919193\n",
      "Epoch: 1 | Step: 23950 | Avg. loss: 5.039 | lr: 0.00031151515151515156\n",
      "Epoch: 1 | Step: 24000 | Avg. loss: 5.042 | lr: 0.0003111111111111111\n",
      "Saving model with test loss of 5.696\n",
      "Epoch: 1 | Step: 24050 | Avg. loss: 4.859 | lr: 0.0003107070707070707\n",
      "Epoch: 1 | Step: 24100 | Avg. loss: 5.166 | lr: 0.0003103030303030303\n",
      "Epoch: 1 | Step: 24150 | Avg. loss: 5.025 | lr: 0.0003098989898989899\n",
      "Epoch: 1 | Step: 24200 | Avg. loss: 5.036 | lr: 0.0003094949494949495\n",
      "Epoch: 1 | Step: 24250 | Avg. loss: 5.060 | lr: 0.0003090909090909091\n",
      "Epoch: 1 | Step: 24300 | Avg. loss: 5.136 | lr: 0.00030868686868686866\n",
      "Epoch: 1 | Step: 24350 | Avg. loss: 5.142 | lr: 0.0003082828282828283\n",
      "Epoch: 1 | Step: 24400 | Avg. loss: 5.045 | lr: 0.00030787878787878786\n",
      "Epoch: 1 | Step: 24450 | Avg. loss: 5.032 | lr: 0.0003074747474747475\n",
      "Epoch: 1 | Step: 24500 | Avg. loss: 5.098 | lr: 0.00030707070707070705\n",
      "Epoch: 1 | Step: 24550 | Avg. loss: 5.153 | lr: 0.0003066666666666667\n",
      "Epoch: 1 | Step: 24600 | Avg. loss: 5.030 | lr: 0.00030626262626262625\n",
      "Epoch: 1 | Step: 24650 | Avg. loss: 5.121 | lr: 0.00030585858585858587\n",
      "Epoch: 1 | Step: 24700 | Avg. loss: 5.073 | lr: 0.0003054545454545455\n",
      "Epoch: 1 | Step: 24750 | Avg. loss: 5.111 | lr: 0.00030505050505050507\n",
      "Epoch: 1 | Step: 24800 | Avg. loss: 5.048 | lr: 0.0003046464646464647\n",
      "Epoch: 1 | Step: 24850 | Avg. loss: 5.171 | lr: 0.00030424242424242426\n",
      "Epoch: 1 | Step: 24900 | Avg. loss: 5.177 | lr: 0.00030383838383838383\n",
      "Epoch: 1 | Step: 24950 | Avg. loss: 5.023 | lr: 0.00030343434343434346\n",
      "Epoch: 1 | Step: 25000 | Avg. loss: 5.029 | lr: 0.00030303030303030303\n",
      "Saving model with test loss of 5.787\n",
      "Epoch: 1 | Step: 25050 | Avg. loss: 5.255 | lr: 0.00030262626262626266\n",
      "Epoch: 1 | Step: 25100 | Avg. loss: 5.140 | lr: 0.0003022222222222222\n",
      "Epoch: 1 | Step: 25150 | Avg. loss: 5.040 | lr: 0.00030181818181818185\n",
      "Epoch: 1 | Step: 25200 | Avg. loss: 4.995 | lr: 0.0003014141414141414\n",
      "Epoch: 1 | Step: 25250 | Avg. loss: 4.936 | lr: 0.000301010101010101\n",
      "Epoch: 1 | Step: 25300 | Avg. loss: 5.121 | lr: 0.0003006060606060606\n",
      "Epoch: 1 | Step: 25350 | Avg. loss: 5.104 | lr: 0.0003002020202020202\n",
      "Epoch: 1 | Step: 25400 | Avg. loss: 5.021 | lr: 0.0002997979797979798\n",
      "Epoch: 1 | Step: 25450 | Avg. loss: 5.151 | lr: 0.0002993939393939394\n",
      "Epoch: 1 | Step: 25500 | Avg. loss: 5.151 | lr: 0.00029898989898989895\n",
      "Epoch: 1 | Step: 25550 | Avg. loss: 5.022 | lr: 0.0002985858585858586\n",
      "Epoch: 1 | Step: 25600 | Avg. loss: 5.133 | lr: 0.00029818181818181815\n",
      "Epoch: 1 | Step: 25650 | Avg. loss: 5.212 | lr: 0.0002977777777777778\n",
      "Epoch: 1 | Step: 25700 | Avg. loss: 5.057 | lr: 0.0002973737373737374\n",
      "Epoch: 1 | Step: 25750 | Avg. loss: 5.036 | lr: 0.000296969696969697\n",
      "Epoch: 1 | Step: 25800 | Avg. loss: 5.091 | lr: 0.0002965656565656566\n",
      "Epoch: 1 | Step: 25850 | Avg. loss: 5.111 | lr: 0.00029616161616161617\n",
      "Epoch: 1 | Step: 25900 | Avg. loss: 5.119 | lr: 0.0002957575757575758\n",
      "Epoch: 1 | Step: 25950 | Avg. loss: 5.079 | lr: 0.00029535353535353536\n",
      "Epoch: 1 | Step: 26000 | Avg. loss: 5.090 | lr: 0.000294949494949495\n",
      "Saving model with test loss of 5.986\n",
      "Epoch: 1 | Step: 26050 | Avg. loss: 4.994 | lr: 0.00029454545454545456\n",
      "Epoch: 1 | Step: 26100 | Avg. loss: 5.045 | lr: 0.00029414141414141413\n",
      "Epoch: 1 | Step: 26150 | Avg. loss: 5.103 | lr: 0.00029373737373737375\n",
      "Epoch: 1 | Step: 26200 | Avg. loss: 5.021 | lr: 0.0002933333333333333\n",
      "Epoch: 1 | Step: 26250 | Avg. loss: 5.137 | lr: 0.00029292929292929295\n",
      "Epoch: 1 | Step: 26300 | Avg. loss: 4.935 | lr: 0.0002925252525252525\n",
      "Epoch: 1 | Step: 26350 | Avg. loss: 5.106 | lr: 0.00029212121212121215\n",
      "Epoch: 1 | Step: 26400 | Avg. loss: 5.111 | lr: 0.0002917171717171717\n",
      "Epoch: 1 | Step: 26450 | Avg. loss: 5.062 | lr: 0.0002913131313131313\n",
      "Epoch: 1 | Step: 26500 | Avg. loss: 5.015 | lr: 0.0002909090909090909\n",
      "Epoch: 1 | Step: 26550 | Avg. loss: 5.088 | lr: 0.0002905050505050505\n",
      "Epoch: 1 | Step: 26600 | Avg. loss: 5.029 | lr: 0.0002901010101010101\n",
      "Epoch: 1 | Step: 26650 | Avg. loss: 5.027 | lr: 0.0002896969696969697\n",
      "Epoch: 1 | Step: 26700 | Avg. loss: 5.049 | lr: 0.00028929292929292925\n",
      "Epoch: 1 | Step: 26750 | Avg. loss: 5.072 | lr: 0.0002888888888888889\n",
      "Epoch: 1 | Step: 26800 | Avg. loss: 5.051 | lr: 0.0002884848484848485\n",
      "Epoch: 1 | Step: 26850 | Avg. loss: 5.173 | lr: 0.0002880808080808081\n",
      "Epoch: 1 | Step: 26900 | Avg. loss: 5.011 | lr: 0.0002876767676767677\n",
      "Epoch: 1 | Step: 26950 | Avg. loss: 5.079 | lr: 0.0002872727272727273\n",
      "Epoch: 1 | Step: 27000 | Avg. loss: 5.097 | lr: 0.0002868686868686869\n",
      "Saving model with test loss of 5.900\n",
      "Epoch: 1 | Step: 27050 | Avg. loss: 5.041 | lr: 0.00028646464646464646\n",
      "Epoch: 1 | Step: 27100 | Avg. loss: 5.125 | lr: 0.0002860606060606061\n",
      "Epoch: 1 | Step: 27150 | Avg. loss: 5.028 | lr: 0.00028565656565656566\n",
      "Epoch: 1 | Step: 27200 | Avg. loss: 5.116 | lr: 0.0002852525252525253\n",
      "Epoch: 1 | Step: 27250 | Avg. loss: 5.043 | lr: 0.00028484848484848485\n",
      "Epoch: 1 | Step: 27300 | Avg. loss: 5.088 | lr: 0.0002844444444444444\n",
      "Epoch: 1 | Step: 27350 | Avg. loss: 5.191 | lr: 0.00028404040404040405\n",
      "Epoch: 1 | Step: 27400 | Avg. loss: 5.105 | lr: 0.0002836363636363636\n",
      "Epoch: 1 | Step: 27450 | Avg. loss: 5.146 | lr: 0.00028323232323232325\n",
      "Epoch: 1 | Step: 27500 | Avg. loss: 5.053 | lr: 0.0002828282828282828\n",
      "Epoch: 1 | Step: 27550 | Avg. loss: 5.034 | lr: 0.00028242424242424244\n",
      "Epoch: 1 | Step: 27600 | Avg. loss: 5.002 | lr: 0.000282020202020202\n",
      "Epoch: 1 | Step: 27650 | Avg. loss: 4.990 | lr: 0.0002816161616161616\n",
      "Epoch: 1 | Step: 27700 | Avg. loss: 5.109 | lr: 0.0002812121212121212\n",
      "Epoch: 1 | Step: 27750 | Avg. loss: 4.998 | lr: 0.0002808080808080808\n",
      "Epoch: 1 | Step: 27800 | Avg. loss: 5.065 | lr: 0.0002804040404040404\n",
      "Epoch: 1 | Step: 27850 | Avg. loss: 4.943 | lr: 0.00028000000000000003\n",
      "Epoch: 1 | Step: 27900 | Avg. loss: 5.014 | lr: 0.0002795959595959596\n",
      "Epoch: 1 | Step: 27950 | Avg. loss: 5.075 | lr: 0.0002791919191919192\n",
      "Epoch: 1 | Step: 28000 | Avg. loss: 5.059 | lr: 0.0002787878787878788\n",
      "Saving model with test loss of 5.594\n",
      "Epoch: 1 | Step: 28050 | Avg. loss: 4.982 | lr: 0.0002783838383838384\n",
      "Epoch: 1 | Step: 28100 | Avg. loss: 4.982 | lr: 0.000277979797979798\n",
      "Epoch: 1 | Step: 28150 | Avg. loss: 4.969 | lr: 0.0002775757575757576\n",
      "Epoch: 1 | Step: 28200 | Avg. loss: 5.012 | lr: 0.0002771717171717172\n",
      "Epoch: 1 | Step: 28250 | Avg. loss: 5.031 | lr: 0.00027676767676767676\n",
      "Epoch: 1 | Step: 28300 | Avg. loss: 5.139 | lr: 0.0002763636363636364\n",
      "Epoch: 1 | Step: 28350 | Avg. loss: 4.975 | lr: 0.00027595959595959595\n",
      "Epoch: 1 | Step: 28400 | Avg. loss: 5.179 | lr: 0.0002755555555555556\n",
      "Epoch: 1 | Step: 28450 | Avg. loss: 4.991 | lr: 0.00027515151515151515\n",
      "Epoch: 1 | Step: 28500 | Avg. loss: 5.178 | lr: 0.0002747474747474747\n",
      "Epoch: 1 | Step: 28550 | Avg. loss: 5.110 | lr: 0.00027434343434343435\n",
      "Epoch: 1 | Step: 28600 | Avg. loss: 5.009 | lr: 0.0002739393939393939\n",
      "Epoch: 1 | Step: 28650 | Avg. loss: 5.113 | lr: 0.00027353535353535354\n",
      "Epoch: 1 | Step: 28700 | Avg. loss: 5.099 | lr: 0.0002731313131313131\n",
      "Epoch: 1 | Step: 28750 | Avg. loss: 5.038 | lr: 0.00027272727272727274\n",
      "Epoch: 1 | Step: 28800 | Avg. loss: 5.068 | lr: 0.0002723232323232323\n",
      "Epoch: 1 | Step: 28850 | Avg. loss: 5.060 | lr: 0.0002719191919191919\n",
      "Epoch: 1 | Step: 28900 | Avg. loss: 5.168 | lr: 0.00027151515151515156\n",
      "Epoch: 1 | Step: 28950 | Avg. loss: 5.089 | lr: 0.00027111111111111113\n",
      "Epoch: 1 | Step: 29000 | Avg. loss: 5.106 | lr: 0.00027070707070707075\n",
      "Saving model with test loss of 5.697\n",
      "Epoch: 1 | Step: 29050 | Avg. loss: 5.194 | lr: 0.0002703030303030303\n",
      "Epoch: 1 | Step: 29100 | Avg. loss: 5.159 | lr: 0.0002698989898989899\n",
      "Epoch: 1 | Step: 29150 | Avg. loss: 5.192 | lr: 0.0002694949494949495\n",
      "Epoch: 1 | Step: 29200 | Avg. loss: 5.143 | lr: 0.0002690909090909091\n",
      "Epoch: 1 | Step: 29250 | Avg. loss: 5.024 | lr: 0.0002686868686868687\n",
      "Epoch: 1 | Step: 29300 | Avg. loss: 5.019 | lr: 0.0002682828282828283\n",
      "Epoch: 1 | Step: 29350 | Avg. loss: 5.142 | lr: 0.0002678787878787879\n",
      "Epoch: 1 | Step: 29400 | Avg. loss: 5.123 | lr: 0.0002674747474747475\n",
      "Epoch: 1 | Step: 29450 | Avg. loss: 5.072 | lr: 0.00026707070707070705\n",
      "Epoch: 1 | Step: 29500 | Avg. loss: 5.044 | lr: 0.0002666666666666667\n",
      "Epoch: 1 | Step: 29550 | Avg. loss: 5.108 | lr: 0.00026626262626262625\n",
      "Epoch: 1 | Step: 29600 | Avg. loss: 5.177 | lr: 0.0002658585858585859\n",
      "Epoch: 1 | Step: 29650 | Avg. loss: 5.185 | lr: 0.00026545454545454545\n",
      "Epoch: 1 | Step: 29700 | Avg. loss: 4.983 | lr: 0.000265050505050505\n",
      "Epoch: 1 | Step: 29750 | Avg. loss: 5.098 | lr: 0.00026464646464646464\n",
      "Epoch: 1 | Step: 29800 | Avg. loss: 5.060 | lr: 0.0002642424242424242\n",
      "Epoch: 1 | Step: 29850 | Avg. loss: 5.075 | lr: 0.00026383838383838384\n",
      "Epoch: 1 | Step: 29900 | Avg. loss: 4.943 | lr: 0.0002634343434343434\n",
      "Epoch: 1 | Step: 29950 | Avg. loss: 5.038 | lr: 0.00026303030303030303\n",
      "Epoch: 1 | Step: 30000 | Avg. loss: 5.124 | lr: 0.00026262626262626266\n",
      "Saving model with test loss of 5.894\n",
      "Epoch: 1 | Step: 30050 | Avg. loss: 5.089 | lr: 0.00026222222222222223\n",
      "Epoch: 1 | Step: 30100 | Avg. loss: 5.016 | lr: 0.00026181818181818185\n",
      "Epoch: 1 | Step: 30150 | Avg. loss: 5.071 | lr: 0.0002614141414141414\n",
      "Epoch: 1 | Step: 30200 | Avg. loss: 5.038 | lr: 0.00026101010101010105\n",
      "Epoch: 1 | Step: 30250 | Avg. loss: 5.137 | lr: 0.0002606060606060606\n",
      "Epoch: 1 | Step: 30300 | Avg. loss: 4.977 | lr: 0.0002602020202020202\n",
      "Epoch: 1 | Step: 30350 | Avg. loss: 5.095 | lr: 0.0002597979797979798\n",
      "Epoch: 1 | Step: 30400 | Avg. loss: 5.014 | lr: 0.0002593939393939394\n",
      "Epoch: 1 | Step: 30450 | Avg. loss: 5.075 | lr: 0.000258989898989899\n",
      "Epoch: 1 | Step: 30500 | Avg. loss: 4.976 | lr: 0.0002585858585858586\n",
      "Epoch: 1 | Step: 30550 | Avg. loss: 5.129 | lr: 0.0002581818181818182\n",
      "Epoch: 1 | Step: 30600 | Avg. loss: 5.093 | lr: 0.0002577777777777778\n",
      "Epoch: 1 | Step: 30650 | Avg. loss: 5.072 | lr: 0.00025737373737373735\n",
      "Epoch: 1 | Step: 30700 | Avg. loss: 4.892 | lr: 0.000256969696969697\n",
      "Epoch: 1 | Step: 30750 | Avg. loss: 5.165 | lr: 0.00025656565656565655\n",
      "Epoch: 1 | Step: 30800 | Avg. loss: 4.946 | lr: 0.00025616161616161617\n",
      "Epoch: 1 | Step: 30850 | Avg. loss: 5.091 | lr: 0.00025575757575757574\n",
      "Epoch: 1 | Step: 30900 | Avg. loss: 5.016 | lr: 0.0002553535353535353\n",
      "Epoch: 1 | Step: 30950 | Avg. loss: 5.052 | lr: 0.00025494949494949494\n",
      "Epoch: 1 | Step: 31000 | Avg. loss: 5.128 | lr: 0.0002545454545454545\n",
      "Saving model with test loss of 5.496\n",
      "Epoch: 1 | Step: 31050 | Avg. loss: 5.033 | lr: 0.0002541414141414142\n",
      "Epoch: 1 | Step: 31100 | Avg. loss: 5.146 | lr: 0.00025373737373737376\n",
      "Epoch: 1 | Step: 31150 | Avg. loss: 4.914 | lr: 0.0002533333333333334\n",
      "Epoch: 1 | Step: 31200 | Avg. loss: 5.177 | lr: 0.00025292929292929295\n",
      "Epoch: 1 | Step: 31250 | Avg. loss: 5.083 | lr: 0.0002525252525252525\n",
      "Epoch: 1 | Step: 31300 | Avg. loss: 5.120 | lr: 0.00025212121212121215\n",
      "Epoch: 1 | Step: 31350 | Avg. loss: 5.132 | lr: 0.0002517171717171717\n",
      "Epoch: 1 | Step: 31400 | Avg. loss: 5.118 | lr: 0.00025131313131313135\n",
      "Epoch: 1 | Step: 31450 | Avg. loss: 5.034 | lr: 0.0002509090909090909\n",
      "Epoch: 1 | Step: 31500 | Avg. loss: 5.088 | lr: 0.0002505050505050505\n",
      "Epoch: 1 | Step: 31550 | Avg. loss: 4.996 | lr: 0.0002501010101010101\n",
      "Epoch: 1 | Step: 31600 | Avg. loss: 5.086 | lr: 0.0002496969696969697\n",
      "Epoch: 1 | Step: 31650 | Avg. loss: 5.149 | lr: 0.0002492929292929293\n",
      "Epoch: 1 | Step: 31700 | Avg. loss: 4.975 | lr: 0.0002488888888888889\n",
      "Epoch: 1 | Step: 31750 | Avg. loss: 5.109 | lr: 0.0002484848484848485\n",
      "Epoch: 1 | Step: 31800 | Avg. loss: 5.107 | lr: 0.0002480808080808081\n",
      "Epoch: 1 | Step: 31850 | Avg. loss: 5.116 | lr: 0.0002476767676767677\n",
      "Epoch: 1 | Step: 31900 | Avg. loss: 5.093 | lr: 0.00024727272727272727\n",
      "Epoch: 1 | Step: 31950 | Avg. loss: 5.051 | lr: 0.0002468686868686869\n",
      "Epoch: 1 | Step: 32000 | Avg. loss: 5.098 | lr: 0.00024646464646464647\n",
      "Saving model with test loss of 5.664\n",
      "Epoch: 1 | Step: 32050 | Avg. loss: 5.003 | lr: 0.0002460606060606061\n",
      "Epoch: 1 | Step: 32100 | Avg. loss: 5.179 | lr: 0.00024565656565656566\n",
      "Epoch: 1 | Step: 32150 | Avg. loss: 5.208 | lr: 0.00024525252525252523\n",
      "Epoch: 1 | Step: 32200 | Avg. loss: 5.142 | lr: 0.00024484848484848486\n",
      "Epoch: 1 | Step: 32250 | Avg. loss: 5.096 | lr: 0.00024444444444444443\n",
      "Epoch: 1 | Step: 32300 | Avg. loss: 5.093 | lr: 0.00024404040404040403\n",
      "Epoch: 1 | Step: 32350 | Avg. loss: 5.119 | lr: 0.00024363636363636362\n",
      "Epoch: 1 | Step: 32400 | Avg. loss: 5.131 | lr: 0.00024323232323232325\n",
      "Epoch: 1 | Step: 32450 | Avg. loss: 5.042 | lr: 0.00024282828282828285\n",
      "Epoch: 1 | Step: 32500 | Avg. loss: 5.135 | lr: 0.00024242424242424245\n",
      "Epoch: 1 | Step: 32550 | Avg. loss: 5.082 | lr: 0.00024202020202020202\n",
      "Epoch: 1 | Step: 32600 | Avg. loss: 5.041 | lr: 0.00024161616161616161\n",
      "Epoch: 1 | Step: 32650 | Avg. loss: 5.197 | lr: 0.0002412121212121212\n",
      "Epoch: 1 | Step: 32700 | Avg. loss: 5.047 | lr: 0.0002408080808080808\n",
      "Epoch: 1 | Step: 32750 | Avg. loss: 5.025 | lr: 0.0002404040404040404\n",
      "Epoch: 1 | Step: 32800 | Avg. loss: 4.902 | lr: 0.00024\n",
      "Epoch: 1 | Step: 32850 | Avg. loss: 5.040 | lr: 0.00023959595959595958\n",
      "Epoch: 1 | Step: 32900 | Avg. loss: 5.043 | lr: 0.0002391919191919192\n",
      "Epoch: 1 | Step: 32950 | Avg. loss: 5.172 | lr: 0.0002387878787878788\n",
      "Epoch: 1 | Step: 33000 | Avg. loss: 4.985 | lr: 0.0002383838383838384\n",
      "Saving model with test loss of 5.742\n",
      "Epoch: 1 | Step: 33050 | Avg. loss: 5.160 | lr: 0.000237979797979798\n",
      "Epoch: 1 | Step: 33100 | Avg. loss: 5.133 | lr: 0.0002375757575757576\n",
      "Epoch: 1 | Step: 33150 | Avg. loss: 5.116 | lr: 0.00023717171717171716\n",
      "Epoch: 1 | Step: 33200 | Avg. loss: 5.055 | lr: 0.00023676767676767676\n",
      "Epoch: 1 | Step: 33250 | Avg. loss: 5.091 | lr: 0.00023636363636363636\n",
      "Epoch: 1 | Step: 33300 | Avg. loss: 5.130 | lr: 0.00023595959595959596\n",
      "Epoch: 1 | Step: 33350 | Avg. loss: 5.097 | lr: 0.00023555555555555556\n",
      "Epoch: 1 | Step: 33400 | Avg. loss: 5.129 | lr: 0.00023515151515151515\n",
      "Epoch: 1 | Step: 33450 | Avg. loss: 5.211 | lr: 0.00023474747474747475\n",
      "Epoch: 1 | Step: 33500 | Avg. loss: 5.160 | lr: 0.00023434343434343435\n",
      "Epoch: 1 | Step: 33550 | Avg. loss: 5.031 | lr: 0.00023393939393939395\n",
      "Epoch: 1 | Step: 33600 | Avg. loss: 4.897 | lr: 0.00023353535353535355\n",
      "Epoch: 1 | Step: 33650 | Avg. loss: 4.997 | lr: 0.00023313131313131314\n",
      "Epoch: 1 | Step: 33700 | Avg. loss: 5.070 | lr: 0.00023272727272727274\n",
      "Epoch: 1 | Step: 33750 | Avg. loss: 5.085 | lr: 0.0002323232323232323\n",
      "Epoch: 1 | Step: 33800 | Avg. loss: 4.962 | lr: 0.0002319191919191919\n",
      "Epoch: 1 | Step: 33850 | Avg. loss: 5.150 | lr: 0.0002315151515151515\n",
      "Epoch: 1 | Step: 33900 | Avg. loss: 5.077 | lr: 0.0002311111111111111\n",
      "Epoch: 1 | Step: 33950 | Avg. loss: 5.090 | lr: 0.0002307070707070707\n",
      "Epoch: 1 | Step: 34000 | Avg. loss: 5.098 | lr: 0.00023030303030303033\n",
      "Saving model with test loss of 5.844\n",
      "Epoch: 1 | Step: 34050 | Avg. loss: 5.075 | lr: 0.0002298989898989899\n",
      "Epoch: 1 | Step: 34100 | Avg. loss: 5.069 | lr: 0.0002294949494949495\n",
      "Epoch: 1 | Step: 34150 | Avg. loss: 5.064 | lr: 0.0002290909090909091\n",
      "Epoch: 1 | Step: 34200 | Avg. loss: 5.065 | lr: 0.0002286868686868687\n",
      "Epoch: 1 | Step: 34250 | Avg. loss: 5.011 | lr: 0.0002282828282828283\n",
      "Epoch: 1 | Step: 34300 | Avg. loss: 5.100 | lr: 0.0002278787878787879\n",
      "Epoch: 1 | Step: 34350 | Avg. loss: 4.979 | lr: 0.00022747474747474746\n",
      "Epoch: 1 | Step: 34400 | Avg. loss: 5.216 | lr: 0.00022707070707070706\n",
      "Epoch: 1 | Step: 34450 | Avg. loss: 5.014 | lr: 0.00022666666666666666\n",
      "Epoch: 1 | Step: 34500 | Avg. loss: 5.089 | lr: 0.00022626262626262628\n",
      "Epoch: 1 | Step: 34550 | Avg. loss: 5.142 | lr: 0.00022585858585858588\n",
      "Epoch: 1 | Step: 34600 | Avg. loss: 5.057 | lr: 0.00022545454545454548\n",
      "Epoch: 1 | Step: 34650 | Avg. loss: 5.099 | lr: 0.00022505050505050507\n",
      "Epoch: 1 | Step: 34700 | Avg. loss: 5.097 | lr: 0.00022464646464646465\n",
      "Epoch: 1 | Step: 34750 | Avg. loss: 5.028 | lr: 0.00022424242424242424\n",
      "Epoch: 1 | Step: 34800 | Avg. loss: 5.139 | lr: 0.00022383838383838384\n",
      "Epoch: 1 | Step: 34850 | Avg. loss: 5.072 | lr: 0.00022343434343434344\n",
      "Epoch: 1 | Step: 34900 | Avg. loss: 5.132 | lr: 0.00022303030303030304\n",
      "Epoch: 1 | Step: 34950 | Avg. loss: 5.069 | lr: 0.0002226262626262626\n",
      "Epoch: 1 | Step: 35000 | Avg. loss: 5.149 | lr: 0.0002222222222222222\n",
      "Saving model with test loss of 5.605\n",
      "Epoch: 1 | Step: 35050 | Avg. loss: 5.117 | lr: 0.00022181818181818183\n",
      "Epoch: 1 | Step: 35100 | Avg. loss: 5.100 | lr: 0.00022141414141414143\n",
      "Epoch: 1 | Step: 35150 | Avg. loss: 5.057 | lr: 0.00022101010101010103\n",
      "Epoch: 1 | Step: 35200 | Avg. loss: 5.029 | lr: 0.00022060606060606062\n",
      "Epoch: 1 | Step: 35250 | Avg. loss: 4.980 | lr: 0.00022020202020202022\n",
      "Epoch: 1 | Step: 35300 | Avg. loss: 5.023 | lr: 0.0002197979797979798\n",
      "Epoch: 1 | Step: 35350 | Avg. loss: 5.112 | lr: 0.0002193939393939394\n",
      "Epoch: 1 | Step: 35400 | Avg. loss: 5.008 | lr: 0.000218989898989899\n",
      "Epoch: 1 | Step: 35450 | Avg. loss: 5.059 | lr: 0.0002185858585858586\n",
      "Epoch: 1 | Step: 35500 | Avg. loss: 5.085 | lr: 0.00021818181818181818\n",
      "Epoch: 1 | Step: 35550 | Avg. loss: 5.144 | lr: 0.00021777777777777776\n",
      "Epoch: 1 | Step: 35600 | Avg. loss: 5.152 | lr: 0.00021737373737373738\n",
      "Epoch: 1 | Step: 35650 | Avg. loss: 5.099 | lr: 0.00021696969696969698\n",
      "Epoch: 1 | Step: 35700 | Avg. loss: 5.181 | lr: 0.00021656565656565658\n",
      "Epoch: 1 | Step: 35750 | Avg. loss: 5.064 | lr: 0.00021616161616161617\n",
      "Epoch: 1 | Step: 35800 | Avg. loss: 5.020 | lr: 0.00021575757575757577\n",
      "Epoch: 1 | Step: 35850 | Avg. loss: 5.097 | lr: 0.00021535353535353537\n",
      "Epoch: 1 | Step: 35900 | Avg. loss: 5.123 | lr: 0.00021494949494949494\n",
      "Epoch: 1 | Step: 35950 | Avg. loss: 5.089 | lr: 0.00021454545454545454\n",
      "Epoch: 1 | Step: 36000 | Avg. loss: 5.190 | lr: 0.00021414141414141414\n",
      "Saving model with test loss of 5.632\n",
      "Epoch: 1 | Step: 36050 | Avg. loss: 4.998 | lr: 0.00021373737373737373\n",
      "Epoch: 1 | Step: 36100 | Avg. loss: 5.154 | lr: 0.00021333333333333336\n",
      "Epoch: 1 | Step: 36150 | Avg. loss: 5.010 | lr: 0.00021292929292929296\n",
      "Epoch: 1 | Step: 36200 | Avg. loss: 5.043 | lr: 0.00021252525252525253\n",
      "Epoch: 1 | Step: 36250 | Avg. loss: 5.224 | lr: 0.00021212121212121213\n",
      "Epoch: 1 | Step: 36300 | Avg. loss: 5.104 | lr: 0.00021171717171717172\n",
      "Epoch: 1 | Step: 36350 | Avg. loss: 5.139 | lr: 0.00021131313131313132\n",
      "Epoch: 1 | Step: 36400 | Avg. loss: 5.028 | lr: 0.00021090909090909092\n",
      "Epoch: 1 | Step: 36450 | Avg. loss: 5.104 | lr: 0.00021050505050505052\n",
      "Epoch: 1 | Step: 36500 | Avg. loss: 5.087 | lr: 0.0002101010101010101\n",
      "Epoch: 1 | Step: 36550 | Avg. loss: 4.982 | lr: 0.0002096969696969697\n",
      "Epoch: 1 | Step: 36600 | Avg. loss: 5.086 | lr: 0.00020929292929292928\n",
      "Epoch: 1 | Step: 36650 | Avg. loss: 5.107 | lr: 0.0002088888888888889\n",
      "Epoch: 1 | Step: 36700 | Avg. loss: 5.091 | lr: 0.0002084848484848485\n",
      "Epoch: 1 | Step: 36750 | Avg. loss: 5.087 | lr: 0.0002080808080808081\n",
      "Epoch: 1 | Step: 36800 | Avg. loss: 5.057 | lr: 0.00020767676767676768\n",
      "Epoch: 1 | Step: 36850 | Avg. loss: 5.111 | lr: 0.00020727272727272727\n",
      "Epoch: 1 | Step: 36900 | Avg. loss: 5.188 | lr: 0.00020686868686868687\n",
      "Epoch: 1 | Step: 36950 | Avg. loss: 5.057 | lr: 0.00020646464646464647\n",
      "Epoch: 1 | Step: 37000 | Avg. loss: 5.091 | lr: 0.00020606060606060607\n",
      "Saving model with test loss of 5.705\n",
      "Epoch: 1 | Step: 37050 | Avg. loss: 5.168 | lr: 0.00020565656565656567\n",
      "Epoch: 1 | Step: 37100 | Avg. loss: 5.113 | lr: 0.00020525252525252524\n",
      "Epoch: 1 | Step: 37150 | Avg. loss: 5.190 | lr: 0.00020484848484848483\n",
      "Epoch: 1 | Step: 37200 | Avg. loss: 5.148 | lr: 0.00020444444444444446\n",
      "Epoch: 1 | Step: 37250 | Avg. loss: 5.082 | lr: 0.00020404040404040406\n",
      "Epoch: 1 | Step: 37300 | Avg. loss: 5.100 | lr: 0.00020363636363636366\n",
      "Epoch: 1 | Step: 37350 | Avg. loss: 5.199 | lr: 0.00020323232323232325\n",
      "Epoch: 1 | Step: 37400 | Avg. loss: 5.188 | lr: 0.00020282828282828282\n",
      "Epoch: 1 | Step: 37450 | Avg. loss: 5.006 | lr: 0.00020242424242424242\n",
      "Epoch: 1 | Step: 37500 | Avg. loss: 4.991 | lr: 0.00020202020202020202\n",
      "Epoch: 1 | Step: 37550 | Avg. loss: 5.100 | lr: 0.00020161616161616162\n",
      "Epoch: 1 | Step: 37600 | Avg. loss: 5.038 | lr: 0.00020121212121212122\n",
      "Epoch: 1 | Step: 37650 | Avg. loss: 4.992 | lr: 0.00020080808080808081\n",
      "Epoch: 1 | Step: 37700 | Avg. loss: 5.079 | lr: 0.0002004040404040404\n",
      "Epoch: 1 | Step: 37750 | Avg. loss: 5.056 | lr: 0.0002\n",
      "Epoch: 1 | Step: 37800 | Avg. loss: 5.015 | lr: 0.0001995959595959596\n",
      "Epoch: 1 | Step: 37850 | Avg. loss: 5.089 | lr: 0.0001991919191919192\n",
      "Epoch: 1 | Step: 37900 | Avg. loss: 5.057 | lr: 0.0001987878787878788\n",
      "Epoch: 1 | Step: 37950 | Avg. loss: 4.965 | lr: 0.0001983838383838384\n",
      "Epoch: 1 | Step: 38000 | Avg. loss: 5.118 | lr: 0.00019797979797979797\n",
      "Saving model with test loss of 5.703\n",
      "Epoch: 1 | Step: 38050 | Avg. loss: 5.029 | lr: 0.00019757575757575757\n",
      "Epoch: 1 | Step: 38100 | Avg. loss: 5.078 | lr: 0.00019717171717171717\n",
      "Epoch: 1 | Step: 38150 | Avg. loss: 5.207 | lr: 0.00019676767676767677\n",
      "Epoch: 1 | Step: 38200 | Avg. loss: 5.046 | lr: 0.00019636363636363636\n",
      "Epoch: 1 | Step: 38250 | Avg. loss: 5.009 | lr: 0.000195959595959596\n",
      "Epoch: 1 | Step: 38300 | Avg. loss: 5.076 | lr: 0.00019555555555555556\n",
      "Epoch: 1 | Step: 38350 | Avg. loss: 5.159 | lr: 0.00019515151515151516\n",
      "Epoch: 1 | Step: 38400 | Avg. loss: 5.032 | lr: 0.00019474747474747476\n",
      "Epoch: 1 | Step: 38450 | Avg. loss: 5.147 | lr: 0.00019434343434343435\n",
      "Epoch: 1 | Step: 38500 | Avg. loss: 4.945 | lr: 0.00019393939393939395\n",
      "Epoch: 1 | Step: 38550 | Avg. loss: 5.099 | lr: 0.00019353535353535355\n",
      "Epoch: 1 | Step: 38600 | Avg. loss: 5.091 | lr: 0.00019313131313131312\n",
      "Epoch: 1 | Step: 38650 | Avg. loss: 5.096 | lr: 0.00019272727272727272\n",
      "Epoch: 1 | Step: 38700 | Avg. loss: 5.117 | lr: 0.00019232323232323232\n",
      "Epoch: 1 | Step: 38750 | Avg. loss: 4.989 | lr: 0.00019191919191919191\n",
      "Epoch: 1 | Step: 38800 | Avg. loss: 5.050 | lr: 0.00019151515151515154\n",
      "Epoch: 1 | Step: 38850 | Avg. loss: 5.131 | lr: 0.00019111111111111114\n",
      "Epoch: 1 | Step: 38900 | Avg. loss: 5.098 | lr: 0.0001907070707070707\n",
      "Epoch: 1 | Step: 38950 | Avg. loss: 5.058 | lr: 0.0001903030303030303\n",
      "Epoch: 1 | Step: 39000 | Avg. loss: 5.148 | lr: 0.0001898989898989899\n",
      "Saving model with test loss of 5.662\n",
      "Epoch: 1 | Step: 39050 | Avg. loss: 5.067 | lr: 0.0001894949494949495\n",
      "Epoch: 1 | Step: 39100 | Avg. loss: 5.187 | lr: 0.0001890909090909091\n",
      "Epoch: 1 | Step: 39150 | Avg. loss: 5.152 | lr: 0.0001886868686868687\n",
      "Epoch: 1 | Step: 39200 | Avg. loss: 4.960 | lr: 0.00018828282828282827\n",
      "Epoch: 1 | Step: 39250 | Avg. loss: 5.055 | lr: 0.00018787878787878787\n",
      "Epoch: 1 | Step: 39300 | Avg. loss: 5.080 | lr: 0.0001874747474747475\n",
      "Epoch: 1 | Step: 39350 | Avg. loss: 5.208 | lr: 0.0001870707070707071\n",
      "Epoch: 1 | Step: 39400 | Avg. loss: 5.215 | lr: 0.0001866666666666667\n",
      "Epoch: 1 | Step: 39450 | Avg. loss: 5.212 | lr: 0.00018626262626262628\n",
      "Epoch: 1 | Step: 39500 | Avg. loss: 5.117 | lr: 0.00018585858585858586\n",
      "Epoch: 1 | Step: 39550 | Avg. loss: 5.018 | lr: 0.00018545454545454545\n",
      "Epoch: 1 | Step: 39600 | Avg. loss: 5.182 | lr: 0.00018505050505050505\n",
      "Epoch: 1 | Step: 39650 | Avg. loss: 5.086 | lr: 0.00018464646464646465\n",
      "Epoch: 1 | Step: 39700 | Avg. loss: 5.080 | lr: 0.00018424242424242425\n",
      "Epoch: 1 | Step: 39750 | Avg. loss: 4.999 | lr: 0.00018383838383838384\n",
      "Epoch: 1 | Step: 39800 | Avg. loss: 4.974 | lr: 0.00018343434343434342\n",
      "Epoch: 1 | Step: 39850 | Avg. loss: 5.205 | lr: 0.00018303030303030304\n",
      "Epoch: 1 | Step: 39900 | Avg. loss: 5.203 | lr: 0.00018262626262626264\n",
      "Epoch: 1 | Step: 39950 | Avg. loss: 5.300 | lr: 0.00018222222222222224\n",
      "Epoch: 1 | Step: 40000 | Avg. loss: 5.032 | lr: 0.00018181818181818183\n",
      "Saving model with test loss of 5.917\n",
      "Epoch: 1 | Step: 40050 | Avg. loss: 5.153 | lr: 0.00018141414141414143\n",
      "Epoch: 1 | Step: 40100 | Avg. loss: 5.095 | lr: 0.000181010101010101\n",
      "Epoch: 1 | Step: 40150 | Avg. loss: 5.184 | lr: 0.0001806060606060606\n",
      "Epoch: 1 | Step: 40200 | Avg. loss: 5.170 | lr: 0.0001802020202020202\n",
      "Epoch: 1 | Step: 40250 | Avg. loss: 5.152 | lr: 0.0001797979797979798\n",
      "Epoch: 1 | Step: 40300 | Avg. loss: 5.194 | lr: 0.0001793939393939394\n",
      "Epoch: 1 | Step: 40350 | Avg. loss: 5.163 | lr: 0.000178989898989899\n",
      "Epoch: 1 | Step: 40400 | Avg. loss: 5.044 | lr: 0.0001785858585858586\n",
      "Epoch: 1 | Step: 40450 | Avg. loss: 5.114 | lr: 0.0001781818181818182\n",
      "Epoch: 1 | Step: 40500 | Avg. loss: 5.153 | lr: 0.00017777777777777779\n",
      "Epoch: 1 | Step: 40550 | Avg. loss: 5.156 | lr: 0.00017737373737373738\n",
      "Epoch: 1 | Step: 40600 | Avg. loss: 5.092 | lr: 0.00017696969696969698\n",
      "Epoch: 1 | Step: 40650 | Avg. loss: 4.998 | lr: 0.00017656565656565658\n",
      "Epoch: 1 | Step: 40700 | Avg. loss: 5.178 | lr: 0.00017616161616161615\n",
      "Epoch: 1 | Step: 40750 | Avg. loss: 5.120 | lr: 0.00017575757575757575\n",
      "Epoch: 1 | Step: 40800 | Avg. loss: 4.907 | lr: 0.00017535353535353535\n",
      "Epoch: 1 | Step: 40850 | Avg. loss: 4.990 | lr: 0.00017494949494949494\n",
      "Epoch: 1 | Step: 40900 | Avg. loss: 5.094 | lr: 0.00017454545454545457\n",
      "Epoch: 1 | Step: 40950 | Avg. loss: 5.121 | lr: 0.00017414141414141417\n",
      "Epoch: 1 | Step: 41000 | Avg. loss: 5.106 | lr: 0.00017373737373737374\n",
      "Saving model with test loss of 5.863\n",
      "Epoch: 1 | Step: 41050 | Avg. loss: 4.982 | lr: 0.00017333333333333334\n",
      "Epoch: 1 | Step: 41100 | Avg. loss: 5.139 | lr: 0.00017292929292929293\n",
      "Epoch: 1 | Step: 41150 | Avg. loss: 5.094 | lr: 0.00017252525252525253\n",
      "Epoch: 1 | Step: 41200 | Avg. loss: 5.149 | lr: 0.00017212121212121213\n",
      "Epoch: 1 | Step: 41250 | Avg. loss: 5.072 | lr: 0.00017171717171717173\n",
      "Epoch: 1 | Step: 41300 | Avg. loss: 4.973 | lr: 0.0001713131313131313\n",
      "Epoch: 1 | Step: 41350 | Avg. loss: 5.129 | lr: 0.0001709090909090909\n",
      "Epoch: 1 | Step: 41400 | Avg. loss: 5.156 | lr: 0.0001705050505050505\n",
      "Epoch: 1 | Step: 41450 | Avg. loss: 5.139 | lr: 0.00017010101010101012\n",
      "Epoch: 1 | Step: 41500 | Avg. loss: 5.110 | lr: 0.00016969696969696972\n",
      "Epoch: 1 | Step: 41550 | Avg. loss: 5.175 | lr: 0.00016929292929292932\n",
      "Epoch: 1 | Step: 41600 | Avg. loss: 5.180 | lr: 0.00016888888888888889\n",
      "Epoch: 1 | Step: 41650 | Avg. loss: 5.157 | lr: 0.00016848484848484848\n",
      "Epoch: 1 | Step: 41700 | Avg. loss: 5.087 | lr: 0.00016808080808080808\n",
      "Epoch: 1 | Step: 41750 | Avg. loss: 5.159 | lr: 0.00016767676767676768\n",
      "Epoch: 1 | Step: 41800 | Avg. loss: 5.167 | lr: 0.00016727272727272728\n",
      "Epoch: 1 | Step: 41850 | Avg. loss: 5.058 | lr: 0.00016686868686868688\n",
      "Epoch: 1 | Step: 41900 | Avg. loss: 5.020 | lr: 0.00016646464646464645\n",
      "Epoch: 1 | Step: 41950 | Avg. loss: 5.101 | lr: 0.00016606060606060604\n",
      "Epoch: 1 | Step: 42000 | Avg. loss: 4.988 | lr: 0.00016565656565656567\n",
      "Saving model with test loss of 5.588\n",
      "Epoch: 1 | Step: 42050 | Avg. loss: 5.086 | lr: 0.00016525252525252527\n",
      "Epoch: 1 | Step: 42100 | Avg. loss: 5.156 | lr: 0.00016484848484848487\n",
      "Epoch: 1 | Step: 42150 | Avg. loss: 5.135 | lr: 0.00016444444444444446\n",
      "Epoch: 1 | Step: 42200 | Avg. loss: 5.067 | lr: 0.00016404040404040403\n",
      "Epoch: 1 | Step: 42250 | Avg. loss: 5.090 | lr: 0.00016363636363636363\n",
      "Epoch: 1 | Step: 42300 | Avg. loss: 5.040 | lr: 0.00016323232323232323\n",
      "Epoch: 1 | Step: 42350 | Avg. loss: 4.987 | lr: 0.00016282828282828283\n",
      "Epoch: 1 | Step: 42400 | Avg. loss: 5.081 | lr: 0.00016242424242424243\n",
      "Epoch: 1 | Step: 42450 | Avg. loss: 5.129 | lr: 0.00016202020202020202\n",
      "Epoch: 1 | Step: 42500 | Avg. loss: 5.155 | lr: 0.00016161616161616162\n",
      "Epoch: 1 | Step: 42550 | Avg. loss: 4.988 | lr: 0.00016121212121212122\n",
      "Epoch: 1 | Step: 42600 | Avg. loss: 5.169 | lr: 0.00016080808080808082\n",
      "Epoch: 1 | Step: 42650 | Avg. loss: 5.046 | lr: 0.00016040404040404042\n",
      "Epoch: 1 | Step: 42700 | Avg. loss: 5.166 | lr: 0.00016\n",
      "Epoch: 1 | Step: 42750 | Avg. loss: 5.132 | lr: 0.0001595959595959596\n",
      "Epoch: 1 | Step: 42800 | Avg. loss: 5.035 | lr: 0.00015919191919191918\n",
      "Epoch: 1 | Step: 42850 | Avg. loss: 5.011 | lr: 0.00015878787878787878\n",
      "Epoch: 1 | Step: 42900 | Avg. loss: 5.198 | lr: 0.00015838383838383838\n",
      "Epoch: 1 | Step: 42950 | Avg. loss: 5.112 | lr: 0.00015797979797979798\n",
      "Epoch: 1 | Step: 43000 | Avg. loss: 5.061 | lr: 0.00015757575757575757\n",
      "Saving model with test loss of 5.617\n",
      "Epoch: 1 | Step: 43050 | Avg. loss: 4.911 | lr: 0.0001571717171717172\n",
      "Epoch: 1 | Step: 43100 | Avg. loss: 5.014 | lr: 0.00015676767676767677\n",
      "Epoch: 1 | Step: 43150 | Avg. loss: 5.030 | lr: 0.00015636363636363637\n",
      "Epoch: 1 | Step: 43200 | Avg. loss: 4.993 | lr: 0.00015595959595959597\n",
      "Epoch: 1 | Step: 43250 | Avg. loss: 5.104 | lr: 0.00015555555555555556\n",
      "Epoch: 1 | Step: 43300 | Avg. loss: 5.128 | lr: 0.00015515151515151516\n",
      "Epoch: 1 | Step: 43350 | Avg. loss: 5.109 | lr: 0.00015474747474747476\n",
      "Epoch: 1 | Step: 43400 | Avg. loss: 5.157 | lr: 0.00015434343434343433\n",
      "Epoch: 1 | Step: 43450 | Avg. loss: 5.122 | lr: 0.00015393939393939393\n",
      "Epoch: 1 | Step: 43500 | Avg. loss: 5.042 | lr: 0.00015353535353535353\n",
      "Epoch: 1 | Step: 43550 | Avg. loss: 5.047 | lr: 0.00015313131313131312\n",
      "Epoch: 1 | Step: 43600 | Avg. loss: 4.968 | lr: 0.00015272727272727275\n",
      "Epoch: 1 | Step: 43650 | Avg. loss: 5.061 | lr: 0.00015232323232323235\n",
      "Epoch: 1 | Step: 43700 | Avg. loss: 5.108 | lr: 0.00015191919191919192\n",
      "Epoch: 1 | Step: 43750 | Avg. loss: 5.001 | lr: 0.00015151515151515152\n",
      "Epoch: 1 | Step: 43800 | Avg. loss: 5.148 | lr: 0.0001511111111111111\n",
      "Epoch: 1 | Step: 43850 | Avg. loss: 5.052 | lr: 0.0001507070707070707\n",
      "Epoch: 1 | Step: 43900 | Avg. loss: 5.070 | lr: 0.0001503030303030303\n",
      "Epoch: 1 | Step: 43950 | Avg. loss: 5.166 | lr: 0.0001498989898989899\n",
      "Epoch: 1 | Step: 44000 | Avg. loss: 4.982 | lr: 0.00014949494949494948\n",
      "Saving model with test loss of 5.722\n",
      "Epoch: 1 | Step: 44050 | Avg. loss: 5.104 | lr: 0.00014909090909090908\n",
      "Epoch: 1 | Step: 44100 | Avg. loss: 5.002 | lr: 0.0001486868686868687\n",
      "Epoch: 1 | Step: 44150 | Avg. loss: 5.108 | lr: 0.0001482828282828283\n",
      "Epoch: 1 | Step: 44200 | Avg. loss: 5.022 | lr: 0.0001478787878787879\n",
      "Epoch: 1 | Step: 44250 | Avg. loss: 4.953 | lr: 0.0001474747474747475\n",
      "Epoch: 1 | Step: 44300 | Avg. loss: 4.938 | lr: 0.00014707070707070706\n",
      "Epoch: 1 | Step: 44350 | Avg. loss: 5.024 | lr: 0.00014666666666666666\n",
      "Epoch: 1 | Step: 44400 | Avg. loss: 5.059 | lr: 0.00014626262626262626\n",
      "Epoch: 1 | Step: 44450 | Avg. loss: 5.140 | lr: 0.00014585858585858586\n",
      "Epoch: 1 | Step: 44500 | Avg. loss: 5.034 | lr: 0.00014545454545454546\n",
      "Epoch: 1 | Step: 44550 | Avg. loss: 5.184 | lr: 0.00014505050505050505\n",
      "Epoch: 1 | Step: 44600 | Avg. loss: 4.958 | lr: 0.00014464646464646463\n",
      "Epoch: 1 | Step: 44650 | Avg. loss: 5.070 | lr: 0.00014424242424242425\n",
      "Epoch: 1 | Step: 44700 | Avg. loss: 5.005 | lr: 0.00014383838383838385\n",
      "Epoch: 1 | Step: 44750 | Avg. loss: 5.113 | lr: 0.00014343434343434345\n",
      "Epoch: 1 | Step: 44800 | Avg. loss: 5.085 | lr: 0.00014303030303030304\n",
      "Epoch: 1 | Step: 44850 | Avg. loss: 4.979 | lr: 0.00014262626262626264\n",
      "Epoch: 1 | Step: 44900 | Avg. loss: 5.036 | lr: 0.0001422222222222222\n",
      "Epoch: 1 | Step: 44950 | Avg. loss: 5.013 | lr: 0.0001418181818181818\n",
      "Epoch: 1 | Step: 45000 | Avg. loss: 5.006 | lr: 0.0001414141414141414\n",
      "Saving model with test loss of 5.959\n",
      "Epoch: 1 | Step: 45050 | Avg. loss: 5.065 | lr: 0.000141010101010101\n",
      "Epoch: 1 | Step: 45100 | Avg. loss: 5.098 | lr: 0.0001406060606060606\n",
      "Epoch: 1 | Step: 45150 | Avg. loss: 5.053 | lr: 0.0001402020202020202\n",
      "Epoch: 1 | Step: 45200 | Avg. loss: 5.060 | lr: 0.0001397979797979798\n",
      "Epoch: 1 | Step: 45250 | Avg. loss: 5.050 | lr: 0.0001393939393939394\n",
      "Epoch: 1 | Step: 45300 | Avg. loss: 5.113 | lr: 0.000138989898989899\n",
      "Epoch: 1 | Step: 45350 | Avg. loss: 5.050 | lr: 0.0001385858585858586\n",
      "Epoch: 1 | Step: 45400 | Avg. loss: 4.944 | lr: 0.0001381818181818182\n",
      "Epoch: 1 | Step: 45450 | Avg. loss: 5.047 | lr: 0.0001377777777777778\n",
      "Epoch: 1 | Step: 45500 | Avg. loss: 5.023 | lr: 0.00013737373737373736\n",
      "Epoch: 1 | Step: 45550 | Avg. loss: 4.986 | lr: 0.00013696969696969696\n",
      "Epoch: 1 | Step: 45600 | Avg. loss: 5.201 | lr: 0.00013656565656565656\n",
      "Epoch: 1 | Step: 45650 | Avg. loss: 5.020 | lr: 0.00013616161616161615\n",
      "Epoch: 1 | Step: 45700 | Avg. loss: 4.948 | lr: 0.00013575757575757578\n",
      "Epoch: 1 | Step: 45750 | Avg. loss: 5.107 | lr: 0.00013535353535353538\n",
      "Epoch: 1 | Step: 45800 | Avg. loss: 5.036 | lr: 0.00013494949494949495\n",
      "Epoch: 1 | Step: 45850 | Avg. loss: 5.004 | lr: 0.00013454545454545455\n",
      "Epoch: 1 | Step: 45900 | Avg. loss: 4.924 | lr: 0.00013414141414141414\n",
      "Epoch: 1 | Step: 45950 | Avg. loss: 5.032 | lr: 0.00013373737373737374\n",
      "Epoch: 1 | Step: 46000 | Avg. loss: 5.242 | lr: 0.00013333333333333334\n",
      "Saving model with test loss of 5.615\n",
      "Epoch: 1 | Step: 46050 | Avg. loss: 5.146 | lr: 0.00013292929292929294\n",
      "Epoch: 1 | Step: 46100 | Avg. loss: 5.039 | lr: 0.0001325252525252525\n",
      "Epoch: 1 | Step: 46150 | Avg. loss: 5.218 | lr: 0.0001321212121212121\n",
      "Epoch: 1 | Step: 46200 | Avg. loss: 5.150 | lr: 0.0001317171717171717\n",
      "Epoch: 1 | Step: 46250 | Avg. loss: 5.115 | lr: 0.00013131313131313133\n",
      "Epoch: 1 | Step: 46300 | Avg. loss: 5.113 | lr: 0.00013090909090909093\n",
      "Epoch: 1 | Step: 46350 | Avg. loss: 5.213 | lr: 0.00013050505050505053\n",
      "Epoch: 1 | Step: 46400 | Avg. loss: 4.988 | lr: 0.0001301010101010101\n",
      "Epoch: 1 | Step: 46450 | Avg. loss: 4.998 | lr: 0.0001296969696969697\n",
      "Epoch: 1 | Step: 46500 | Avg. loss: 4.879 | lr: 0.0001292929292929293\n",
      "Epoch: 1 | Step: 46550 | Avg. loss: 5.042 | lr: 0.0001288888888888889\n",
      "Epoch: 1 | Step: 46600 | Avg. loss: 5.092 | lr: 0.0001284848484848485\n",
      "Epoch: 1 | Step: 46650 | Avg. loss: 4.980 | lr: 0.00012808080808080809\n",
      "Epoch: 1 | Step: 46700 | Avg. loss: 5.080 | lr: 0.00012767676767676766\n",
      "Epoch: 1 | Step: 46750 | Avg. loss: 5.069 | lr: 0.00012727272727272725\n",
      "Epoch: 1 | Step: 46800 | Avg. loss: 5.111 | lr: 0.00012686868686868688\n",
      "Epoch: 1 | Step: 46850 | Avg. loss: 5.115 | lr: 0.00012646464646464648\n",
      "Epoch: 1 | Step: 46900 | Avg. loss: 5.026 | lr: 0.00012606060606060608\n",
      "Epoch: 1 | Step: 46950 | Avg. loss: 5.046 | lr: 0.00012565656565656567\n",
      "Epoch: 1 | Step: 47000 | Avg. loss: 5.101 | lr: 0.00012525252525252524\n",
      "Saving model with test loss of 5.858\n",
      "Epoch: 1 | Step: 47050 | Avg. loss: 5.049 | lr: 0.00012484848484848484\n",
      "Epoch: 1 | Step: 47100 | Avg. loss: 5.037 | lr: 0.00012444444444444444\n",
      "Epoch: 1 | Step: 47150 | Avg. loss: 4.980 | lr: 0.00012404040404040404\n",
      "Epoch: 1 | Step: 47200 | Avg. loss: 5.060 | lr: 0.00012363636363636364\n",
      "Epoch: 1 | Step: 47250 | Avg. loss: 4.994 | lr: 0.00012323232323232323\n",
      "Epoch: 1 | Step: 47300 | Avg. loss: 5.045 | lr: 0.00012282828282828283\n",
      "Epoch: 1 | Step: 47350 | Avg. loss: 5.278 | lr: 0.00012242424242424243\n",
      "Epoch: 1 | Step: 47400 | Avg. loss: 5.034 | lr: 0.00012202020202020201\n",
      "Epoch: 1 | Step: 47450 | Avg. loss: 5.117 | lr: 0.00012161616161616162\n",
      "Epoch: 1 | Step: 47500 | Avg. loss: 5.116 | lr: 0.00012121212121212122\n",
      "Epoch: 1 | Step: 47550 | Avg. loss: 4.899 | lr: 0.00012080808080808081\n",
      "Epoch: 1 | Step: 47600 | Avg. loss: 5.055 | lr: 0.0001204040404040404\n",
      "Epoch: 1 | Step: 47650 | Avg. loss: 5.196 | lr: 0.00012\n",
      "Epoch: 1 | Step: 47700 | Avg. loss: 5.197 | lr: 0.0001195959595959596\n",
      "Epoch: 1 | Step: 47750 | Avg. loss: 5.106 | lr: 0.0001191919191919192\n",
      "Epoch: 1 | Step: 47800 | Avg. loss: 5.174 | lr: 0.0001187878787878788\n",
      "Epoch: 1 | Step: 47850 | Avg. loss: 5.191 | lr: 0.00011838383838383838\n",
      "Epoch: 1 | Step: 47900 | Avg. loss: 5.176 | lr: 0.00011797979797979798\n",
      "Epoch: 1 | Step: 47950 | Avg. loss: 4.982 | lr: 0.00011757575757575758\n",
      "Epoch: 1 | Step: 48000 | Avg. loss: 4.922 | lr: 0.00011717171717171717\n",
      "Saving model with test loss of 5.727\n",
      "Epoch: 1 | Step: 48050 | Avg. loss: 4.949 | lr: 0.00011676767676767677\n",
      "Epoch: 1 | Step: 48100 | Avg. loss: 4.993 | lr: 0.00011636363636363637\n",
      "Epoch: 1 | Step: 48150 | Avg. loss: 5.015 | lr: 0.00011595959595959596\n",
      "Epoch: 1 | Step: 48200 | Avg. loss: 5.036 | lr: 0.00011555555555555555\n",
      "Epoch: 1 | Step: 48250 | Avg. loss: 5.065 | lr: 0.00011515151515151516\n",
      "Epoch: 1 | Step: 48300 | Avg. loss: 5.077 | lr: 0.00011474747474747475\n",
      "Epoch: 1 | Step: 48350 | Avg. loss: 5.126 | lr: 0.00011434343434343435\n",
      "Epoch: 1 | Step: 48400 | Avg. loss: 5.166 | lr: 0.00011393939393939394\n",
      "Epoch: 1 | Step: 48450 | Avg. loss: 5.000 | lr: 0.00011353535353535353\n",
      "Epoch: 1 | Step: 48500 | Avg. loss: 5.081 | lr: 0.00011313131313131314\n",
      "Epoch: 1 | Step: 48550 | Avg. loss: 5.063 | lr: 0.00011272727272727274\n",
      "Epoch: 1 | Step: 48600 | Avg. loss: 5.059 | lr: 0.00011232323232323232\n",
      "Epoch: 1 | Step: 48650 | Avg. loss: 5.072 | lr: 0.00011191919191919192\n",
      "Epoch: 1 | Step: 48700 | Avg. loss: 5.060 | lr: 0.00011151515151515152\n",
      "Epoch: 1 | Step: 48750 | Avg. loss: 5.176 | lr: 0.0001111111111111111\n",
      "Epoch: 1 | Step: 48800 | Avg. loss: 5.089 | lr: 0.00011070707070707071\n",
      "Epoch: 1 | Step: 48850 | Avg. loss: 5.047 | lr: 0.00011030303030303031\n",
      "Epoch: 1 | Step: 48900 | Avg. loss: 5.035 | lr: 0.0001098989898989899\n",
      "Epoch: 1 | Step: 48950 | Avg. loss: 5.059 | lr: 0.0001094949494949495\n",
      "Epoch: 1 | Step: 49000 | Avg. loss: 5.023 | lr: 0.00010909090909090909\n",
      "Saving model with test loss of 5.991\n",
      "Epoch: 1 | Step: 49050 | Avg. loss: 4.918 | lr: 0.00010868686868686869\n",
      "Epoch: 1 | Step: 49100 | Avg. loss: 5.100 | lr: 0.00010828282828282829\n",
      "Epoch: 1 | Step: 49150 | Avg. loss: 4.990 | lr: 0.00010787878787878789\n",
      "Epoch: 1 | Step: 49200 | Avg. loss: 5.182 | lr: 0.00010747474747474747\n",
      "Epoch: 1 | Step: 49250 | Avg. loss: 4.956 | lr: 0.00010707070707070707\n",
      "Epoch: 1 | Step: 49300 | Avg. loss: 5.056 | lr: 0.00010666666666666668\n",
      "Epoch: 1 | Step: 49350 | Avg. loss: 5.109 | lr: 0.00010626262626262626\n",
      "Epoch: 1 | Step: 49400 | Avg. loss: 4.963 | lr: 0.00010585858585858586\n",
      "Epoch: 1 | Step: 49450 | Avg. loss: 5.096 | lr: 0.00010545454545454546\n",
      "Epoch: 1 | Step: 49500 | Avg. loss: 5.160 | lr: 0.00010505050505050504\n",
      "Epoch: 1 | Step: 49550 | Avg. loss: 5.093 | lr: 0.00010464646464646464\n",
      "Epoch: 1 | Step: 49600 | Avg. loss: 5.132 | lr: 0.00010424242424242425\n",
      "Epoch: 1 | Step: 49650 | Avg. loss: 5.094 | lr: 0.00010383838383838384\n",
      "Epoch: 1 | Step: 49700 | Avg. loss: 5.045 | lr: 0.00010343434343434344\n",
      "Epoch: 1 | Step: 49750 | Avg. loss: 5.071 | lr: 0.00010303030303030303\n",
      "Epoch: 1 | Step: 49800 | Avg. loss: 5.031 | lr: 0.00010262626262626262\n",
      "Epoch: 1 | Step: 49850 | Avg. loss: 5.178 | lr: 0.00010222222222222223\n",
      "Epoch: 1 | Step: 49900 | Avg. loss: 5.158 | lr: 0.00010181818181818183\n",
      "Epoch: 1 | Step: 49950 | Avg. loss: 5.106 | lr: 0.00010141414141414141\n",
      "Epoch: 1 | Step: 50000 | Avg. loss: 5.054 | lr: 0.00010101010101010101\n",
      "Saving model with test loss of 5.770\n",
      "Epoch: 1 | Step: 50050 | Avg. loss: 5.154 | lr: 0.00010060606060606061\n",
      "Epoch: 1 | Step: 50100 | Avg. loss: 5.046 | lr: 0.0001002020202020202\n",
      "Epoch: 1 | Step: 50150 | Avg. loss: 5.208 | lr: 9.97979797979798e-05\n",
      "Epoch: 1 | Step: 50200 | Avg. loss: 5.094 | lr: 9.93939393939394e-05\n",
      "Epoch: 1 | Step: 50250 | Avg. loss: 5.025 | lr: 9.898989898989899e-05\n",
      "Epoch: 1 | Step: 50300 | Avg. loss: 5.075 | lr: 9.858585858585858e-05\n",
      "Epoch: 1 | Step: 50350 | Avg. loss: 4.982 | lr: 9.818181818181818e-05\n",
      "Epoch: 1 | Step: 50400 | Avg. loss: 5.161 | lr: 9.777777777777778e-05\n",
      "Epoch: 1 | Step: 50450 | Avg. loss: 4.988 | lr: 9.737373737373738e-05\n",
      "Epoch: 1 | Step: 50500 | Avg. loss: 5.090 | lr: 9.696969696969698e-05\n",
      "Epoch: 1 | Step: 50550 | Avg. loss: 5.030 | lr: 9.656565656565656e-05\n",
      "Epoch: 1 | Step: 50600 | Avg. loss: 5.071 | lr: 9.616161616161616e-05\n",
      "Epoch: 1 | Step: 50650 | Avg. loss: 5.152 | lr: 9.575757575757577e-05\n",
      "Epoch: 1 | Step: 50700 | Avg. loss: 5.149 | lr: 9.535353535353535e-05\n",
      "Epoch: 1 | Step: 50750 | Avg. loss: 5.139 | lr: 9.494949494949495e-05\n",
      "Epoch: 1 | Step: 50800 | Avg. loss: 5.232 | lr: 9.454545454545455e-05\n",
      "Epoch: 1 | Step: 50850 | Avg. loss: 5.031 | lr: 9.414141414141413e-05\n",
      "Epoch: 1 | Step: 50900 | Avg. loss: 5.135 | lr: 9.373737373737375e-05\n",
      "Epoch: 1 | Step: 50950 | Avg. loss: 4.996 | lr: 9.333333333333334e-05\n",
      "Epoch: 1 | Step: 51000 | Avg. loss: 5.050 | lr: 9.292929292929293e-05\n",
      "Saving model with test loss of 5.549\n",
      "Epoch: 1 | Step: 51050 | Avg. loss: 5.213 | lr: 9.252525252525253e-05\n",
      "Epoch: 1 | Step: 51100 | Avg. loss: 5.117 | lr: 9.212121212121212e-05\n",
      "Epoch: 1 | Step: 51150 | Avg. loss: 5.044 | lr: 9.171717171717171e-05\n",
      "Epoch: 1 | Step: 51200 | Avg. loss: 5.112 | lr: 9.131313131313132e-05\n",
      "Epoch: 1 | Step: 51250 | Avg. loss: 5.013 | lr: 9.090909090909092e-05\n",
      "Epoch: 1 | Step: 51300 | Avg. loss: 5.089 | lr: 9.05050505050505e-05\n",
      "Epoch: 1 | Step: 51350 | Avg. loss: 5.084 | lr: 9.01010101010101e-05\n",
      "Epoch: 1 | Step: 51400 | Avg. loss: 4.914 | lr: 8.96969696969697e-05\n",
      "Epoch: 1 | Step: 51450 | Avg. loss: 5.003 | lr: 8.92929292929293e-05\n",
      "Epoch: 1 | Step: 51500 | Avg. loss: 4.987 | lr: 8.888888888888889e-05\n",
      "Epoch: 1 | Step: 51550 | Avg. loss: 5.100 | lr: 8.848484848484849e-05\n",
      "Epoch: 1 | Step: 51600 | Avg. loss: 5.044 | lr: 8.808080808080808e-05\n",
      "Epoch: 1 | Step: 51650 | Avg. loss: 4.942 | lr: 8.767676767676767e-05\n",
      "Epoch: 1 | Step: 51700 | Avg. loss: 5.179 | lr: 8.727272727272728e-05\n",
      "Epoch: 1 | Step: 51750 | Avg. loss: 4.976 | lr: 8.686868686868687e-05\n",
      "Epoch: 1 | Step: 51800 | Avg. loss: 5.081 | lr: 8.646464646464647e-05\n",
      "Epoch: 1 | Step: 51850 | Avg. loss: 5.046 | lr: 8.606060606060606e-05\n",
      "Epoch: 1 | Step: 51900 | Avg. loss: 4.892 | lr: 8.565656565656565e-05\n",
      "Epoch: 1 | Step: 51950 | Avg. loss: 5.067 | lr: 8.525252525252525e-05\n",
      "Epoch: 1 | Step: 52000 | Avg. loss: 5.125 | lr: 8.484848484848486e-05\n",
      "Saving model with test loss of 5.615\n",
      "Epoch: 1 | Step: 52050 | Avg. loss: 5.058 | lr: 8.444444444444444e-05\n",
      "Epoch: 1 | Step: 52100 | Avg. loss: 5.166 | lr: 8.404040404040404e-05\n",
      "Epoch: 1 | Step: 52150 | Avg. loss: 4.986 | lr: 8.363636363636364e-05\n",
      "Epoch: 1 | Step: 52200 | Avg. loss: 5.106 | lr: 8.323232323232322e-05\n",
      "Epoch: 1 | Step: 52250 | Avg. loss: 5.119 | lr: 8.282828282828283e-05\n",
      "Epoch: 1 | Step: 52300 | Avg. loss: 5.182 | lr: 8.242424242424243e-05\n",
      "Epoch: 1 | Step: 52350 | Avg. loss: 5.044 | lr: 8.202020202020202e-05\n",
      "Epoch: 1 | Step: 52400 | Avg. loss: 5.104 | lr: 8.161616161616161e-05\n",
      "Epoch: 1 | Step: 52450 | Avg. loss: 5.078 | lr: 8.121212121212121e-05\n",
      "Epoch: 1 | Step: 52500 | Avg. loss: 5.116 | lr: 8.080808080808081e-05\n",
      "Epoch: 1 | Step: 52550 | Avg. loss: 4.998 | lr: 8.040404040404041e-05\n",
      "Epoch: 1 | Step: 52600 | Avg. loss: 5.058 | lr: 8e-05\n",
      "Epoch: 1 | Step: 52650 | Avg. loss: 5.072 | lr: 7.959595959595959e-05\n",
      "Epoch: 1 | Step: 52700 | Avg. loss: 5.097 | lr: 7.919191919191919e-05\n",
      "Epoch: 1 | Step: 52750 | Avg. loss: 5.107 | lr: 7.878787878787879e-05\n",
      "Epoch: 1 | Step: 52800 | Avg. loss: 5.137 | lr: 7.838383838383838e-05\n",
      "Epoch: 1 | Step: 52850 | Avg. loss: 5.242 | lr: 7.797979797979798e-05\n",
      "Epoch: 1 | Step: 52900 | Avg. loss: 5.121 | lr: 7.757575757575758e-05\n",
      "Epoch: 1 | Step: 52950 | Avg. loss: 5.153 | lr: 7.717171717171716e-05\n",
      "Epoch: 1 | Step: 53000 | Avg. loss: 5.210 | lr: 7.676767676767676e-05\n",
      "Saving model with test loss of 5.623\n",
      "Epoch: 1 | Step: 53050 | Avg. loss: 4.998 | lr: 7.636363636363637e-05\n",
      "Epoch: 1 | Step: 53100 | Avg. loss: 5.087 | lr: 7.595959595959596e-05\n",
      "Epoch: 1 | Step: 53150 | Avg. loss: 4.873 | lr: 7.555555555555556e-05\n",
      "Epoch: 1 | Step: 53200 | Avg. loss: 5.251 | lr: 7.515151515151515e-05\n",
      "Epoch: 1 | Step: 53250 | Avg. loss: 5.055 | lr: 7.474747474747474e-05\n",
      "Epoch: 1 | Step: 53300 | Avg. loss: 5.106 | lr: 7.434343434343435e-05\n",
      "Epoch: 1 | Step: 53350 | Avg. loss: 5.027 | lr: 7.393939393939395e-05\n",
      "Epoch: 1 | Step: 53400 | Avg. loss: 5.121 | lr: 7.353535353535353e-05\n",
      "Epoch: 1 | Step: 53450 | Avg. loss: 5.232 | lr: 7.313131313131313e-05\n",
      "Epoch: 1 | Step: 53500 | Avg. loss: 5.036 | lr: 7.272727272727273e-05\n",
      "Epoch: 1 | Step: 53550 | Avg. loss: 5.093 | lr: 7.232323232323231e-05\n",
      "Epoch: 1 | Step: 53600 | Avg. loss: 5.083 | lr: 7.191919191919192e-05\n",
      "Epoch: 1 | Step: 53650 | Avg. loss: 5.113 | lr: 7.151515151515152e-05\n",
      "Epoch: 1 | Step: 53700 | Avg. loss: 5.074 | lr: 7.11111111111111e-05\n",
      "Epoch: 1 | Step: 53750 | Avg. loss: 5.139 | lr: 7.07070707070707e-05\n",
      "Epoch: 1 | Step: 53800 | Avg. loss: 5.117 | lr: 7.03030303030303e-05\n",
      "Epoch: 1 | Step: 53850 | Avg. loss: 4.952 | lr: 6.98989898989899e-05\n",
      "Epoch: 1 | Step: 53900 | Avg. loss: 5.100 | lr: 6.94949494949495e-05\n",
      "Epoch: 1 | Step: 53950 | Avg. loss: 5.096 | lr: 6.90909090909091e-05\n",
      "Epoch: 1 | Step: 54000 | Avg. loss: 5.111 | lr: 6.868686868686868e-05\n",
      "Saving model with test loss of 5.707\n",
      "Epoch: 1 | Step: 54050 | Avg. loss: 5.065 | lr: 6.828282828282828e-05\n",
      "Epoch: 1 | Step: 54100 | Avg. loss: 5.116 | lr: 6.787878787878789e-05\n",
      "Epoch: 1 | Step: 54150 | Avg. loss: 5.089 | lr: 6.747474747474747e-05\n",
      "Epoch: 1 | Step: 54200 | Avg. loss: 5.113 | lr: 6.707070707070707e-05\n",
      "Epoch: 1 | Step: 54250 | Avg. loss: 4.933 | lr: 6.666666666666667e-05\n",
      "Epoch: 1 | Step: 54300 | Avg. loss: 5.121 | lr: 6.626262626262625e-05\n",
      "Epoch: 1 | Step: 54350 | Avg. loss: 5.157 | lr: 6.585858585858585e-05\n",
      "Epoch: 1 | Step: 54400 | Avg. loss: 5.126 | lr: 6.545454545454546e-05\n",
      "Epoch: 1 | Step: 54450 | Avg. loss: 5.064 | lr: 6.505050505050505e-05\n",
      "Epoch: 1 | Step: 54500 | Avg. loss: 4.949 | lr: 6.464646464646465e-05\n",
      "Epoch: 1 | Step: 54550 | Avg. loss: 4.979 | lr: 6.424242424242424e-05\n",
      "Epoch: 1 | Step: 54600 | Avg. loss: 5.051 | lr: 6.383838383838383e-05\n",
      "Epoch: 1 | Step: 54650 | Avg. loss: 5.018 | lr: 6.343434343434344e-05\n",
      "Epoch: 1 | Step: 54700 | Avg. loss: 5.097 | lr: 6.303030303030304e-05\n",
      "Epoch: 1 | Step: 54750 | Avg. loss: 5.049 | lr: 6.262626262626262e-05\n",
      "Epoch: 1 | Step: 54800 | Avg. loss: 5.088 | lr: 6.222222222222222e-05\n",
      "Epoch: 1 | Step: 54850 | Avg. loss: 5.110 | lr: 6.181818181818182e-05\n",
      "Epoch: 1 | Step: 54900 | Avg. loss: 5.123 | lr: 6.141414141414142e-05\n",
      "Epoch: 1 | Step: 54950 | Avg. loss: 5.070 | lr: 6.101010101010101e-05\n",
      "Epoch: 1 | Step: 55000 | Avg. loss: 5.081 | lr: 6.060606060606061e-05\n",
      "Saving model with test loss of 5.660\n",
      "Epoch: 1 | Step: 55050 | Avg. loss: 4.971 | lr: 6.02020202020202e-05\n",
      "Epoch: 1 | Step: 55100 | Avg. loss: 5.045 | lr: 5.97979797979798e-05\n",
      "Epoch: 1 | Step: 55150 | Avg. loss: 5.069 | lr: 5.93939393939394e-05\n",
      "Epoch: 1 | Step: 55200 | Avg. loss: 5.143 | lr: 5.898989898989899e-05\n",
      "Epoch: 1 | Step: 55250 | Avg. loss: 5.215 | lr: 5.858585858585859e-05\n",
      "Epoch: 1 | Step: 55300 | Avg. loss: 5.155 | lr: 5.8181818181818185e-05\n",
      "Epoch: 1 | Step: 55350 | Avg. loss: 4.978 | lr: 5.7777777777777776e-05\n",
      "Epoch: 1 | Step: 55400 | Avg. loss: 5.145 | lr: 5.7373737373737374e-05\n",
      "Epoch: 1 | Step: 55450 | Avg. loss: 5.055 | lr: 5.696969696969697e-05\n",
      "Epoch: 1 | Step: 55500 | Avg. loss: 4.987 | lr: 5.656565656565657e-05\n",
      "Epoch: 1 | Step: 55550 | Avg. loss: 5.142 | lr: 5.616161616161616e-05\n",
      "Epoch: 1 | Step: 55600 | Avg. loss: 5.031 | lr: 5.575757575757576e-05\n",
      "Epoch: 1 | Step: 55650 | Avg. loss: 5.238 | lr: 5.535353535353536e-05\n",
      "Epoch: 1 | Step: 55700 | Avg. loss: 5.007 | lr: 5.494949494949495e-05\n",
      "Epoch: 1 | Step: 55750 | Avg. loss: 5.115 | lr: 5.4545454545454546e-05\n",
      "Epoch: 1 | Step: 55800 | Avg. loss: 5.103 | lr: 5.4141414141414144e-05\n",
      "Epoch: 1 | Step: 55850 | Avg. loss: 5.107 | lr: 5.3737373737373735e-05\n",
      "Epoch: 1 | Step: 55900 | Avg. loss: 5.233 | lr: 5.333333333333334e-05\n",
      "Epoch: 1 | Step: 55950 | Avg. loss: 5.092 | lr: 5.292929292929293e-05\n",
      "Epoch: 1 | Step: 56000 | Avg. loss: 5.050 | lr: 5.252525252525252e-05\n",
      "Saving model with test loss of 5.727\n",
      "Epoch: 1 | Step: 56050 | Avg. loss: 5.190 | lr: 5.212121212121213e-05\n",
      "Epoch: 1 | Step: 56100 | Avg. loss: 5.103 | lr: 5.171717171717172e-05\n",
      "Epoch: 1 | Step: 56150 | Avg. loss: 5.041 | lr: 5.131313131313131e-05\n",
      "Epoch: 1 | Step: 56200 | Avg. loss: 5.012 | lr: 5.0909090909090914e-05\n",
      "Epoch: 1 | Step: 56250 | Avg. loss: 5.082 | lr: 5.0505050505050505e-05\n",
      "Epoch: 1 | Step: 56300 | Avg. loss: 4.932 | lr: 5.01010101010101e-05\n",
      "Epoch: 1 | Step: 56350 | Avg. loss: 5.035 | lr: 4.96969696969697e-05\n",
      "Epoch: 1 | Step: 56400 | Avg. loss: 5.149 | lr: 4.929292929292929e-05\n",
      "Epoch: 1 | Step: 56450 | Avg. loss: 5.046 | lr: 4.888888888888889e-05\n",
      "Epoch: 1 | Step: 56500 | Avg. loss: 5.089 | lr: 4.848484848484849e-05\n",
      "Epoch: 1 | Step: 56550 | Avg. loss: 5.017 | lr: 4.808080808080808e-05\n",
      "Epoch: 1 | Step: 56600 | Avg. loss: 5.009 | lr: 4.767676767676768e-05\n",
      "Epoch: 1 | Step: 56650 | Avg. loss: 5.136 | lr: 4.7272727272727275e-05\n",
      "Epoch: 1 | Step: 56700 | Avg. loss: 5.079 | lr: 4.686868686868687e-05\n",
      "Epoch: 1 | Step: 56750 | Avg. loss: 4.973 | lr: 4.6464646464646464e-05\n",
      "Epoch: 1 | Step: 56800 | Avg. loss: 5.119 | lr: 4.606060606060606e-05\n",
      "Epoch: 1 | Step: 56850 | Avg. loss: 5.063 | lr: 4.565656565656566e-05\n",
      "Epoch: 1 | Step: 56900 | Avg. loss: 5.018 | lr: 4.525252525252525e-05\n",
      "Epoch: 1 | Step: 56950 | Avg. loss: 5.035 | lr: 4.484848484848485e-05\n",
      "Epoch: 1 | Step: 57000 | Avg. loss: 5.113 | lr: 4.4444444444444447e-05\n",
      "Saving model with test loss of 5.668\n",
      "Epoch: 1 | Step: 57050 | Avg. loss: 5.147 | lr: 4.404040404040404e-05\n",
      "Epoch: 1 | Step: 57100 | Avg. loss: 4.970 | lr: 4.363636363636364e-05\n",
      "Epoch: 1 | Step: 57150 | Avg. loss: 5.124 | lr: 4.3232323232323234e-05\n",
      "Epoch: 1 | Step: 57200 | Avg. loss: 5.037 | lr: 4.2828282828282825e-05\n",
      "Epoch: 1 | Step: 57250 | Avg. loss: 5.138 | lr: 4.242424242424243e-05\n",
      "Epoch: 1 | Step: 57300 | Avg. loss: 5.060 | lr: 4.202020202020202e-05\n",
      "Epoch: 1 | Step: 57350 | Avg. loss: 5.005 | lr: 4.161616161616161e-05\n",
      "Epoch: 1 | Step: 57400 | Avg. loss: 5.111 | lr: 4.1212121212121216e-05\n",
      "Epoch: 1 | Step: 57450 | Avg. loss: 5.095 | lr: 4.080808080808081e-05\n",
      "Epoch: 1 | Step: 57500 | Avg. loss: 5.071 | lr: 4.0404040404040405e-05\n",
      "Epoch: 1 | Step: 57550 | Avg. loss: 5.058 | lr: 4e-05\n",
      "Epoch: 1 | Step: 57600 | Avg. loss: 4.977 | lr: 3.9595959595959594e-05\n",
      "Epoch: 1 | Step: 57650 | Avg. loss: 4.981 | lr: 3.919191919191919e-05\n",
      "Epoch: 1 | Step: 57700 | Avg. loss: 4.937 | lr: 3.878787878787879e-05\n",
      "Epoch: 1 | Step: 57750 | Avg. loss: 5.046 | lr: 3.838383838383838e-05\n",
      "Epoch: 1 | Step: 57800 | Avg. loss: 5.124 | lr: 3.797979797979798e-05\n",
      "Epoch: 1 | Step: 57850 | Avg. loss: 5.139 | lr: 3.757575757575758e-05\n",
      "Epoch: 1 | Step: 57900 | Avg. loss: 5.127 | lr: 3.7171717171717175e-05\n",
      "Epoch: 1 | Step: 57950 | Avg. loss: 5.102 | lr: 3.6767676767676766e-05\n",
      "Epoch: 1 | Step: 58000 | Avg. loss: 4.982 | lr: 3.6363636363636364e-05\n",
      "Saving model with test loss of 6.019\n",
      "Epoch: 1 | Step: 58050 | Avg. loss: 4.995 | lr: 3.595959595959596e-05\n",
      "Epoch: 1 | Step: 58100 | Avg. loss: 5.262 | lr: 3.555555555555555e-05\n",
      "Epoch: 1 | Step: 58150 | Avg. loss: 5.148 | lr: 3.515151515151515e-05\n",
      "Epoch: 1 | Step: 58200 | Avg. loss: 5.163 | lr: 3.474747474747475e-05\n",
      "Epoch: 1 | Step: 58250 | Avg. loss: 5.170 | lr: 3.434343434343434e-05\n",
      "Epoch: 1 | Step: 58300 | Avg. loss: 5.095 | lr: 3.3939393939393945e-05\n",
      "Epoch: 1 | Step: 58350 | Avg. loss: 5.035 | lr: 3.3535353535353536e-05\n",
      "Epoch: 1 | Step: 58400 | Avg. loss: 4.981 | lr: 3.313131313131313e-05\n",
      "Epoch: 1 | Step: 58450 | Avg. loss: 5.072 | lr: 3.272727272727273e-05\n",
      "Epoch: 1 | Step: 58500 | Avg. loss: 5.032 | lr: 3.232323232323232e-05\n",
      "Epoch: 1 | Step: 58550 | Avg. loss: 5.046 | lr: 3.1919191919191914e-05\n",
      "Epoch: 1 | Step: 58600 | Avg. loss: 5.196 | lr: 3.151515151515152e-05\n",
      "Epoch: 1 | Step: 58650 | Avg. loss: 5.114 | lr: 3.111111111111111e-05\n",
      "Epoch: 1 | Step: 58700 | Avg. loss: 5.038 | lr: 3.070707070707071e-05\n",
      "Epoch: 1 | Step: 58750 | Avg. loss: 5.172 | lr: 3.0303030303030306e-05\n",
      "Epoch: 1 | Step: 58800 | Avg. loss: 5.093 | lr: 2.98989898989899e-05\n",
      "Epoch: 1 | Step: 58850 | Avg. loss: 5.151 | lr: 2.9494949494949495e-05\n",
      "Epoch: 1 | Step: 58900 | Avg. loss: 4.962 | lr: 2.9090909090909093e-05\n",
      "Epoch: 1 | Step: 58950 | Avg. loss: 5.028 | lr: 2.8686868686868687e-05\n",
      "Epoch: 1 | Step: 59000 | Avg. loss: 4.922 | lr: 2.8282828282828285e-05\n",
      "Saving model with test loss of 5.911\n",
      "Epoch: 1 | Step: 59050 | Avg. loss: 5.130 | lr: 2.787878787878788e-05\n",
      "Epoch: 1 | Step: 59100 | Avg. loss: 5.036 | lr: 2.7474747474747474e-05\n",
      "Epoch: 1 | Step: 59150 | Avg. loss: 5.100 | lr: 2.7070707070707072e-05\n",
      "Epoch: 1 | Step: 59200 | Avg. loss: 5.084 | lr: 2.666666666666667e-05\n",
      "Epoch: 1 | Step: 59250 | Avg. loss: 5.022 | lr: 2.626262626262626e-05\n",
      "Epoch: 1 | Step: 59300 | Avg. loss: 5.049 | lr: 2.585858585858586e-05\n",
      "Epoch: 1 | Step: 59350 | Avg. loss: 5.154 | lr: 2.5454545454545457e-05\n",
      "Epoch: 1 | Step: 59400 | Avg. loss: 4.924 | lr: 2.505050505050505e-05\n",
      "Epoch: 1 | Step: 59450 | Avg. loss: 5.187 | lr: 2.4646464646464646e-05\n",
      "Epoch: 1 | Step: 59500 | Avg. loss: 5.164 | lr: 2.4242424242424244e-05\n",
      "Epoch: 1 | Step: 59550 | Avg. loss: 5.069 | lr: 2.383838383838384e-05\n",
      "Epoch: 1 | Step: 59600 | Avg. loss: 5.035 | lr: 2.3434343434343436e-05\n",
      "Epoch: 1 | Step: 59650 | Avg. loss: 5.032 | lr: 2.303030303030303e-05\n",
      "Epoch: 1 | Step: 59700 | Avg. loss: 5.015 | lr: 2.2626262626262625e-05\n",
      "Epoch: 1 | Step: 59750 | Avg. loss: 5.003 | lr: 2.2222222222222223e-05\n",
      "Epoch: 1 | Step: 59800 | Avg. loss: 5.000 | lr: 2.181818181818182e-05\n",
      "Epoch: 1 | Step: 59850 | Avg. loss: 5.071 | lr: 2.1414141414141412e-05\n",
      "Epoch: 1 | Step: 59900 | Avg. loss: 5.088 | lr: 2.101010101010101e-05\n",
      "Epoch: 1 | Step: 59950 | Avg. loss: 5.114 | lr: 2.0606060606060608e-05\n",
      "Epoch: 1 | Step: 60000 | Avg. loss: 5.097 | lr: 2.0202020202020203e-05\n",
      "Saving model with test loss of 5.797\n",
      "Epoch: 1 | Step: 60050 | Avg. loss: 5.005 | lr: 1.9797979797979797e-05\n",
      "Epoch: 1 | Step: 60100 | Avg. loss: 5.054 | lr: 1.9393939393939395e-05\n",
      "Epoch: 1 | Step: 60150 | Avg. loss: 5.065 | lr: 1.898989898989899e-05\n",
      "Epoch: 1 | Step: 60200 | Avg. loss: 4.999 | lr: 1.8585858585858588e-05\n",
      "Epoch: 1 | Step: 60250 | Avg. loss: 5.067 | lr: 1.8181818181818182e-05\n",
      "Epoch: 1 | Step: 60300 | Avg. loss: 5.027 | lr: 1.7777777777777777e-05\n",
      "Epoch: 1 | Step: 60350 | Avg. loss: 5.123 | lr: 1.7373737373737375e-05\n",
      "Epoch: 1 | Step: 60400 | Avg. loss: 5.104 | lr: 1.6969696969696972e-05\n",
      "Epoch: 1 | Step: 60450 | Avg. loss: 5.060 | lr: 1.6565656565656564e-05\n",
      "Epoch: 1 | Step: 60500 | Avg. loss: 5.093 | lr: 1.616161616161616e-05\n",
      "Epoch: 1 | Step: 60550 | Avg. loss: 4.928 | lr: 1.575757575757576e-05\n",
      "Epoch: 1 | Step: 60600 | Avg. loss: 5.035 | lr: 1.5353535353535354e-05\n",
      "Epoch: 1 | Step: 60650 | Avg. loss: 5.025 | lr: 1.494949494949495e-05\n",
      "Epoch: 1 | Step: 60700 | Avg. loss: 5.095 | lr: 1.4545454545454546e-05\n",
      "Epoch: 1 | Step: 60750 | Avg. loss: 5.147 | lr: 1.4141414141414143e-05\n",
      "Epoch: 1 | Step: 60800 | Avg. loss: 5.148 | lr: 1.3737373737373737e-05\n",
      "Epoch: 1 | Step: 60850 | Avg. loss: 5.158 | lr: 1.3333333333333335e-05\n",
      "Epoch: 1 | Step: 60900 | Avg. loss: 5.077 | lr: 1.292929292929293e-05\n",
      "Epoch: 1 | Step: 60950 | Avg. loss: 5.095 | lr: 1.2525252525252526e-05\n",
      "Epoch: 1 | Step: 61000 | Avg. loss: 4.949 | lr: 1.2121212121212122e-05\n",
      "Saving model with test loss of 5.591\n",
      "Epoch: 1 | Step: 61050 | Avg. loss: 5.181 | lr: 1.1717171717171718e-05\n",
      "Epoch: 1 | Step: 61100 | Avg. loss: 5.039 | lr: 1.1313131313131313e-05\n",
      "Epoch: 1 | Step: 61150 | Avg. loss: 5.042 | lr: 1.090909090909091e-05\n",
      "Epoch: 1 | Step: 61200 | Avg. loss: 4.948 | lr: 1.0505050505050505e-05\n",
      "Epoch: 1 | Step: 61250 | Avg. loss: 5.064 | lr: 1.0101010101010101e-05\n",
      "Epoch: 1 | Step: 61300 | Avg. loss: 5.129 | lr: 9.696969696969698e-06\n",
      "Epoch: 1 | Step: 61350 | Avg. loss: 5.159 | lr: 9.292929292929294e-06\n",
      "Epoch: 1 | Step: 61400 | Avg. loss: 5.042 | lr: 8.888888888888888e-06\n",
      "Epoch: 1 | Step: 61450 | Avg. loss: 5.040 | lr: 8.484848484848486e-06\n",
      "Epoch: 1 | Step: 61500 | Avg. loss: 4.964 | lr: 8.08080808080808e-06\n",
      "Epoch: 1 | Step: 61550 | Avg. loss: 5.022 | lr: 7.676767676767677e-06\n",
      "Epoch: 1 | Step: 61600 | Avg. loss: 5.047 | lr: 7.272727272727273e-06\n",
      "Epoch: 1 | Step: 61650 | Avg. loss: 5.164 | lr: 6.8686868686868685e-06\n",
      "Epoch: 1 | Step: 61700 | Avg. loss: 5.050 | lr: 6.464646464646465e-06\n",
      "Epoch: 1 | Step: 61750 | Avg. loss: 5.075 | lr: 6.060606060606061e-06\n",
      "Epoch: 1 | Step: 61800 | Avg. loss: 5.060 | lr: 5.656565656565656e-06\n",
      "Epoch: 1 | Step: 61850 | Avg. loss: 5.106 | lr: 5.2525252525252526e-06\n",
      "Epoch: 1 | Step: 61900 | Avg. loss: 5.076 | lr: 4.848484848484849e-06\n",
      "Epoch: 1 | Step: 61950 | Avg. loss: 4.999 | lr: 4.444444444444444e-06\n",
      "Epoch: 1 | Step: 62000 | Avg. loss: 5.018 | lr: 4.04040404040404e-06\n",
      "Saving model with test loss of 5.495\n",
      "Epoch: 1 | Step: 62050 | Avg. loss: 5.009 | lr: 3.6363636363636366e-06\n",
      "Epoch: 1 | Step: 62100 | Avg. loss: 5.233 | lr: 3.2323232323232324e-06\n",
      "Epoch: 1 | Step: 62150 | Avg. loss: 5.141 | lr: 2.828282828282828e-06\n",
      "Epoch: 1 | Step: 62200 | Avg. loss: 5.118 | lr: 2.4242424242424244e-06\n",
      "Epoch: 1 | Step: 62250 | Avg. loss: 5.109 | lr: 2.02020202020202e-06\n",
      "Epoch: 1 | Step: 62300 | Avg. loss: 5.034 | lr: 1.6161616161616162e-06\n",
      "Epoch: 1 | Step: 62350 | Avg. loss: 5.066 | lr: 1.2121212121212122e-06\n",
      "Epoch: 1 | Step: 62400 | Avg. loss: 5.005 | lr: 8.080808080808081e-07\n",
      "Epoch: 1 | Step: 62450 | Avg. loss: 5.102 | lr: 4.0404040404040405e-07\n",
      "Epoch: 1 | Step: 62500 | Avg. loss: 5.109 | lr: 0.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch_idx in range(n_epochs):\n",
    "  # Randomize data order\n",
    "  data_generator = get_data_generator(train_dataset, LANG_TOKEN_MAPPING,\n",
    "                                      tokenizer, batch_size)\n",
    "                \n",
    "  for batch_idx, (input_batch, label_batch) \\\n",
    "      in tqdm_notebook(enumerate(data_generator), total=n_batches):\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Forward pass\n",
    "    model_out = model.forward(\n",
    "        input_ids = input_batch,\n",
    "        labels = label_batch)\n",
    "\n",
    "    # Calculate loss and update weights\n",
    "    loss = model_out.loss\n",
    "    losses.append(loss.item())\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "\n",
    "    # Print training update info\n",
    "    if (batch_idx + 1) % print_freq == 0:\n",
    "      avg_loss = np.mean(losses[-print_freq:])\n",
    "      print('Epoch: {} | Step: {} | Avg. loss: {:.3f} | lr: {}'.format(\n",
    "          epoch_idx+1, batch_idx+1, avg_loss, scheduler.get_last_lr()[0]))\n",
    "      \n",
    "    if (batch_idx + 1) % checkpoint_freq == 0:\n",
    "      test_loss = eval_model(model, test_dataset)\n",
    "      print('Saving model with test loss of {:.3f}'.format(test_loss))\n",
    "      torch.save(model.state_dict(), model_path)\n",
    "\n",
    "torch.save(model.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "studied-titanium",
   "metadata": {
    "gradient": {}
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(model_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "determined-setup",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f88287548e0>]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "window_size = 50\n",
    "smoothed_losses = []\n",
    "for i in range(len(losses)-window_size):\n",
    "  smoothed_losses.append(np.mean(losses[i:i+window_size]))\n",
    "\n",
    "plt.plot(smoothed_losses[100:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "taken-indiana",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw input text: If your country produced ODS for this purpose, please enter the amount so produced in column 6 on Data Form 3.”\n",
      "Truncated input text: <es>▁If your country▁produced▁ODS for this▁purpose,▁please▁enter the▁amount so▁produced in▁column 6 on Data▁Form 3.”<pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n"
     ]
    }
   ],
   "source": [
    "test_sentence = test_dataset[0]['translation']['en']\n",
    "print('Raw input text:', test_sentence)\n",
    "\n",
    "input_ids = encode_input_str(\n",
    "    text = test_sentence,\n",
    "    target_lang = 'es',\n",
    "    tokenizer = tokenizer,\n",
    "    seq_len = model.config.max_length,\n",
    "    lang_token_map = LANG_TOKEN_MAPPING)\n",
    "input_ids = input_ids.unsqueeze(0).cuda()\n",
    "\n",
    "print('Truncated input text:', tokenizer.convert_tokens_to_string(\n",
    "    tokenizer.convert_ids_to_tokens(input_ids[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "amazing-funeral",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ".\n",
      "..\n"
     ]
    }
   ],
   "source": [
    "output_tokens = model.generate(input_ids, num_beams=10, num_return_sequences=3)\n",
    "# print(output_tokens)\n",
    "for token_set in output_tokens:\n",
    "  print(tokenizer.decode(token_set, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "numerical-tyler",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A surfboarder ran into a shark  ->  \n"
     ]
    }
   ],
   "source": [
    "output_language = 'en' #@param [\"en\", \"de\"]\n",
    "#@title Slick Blue Translate\n",
    "input_text = 'A surfboarder ran into a shark' #@param {type:\"string\"}\n",
    "\n",
    "input_ids = encode_input_str(\n",
    "    text = input_text,\n",
    "    target_lang = output_language,\n",
    "    tokenizer = tokenizer,\n",
    "    seq_len = model.config.max_length,\n",
    "    lang_token_map = LANG_TOKEN_MAPPING)\n",
    "input_ids = input_ids.unsqueeze(0).cuda()\n",
    "\n",
    "output_tokens = model.generate(input_ids, num_beams=20, length_penalty=0.2)\n",
    "print(input_text + '  ->  ' + \\\n",
    "      tokenizer.decode(output_tokens[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "documented-assault",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96bb71d2666b46d5b0008d2e478badff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=2488.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee54a5a372724d3495756a63ab0070e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=1554.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, load_metric\n",
    "\n",
    "metric = load_metric(\"bleu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "postal-indianapolis",
   "metadata": {},
   "outputs": [],
   "source": [
    " compute_metrics=compute_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "intelligent-marina",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.compute_metrics(eval_preds)>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "divided-portfolio",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLeu = 36.37\n"
     ]
    }
   ],
   "source": [
    "print(f'Bleu score = {model.compute_metrics*100:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wireless-township",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reliable-butter",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
