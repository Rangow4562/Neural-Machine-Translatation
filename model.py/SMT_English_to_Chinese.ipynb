{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "SMT_English_to_Chinese.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "2cVVv4e0VoF0"
      },
      "source": [
        "import pickle"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aKVfJuwSxY-w"
      },
      "source": [
        "tokenized_stores = {'en_train': [], 'en_dev': [], 'en_test': [], 'zh_train': [], 'zh_dev': [], 'zh_test': []}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5OCBe2XixY--"
      },
      "source": [
        "\n",
        "for key in tokenized_stores:\n",
        "    file_name = \"/content/sample_data/\" + str(key)[3:] + \".\" + str(key)[0:2]\n",
        "    load = open(file_name)\n",
        "    sentences = load.read().split('\\n')\n",
        "    \n",
        "    for sentence in sentences:\n",
        "        token_store = sentence.split(' ')\n",
        "        tokenized_stores[key].append(token_store)\n",
        "        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4SWltGqbxY_H",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e78edff-df27-4358-a1fa-77077c2b6336"
      },
      "source": [
        "print(tokenized_stores['zh_train'][2])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['研究者', '确定', '了', '12', '项', '评估', '去', '脂', '体重', '变化', '情况', '的', '绝经期', '激素', '疗法', '随机', '试验', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dIZNBPNAxY_S"
      },
      "source": [
        "train_size = len(tokenized_stores['en_train'])\n",
        "dev_size = len(tokenized_stores['en_dev'])\n",
        "test_size = len(tokenized_stores['en_test'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6VMMiv4BxY_c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f3752c1-4c89-43a6-88d5-ef699b99ffc5"
      },
      "source": [
        "\n",
        "\n",
        "en_words = {}\n",
        "zh_words = {}\n",
        "\n",
        "for key in tokenized_stores:\n",
        "    if str(key)[0] == 'e':\n",
        "        # creating en_words\n",
        "        for sentence in tokenized_stores[key]:\n",
        "            for word in sentence:\n",
        "                if word in en_words:\n",
        "                    en_words[word] += 1\n",
        "                else:\n",
        "                    en_words[word] = 1\n",
        "    else:\n",
        "        # creating hi_words\n",
        "        for sentence in tokenized_stores[key]:\n",
        "            for word in sentence:\n",
        "                if word in zh_words:\n",
        "                    zh_words[word] += 1\n",
        "                else:\n",
        "                    zh_words[word] = 1\n",
        "                    \n",
        "en_vocab = len(en_words)\n",
        "zh_vocab = len(zh_words)\n",
        "print(\"Number of Unique Words:\")\n",
        "print(\"> English:\", str(en_vocab))\n",
        "print(\"> Chinese:\", str(zh_vocab))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of Unique Words:\n",
            "> English: 38164\n",
            "> Chinese: 34020\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fCA1636BxY_k"
      },
      "source": [
        "\n",
        "t = {}\n",
        "\n",
        "uniform = 1 / (en_vocab * zh_vocab)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sKUGRo00xY_p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2c0827a4-2805-444c-9620-3d81f303ffac"
      },
      "source": [
        "n_iters = 0\n",
        "max_iters = 25\n",
        "\n",
        "fine_tune = 1\n",
        "has_converged = False\n",
        "\n",
        "while n_iters < max_iters and has_converged == False:\n",
        "    has_converged = True\n",
        "    max_change = -1\n",
        "\n",
        "    n_iters += 1\n",
        "    count = {}\n",
        "    total = {}\n",
        "    for index in range(train_size):\n",
        "        s_total = {}\n",
        "        for en_word in tokenized_stores['en_train'][index]:\n",
        "            s_total[en_word] = 0\n",
        "            for zh_word in tokenized_stores['zh_train'][index]:\n",
        "                if (en_word, zh_word) not in t:\n",
        "                    t[(en_word, zh_word)] = uniform\n",
        "                s_total[en_word] += t[(en_word, zh_word)]\n",
        "\n",
        "        for en_word in tokenized_stores['en_train'][index]:\n",
        "            for zh_word in tokenized_stores['zh_train'][index]:\n",
        "                if (en_word, zh_word) not in count:\n",
        "                    count[(en_word, zh_word)] = 0\n",
        "                count[(en_word, zh_word)] += (t[(en_word, zh_word)] / s_total[en_word])\n",
        "\n",
        "                if zh_word not in total:\n",
        "                    total[zh_word] = 0\n",
        "                total[zh_word] += (t[(en_word, zh_word)] / s_total[en_word])\n",
        "\n",
        "\n",
        "\n",
        "    if fine_tune == 0:\n",
        "      updated = {}\n",
        "   \n",
        "      for index in range(train_size):\n",
        "          for zh_word in tokenized_stores['zh_train'][index]:\n",
        "              for en_word in tokenized_stores['en_train'][index]:\n",
        "                  if (en_word, zh_word) in updated:\n",
        "                      continue\n",
        "                  updated[(en_word, zh_word)] = 1\n",
        "                  if abs(t[(en_word, zh_word)] - count[(en_word, zh_word)] / total[zh_word]) > 0.01:\n",
        "                      has_converged = False\n",
        "                      max_change = max(max_change, abs(t[(en_word, zh_word)] - count[(en_word, zh_word)] / total[zh_word]))\n",
        "                  t[(en_word, zh_word)] = count[(en_word, zh_word)] / total[zh_word]\n",
        "\n",
        "    elif fine_tune == 1:\n",
        " \n",
        "      max_words = 1000\n",
        "      n_zh_words = 0\n",
        "      updates = 0\n",
        "\n",
        "      for zh_word_tuples in sorted(zh_words.items(), key = lambda k:(k[1], k[0]), reverse = True):\n",
        "          zh_word = zh_word_tuples[0]\n",
        "          n_zh_words += 1\n",
        "          if n_zh_words > max_words:\n",
        "              break\n",
        "          n_en_words = 0\n",
        "          for en_word_tuples in sorted(en_words.items(), key = lambda k:(k[1], k[0]), reverse = True):\n",
        "              en_word = en_word_tuples[0]\n",
        "              n_en_words += 1\n",
        "              if n_en_words > max_words:\n",
        "                  break\n",
        "              if (en_word, zh_word) not in count or zh_word not in total:\n",
        "                  continue\n",
        "               \n",
        "              else:\n",
        "                  if abs(t[(en_word, zh_word)] - count[(en_word, zh_word)] / total[zh_word]) > 0.005:\n",
        "                      has_converged = False\n",
        "                      max_change = max(max_change, abs(t[(en_word, zh_word)] - count[(en_word, zh_word)] / total[zh_word]))\n",
        "                  t[(en_word, zh_word)] = count[(en_word, zh_word)] / total[zh_word]\n",
        "\n",
        "    print(\"Iteration \" + str(n_iters) + \" Completed, Maximum Change: \" + str(max_change))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration 1 Completed, Maximum Change: 0.20376036404370573\n",
            "Iteration 2 Completed, Maximum Change: 0.3749189216287341\n",
            "Iteration 3 Completed, Maximum Change: 0.2347414474468955\n",
            "Iteration 4 Completed, Maximum Change: 0.14620234522828746\n",
            "Iteration 5 Completed, Maximum Change: 0.08334033565664983\n",
            "Iteration 6 Completed, Maximum Change: 0.06297144913688518\n",
            "Iteration 7 Completed, Maximum Change: 0.0528870916541162\n",
            "Iteration 8 Completed, Maximum Change: 0.04183652106048669\n",
            "Iteration 9 Completed, Maximum Change: 0.0320971384198398\n",
            "Iteration 10 Completed, Maximum Change: 0.02432988941236436\n",
            "Iteration 11 Completed, Maximum Change: 0.019707657146795954\n",
            "Iteration 12 Completed, Maximum Change: 0.017435692722802987\n",
            "Iteration 13 Completed, Maximum Change: 0.015451156885838202\n",
            "Iteration 14 Completed, Maximum Change: 0.01371928371508796\n",
            "Iteration 15 Completed, Maximum Change: 0.012208097288418596\n",
            "Iteration 16 Completed, Maximum Change: 0.010888563752155522\n",
            "Iteration 17 Completed, Maximum Change: 0.009734712019408287\n",
            "Iteration 18 Completed, Maximum Change: 0.008724008884897327\n",
            "Iteration 19 Completed, Maximum Change: 0.007837079653577883\n",
            "Iteration 20 Completed, Maximum Change: 0.007056772623647123\n",
            "Iteration 21 Completed, Maximum Change: 0.006367977976988182\n",
            "Iteration 22 Completed, Maximum Change: 0.005758259965291446\n",
            "Iteration 23 Completed, Maximum Change: 0.0052173036634021175\n",
            "Iteration 24 Completed, Maximum Change: -1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6yaalpW64cA-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "159d6ac0-68cc-4903-98a8-0f7edd3a9bab"
      },
      "source": [
        "\n",
        "limit = 40\n",
        "for element in sorted(t.items(), key = lambda k:(k[1], k[0]), reverse = True):\n",
        "  print(element)\n",
        "  limit -= 1\n",
        "  if limit <= 0:\n",
        "    break"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(('or', '或'), 0.7726819176949373)\n",
            "(('safety', '安全性'), 0.7500700362309497)\n",
            "(('cardiovascular', '心血管'), 0.7468134875684475)\n",
            "(('2017', '2017'), 0.7112205685834544)\n",
            "(('all', '所有'), 0.7046982270580987)\n",
            "(('Health', 'Health'), 0.6983084389338453)\n",
            "(('prespecified', '预设'), 0.6912945477433061)\n",
            "(('secondary', '次要'), 0.6904431429301936)\n",
            "(('chemotherapy', '化疗'), 0.6902246411458516)\n",
            "(('2016', '2016'), 0.6893259410034063)\n",
            "(('median', '中位'), 0.6870573323320014)\n",
            "(('L1', 'L1'), 0.6842905773099405)\n",
            "(('placebo', '安慰剂'), 0.6822322480622127)\n",
            "(('acute', '急性'), 0.6755286315442989)\n",
            "(('52', '52'), 0.6728076323698157)\n",
            "(('T', 'T'), 0.6711851838077961)\n",
            "(('PD', 'PD'), 0.6707055460847534)\n",
            "(('National', 'National'), 0.6685973569291404)\n",
            "(('and', '和'), 0.6682994163093748)\n",
            "(('baseline', '基线'), 0.6677175533677872)\n",
            "(('stroke', '卒'), 0.6669198539562238)\n",
            "(('2015', '2015'), 0.6642072915941152)\n",
            "((';', ';'), 0.6623895359464148)\n",
            "(('mg', 'mg'), 0.6578949810127961)\n",
            "(('ischemic', '缺血性'), 0.6570312560541093)\n",
            "(('oral', '口服'), 0.6565790195248987)\n",
            "(('@-@', '-'), 0.65540105531895)\n",
            "(('adverse', '不良'), 0.6536179703068606)\n",
            "(('asthma', '哮喘'), 0.6521536404741122)\n",
            "(('or', '或者'), 0.6521325029426955)\n",
            "(('progression', '进展'), 0.6500298463550785)\n",
            "(('hemoglobin', '血红蛋白'), 0.6488541551881032)\n",
            "(('D', 'D'), 0.6445329182316741)\n",
            "(('severe', '重度'), 0.6438846095847309)\n",
            "(('P', 'P'), 0.6400777125980542)\n",
            "(('24', '24'), 0.6393359257778964)\n",
            "(('12', '12'), 0.63689750242773)\n",
            "(('data', '数据'), 0.6367704876378708)\n",
            "(('Figure', '图'), 0.631716508817092)\n",
            "(('2014', '2014'), 0.631047752374472)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DpMllrHFDYV-"
      },
      "source": [
        "\n",
        "file = open(\"translation_model.pkl\",\"wb\")\n",
        "pickle.dump(t, file)\n",
        "file.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4wQ0a-EARrIy"
      },
      "source": [
        "\n",
        "model_name = \"translation_model.pkl\"\n",
        "pickle_in = open(model_name,\"rb\")\n",
        "t = pickle.load(pickle_in)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yaz1CgJoCllZ"
      },
      "source": [
        "I = {}\n",
        "for index in range(train_size):\n",
        "    for en_id in range(len(tokenized_stores['en_train'][index])):\n",
        "        length = len(tokenized_stores['en_train'][index])\n",
        "        if length not in I:\n",
        "            I[length] = {} # maps the positional difference to a tuple: (sum of t's, count)\n",
        "        for zh_id in range(len(tokenized_stores['zh_train'][index])):\n",
        "            if (zh_id - en_id) not in I[length]:\n",
        "                I[length][(zh_id - en_id)] = [t[(tokenized_stores['en_train'][index][en_id], tokenized_stores['zh_train'][index][zh_id])], 1]\n",
        "            else:\n",
        "                I[length][(zh_id - en_id)][0] += t[(tokenized_stores['en_train'][index][en_id], tokenized_stores['zh_train'][index][zh_id])]\n",
        "                I[length][(zh_id - en_id)][1] += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wh6ogFdO2r_-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0d071c8f-b63d-407b-9d3a-c13e8308f2f9"
      },
      "source": [
        "\n",
        "sentence_lengths = []\n",
        "for key in I.keys():\n",
        "    if key not in sentence_lengths:\n",
        "        sentence_lengths.append(key)\n",
        "sentence_lengths.sort()\n",
        "print(sentence_lengths)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 79, 80, 83, 93, 96, 100, 107]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1CNo6HGz6lqK"
      },
      "source": [
        "\n",
        " \n",
        "p = {}\n",
        "for key in I.keys():\n",
        "    p[key] = {}\n",
        "    sum_val = 0\n",
        "    for diff in I[key].keys():\n",
        "        p[key][diff] = I[key][diff][0] / I[key][diff][1]\n",
        "        sum_val += p[key][diff]\n",
        "    for diff in p[key].keys():\n",
        "        p[key][diff] /= sum_val"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VWI7HCoI81Qa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8f2153aa-0769-4d6a-f2f7-4c3a2fe2be74"
      },
      "source": [
        "for index in range(train_size):\n",
        "    length_en = len(tokenized_stores['en_train'][index])\n",
        "    length_zh = len(tokenized_stores['zh_train'][index])\n",
        "    if length_zh - length_en > 10 and length_en == 1:\n",
        "        print(\"Length of English Sentence:\", str(length_en))\n",
        "        print(\"Length of Hindi Sentence:\", str(length_zh))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Length of English Sentence: 1\n",
            "Length of Hindi Sentence: 14\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yqzyU-2g9_jk"
      },
      "source": [
        "\n",
        "init = {}\n",
        "for length in p:\n",
        "    max_prob = -1\n",
        "    max_jump = 0\n",
        "    for key in p[length].keys():\n",
        "        if p[length][key] > max_prob:\n",
        "            max_prob = p[length][key]\n",
        "            max_jump = key\n",
        "    init[length] = max_jump"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "12MsJzVKnIcb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5925eebb-832e-4a08-9085-129450429cbc"
      },
      "source": [
        "!pip install nltk"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DlM2-MzmExbc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c3791846-4116-4905-88db-6d78866b792b"
      },
      "source": [
        "\n",
        "bigrams = {}\n",
        "unigrams = {}\n",
        "\n",
        "\n",
        "def model(dataset_size, dataset_name):\n",
        "    global bigrams\n",
        "    global unigrams\n",
        "    for index in range(dataset_size):\n",
        "        token_A = ''\n",
        "        for zh_token in tokenized_stores[dataset_name][index]:\n",
        "            if zh_token not in unigrams:\n",
        "                unigrams[zh_token] = 1\n",
        "            else:\n",
        "                unigrams[zh_token] += 1\n",
        "            \n",
        "            token_B = zh_token\n",
        "            if (token_A, token_B) not in bigrams:\n",
        "                bigrams[(token_A, token_B)] = 1\n",
        "            else:\n",
        "                bigrams[(token_A, token_B)] += 1\n",
        "            token_A = token_B\n",
        "\n",
        "model(train_size, 'zh_token')\n",
        "model(dev_size, 'zh_dev')\n",
        "\n",
        "bigram_count = len(bigrams)\n",
        "unigram_count = len(unigrams)\n",
        "print(\"Number of Unique Bigrams:\", bigram_count)\n",
        "print(\"Number of Unique Unigrams:\", unigram_count)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of Unique Bigrams: 317170\n",
            "Number of Unique Unigrams: 43851\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rx33jxTZbZ3-",
        "outputId": "173eb267-f5c9-48b4-98b7-88c2fb603b85"
      },
      "source": [
        "from itertools import permutations\n",
        "import nltk\n",
        "\n",
        "computed_sentences = []\n",
        "total_BLEU = {1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 7: 0}\n",
        "null_BLEU_count = 0\n",
        "\n",
        "sorted_t = sorted(t.items(), key = lambda k:(k[1], k[0]), reverse = True)\n",
        "\n",
        "def find_translation(en_token):\n",
        "    for element in sorted_t:\n",
        "        if element[0][0].lower() == en_token:\n",
        "            return element[0][1]\n",
        "    return \"\"\n",
        "\n",
        "def get_prob(seq):\n",
        "    if len(seq) < 2:\n",
        "        return 1\n",
        "    score = 0\n",
        "    token_A = ''\n",
        "    for hi_token in seq:\n",
        "        token_B = hi_token\n",
        "        if (token_A, token_B) not in bigrams:\n",
        "            if token_B not in unigrams:\n",
        "                continue\n",
        "            else:\n",
        "                score += unigrams[token_B] / unigram_count\n",
        "        else:\n",
        "            base_token_count = 0\n",
        "            if token_A in unigrams:\n",
        "                base_token_count = unigrams[token_A]\n",
        "            score += (bigrams[(token_A, token_B)] + 1) / (base_token_count + unigram_count)\n",
        "        token_A = token_B\n",
        "    return score\n",
        "\n",
        "count = 0\n",
        "for index in range(test_size):\n",
        "    if len(tokenized_stores['en_test'][index]) > 8 or len(tokenized_stores['en_test'][index]) < 2:\n",
        "        continue\n",
        "\n",
        "    translated_words = []\n",
        "    for en_token in tokenized_stores['en_test'][index]:\n",
        "        translation = find_translation(en_token)\n",
        "        if translation != \"\":\n",
        "            translated_words.append(translation)\n",
        "\n",
        "    perm = permutations(translated_words)\n",
        "\n",
        "    best_seq = translated_words\n",
        "    best_prob = -1\n",
        "\n",
        "    for seq in perm:\n",
        "        prob = get_prob(seq)\n",
        "        if prob > best_prob:\n",
        "            best_prob = prob\n",
        "            best_seq = seq\n",
        "\n",
        "    BLEU_scores = []\n",
        "    BLEU_scores.append(nltk.translate.bleu_score.sentence_bleu([tokenized_stores['zh_test'][index]], best_seq, smoothing_function=nltk.translate.bleu_score.SmoothingFunction().method1))\n",
        "    BLEU_scores.append(nltk.translate.bleu_score.sentence_bleu([tokenized_stores['zh_test'][index]], best_seq, smoothing_function=nltk.translate.bleu_score.SmoothingFunction().method2))\n",
        "    BLEU_scores.append(nltk.translate.bleu_score.sentence_bleu([tokenized_stores['zh_test'][index]], best_seq, smoothing_function=nltk.translate.bleu_score.SmoothingFunction().method3))\n",
        "    BLEU_scores.append(nltk.translate.bleu_score.sentence_bleu([tokenized_stores['zh_test'][index]], best_seq, smoothing_function=nltk.translate.bleu_score.SmoothingFunction().method4))\n",
        "    BLEU_scores.append(nltk.translate.bleu_score.sentence_bleu([tokenized_stores['zh_test'][index]], best_seq, smoothing_function=nltk.translate.bleu_score.SmoothingFunction().method5))\n",
        "    BLEU_scores.append(nltk.translate.bleu_score.sentence_bleu([tokenized_stores['zh_test'][index]], best_seq, smoothing_function=nltk.translate.bleu_score.SmoothingFunction().method7))\n",
        "\n",
        "    for key in total_BLEU.keys():\n",
        "        if key == 7:\n",
        "            consider = 5\n",
        "        else: consider = key - 1\n",
        "        total_BLEU[key] += BLEU_scores[consider]\n",
        "    \n",
        "    if BLEU_scores[0] == 0:\n",
        "        null_BLEU_count += 1\n",
        "    \n",
        "    count += 1\n",
        "    print(\"Sentence Index: \", str(count))\n",
        "    print(\"English Sentence:\", str(tokenized_stores['en_test'][index]))\n",
        "    print(\"Reference Chinese Sentence:\", str(tokenized_stores['zh_test'][index]))\n",
        "    print(\"Translated Sentence:\", str(best_seq))\n",
        "    print(\"Translation BLEU Scores\", str(BLEU_scores))\n",
        "    print()\n",
        "    \n",
        "    computed_sentences.append([tokenized_stores['en_test'][index], tokenized_stores['zh_test'][index], best_seq, BLEU_scores])\n",
        "\n",
        "tested = count"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sentence Index:  1\n",
            "English Sentence: ['Your', 'self-confidence', 'also', 'increases', 'with', 'teeth', '.']\n",
            "Reference Hindi Sentence: ['你的'，'自信'，'也'，'增加'，'与'，'牙齿', '।']\n",
            "Translated Sentence: ('।', '你的'，'自信'，'也'，'增加', '।', '也')\n",
            "Translation BLEU Scores [0.04849815007763549, 0.19822631894109965, 0.07654112967106118, 0.18815926093992244, 0.09193101000946054, 0.2362891668915472]\n",
            "\n",
            "Sentence Index:  2\n",
            "English Sentence: ['Your', 'self-confidence', 'also', 'increases', 'with', 'teeth', '.']\n",
            "Reference Hindi Sentence: [“细菌”、“停留”、“中间”、“我们的”、“牙龈”、“和”、“牙齿”, '।']\n",
            "Translated Sentence: ('।', “细菌”、“停留”、“中间”、“我们的”, '।', '也')\n",
            "Translation BLEU Scores [0.034052233956373766, 0.17820132316770917, 0.06770186228657868, 0.20529031524114758, 0.08738752502425957, 0.2638193908491072]\n",
            "\n",
            "Sentence Index:  3\n",
            "English Sentence: ['They', 'make', 'teeth', 'dirty', 'and', 'breath', 'stinky', '.']\n",
            "Reference Hindi Sentence: [“他们”、“制造”、“牙齿”、“脏”、“和”、“呼吸”、“臭”, '।']\n",
            "Translated Sentence: ('।', “脏”、“和”、“呼吸”、, '।', '也')\n",
            "Translation BLEU Scores [[0.019229913610012243, 0.10063351655856652, 0.03823246852690465, 0.11593116128936173, 0.04934931901860593, 0.14898359094953884]\n",
            "\n",
            "Sentence Index:  4\n",
            "English Sentence: ['Clean', 'your', 'teeth', 'properly', '.']\n",
            "Reference Hindi Sentence: [“清洁”、“您的”、“牙齿”、“正确”、, '।']\n",
            "Translated Sentence: ('।',“清洁”、“您的”、“牙齿” '।', '也')\n",
            "Translation BLEU Scores [0.045131921809482646, 0.1878296463217631, 0.08973024087021203, 0.10990031779775743, 0.06060465697360007, 0.14348435345097704]\n",
            "\n",
            "Sentence Index:  5\n",
            "English Sentence: ['Try', 'your', 'best', 'to', 'quit', 'it', '.']\n",
            "Reference Hindi Sentence: ['尝试'、'你的'、'最好的'、'去'、'退出'、'它', '।']\n",
            "Translated Sentence: ('।',、'最好的'、'去'、'退出', '।', '也')\n",
            "Translation BLEU Scores [0.045131921809482646, 0.1878296463217631, 0.08973024087021203, 0.10990031779775743, 0.06060465697360007, 0.14348435345097704]\n",
            "\n",
            "Sentence Index:  6\n",
            "English Sentence: ['Keep', 'doing', 'light', 'physical', 'activities', '.']\n",
            "Reference Hindi Sentence: [“细菌”、“停留”、“中间”、“我们的”、“牙龈”、“和”、“牙齿”, '।']\n",
            "Translated Sentence: ('।', “细菌”、“停留”、“中间”、“我们的”, '।', '也')\n",
            "Translation BLEU Scores [0.041095991233501536, 0.2069322016847137, 0.08170609724417771, 0.20085617938772668, 0.09050304661661013, 0.2569807435004382]\n",
            "\n",
            "Sentence Index:  7\n",
            "English Sentence: ['Stop', 'smoking', '.']\n",
            "Reference Hindi Sentence: [“'停止吸烟', '।']\n",
            "Translated Sentence: ('।','停止吸烟', '।', '也')\n",
            "Translation BLEU Scores [0.05501080739920602, 0.19765609300943976, 0.10937121222607606, 0.05867012709475033, 0.047198954308813615, 0.08917419648057194]\n",
            "\n",
            "Sentence Index:  8\n",
            "English Sentence: ['If', 'there', 'is', 'health', 'there', 'is', 'everything', '.']\n",
            "Reference Hindi Sentence: [“如果”、“那里”、“是”、“健康”、“那里”、“是”、“一切”, '।']\n",
            "Translated Sentence: ('।', “那里”、“是”、“健康”、“那里”、, '।', '也')\n",
            "Translation BLEU Scores Translation BLEU Scores [0.09554427922043669, 0.3976353643835253, 0.18995892141289816, 0.2326589746035907, 0.1283000598199168, 0.3037563786393773]\n",
            "\n",
            "Sentence Index:  9\n",
            "English Sentence: ['Summer', 'season', 'has', 'started', '.']\n",
            "Reference Hindi Sentence: ['夏天'，'季节'，'有'，'开始'，, '।']\n",
            "Translated Sentence: ('।', '夏天'，'季节'，'有'，'开始, '।', '也')\n",
            "Translation BLEU Scores [0.043472087194499145, 0.22089591134157885, 0.08643019616048525, 0.2620796772330101, 0.11913576983277992, 0.32719347222300477]\n",
            "\n",
            "Sentence Index:  10\n",
            "English Sentence: ['Your', 'self-confidence', 'also', 'increases', 'with', 'teeth', '.']\n",
            "Reference Hindi Sentence: ['你的'、'自信'、'也'、'增加'、'与'、'牙齿', '।']\n",
            "Translated Sentence: ('।', '你的'、'自信'、'也'、'增加'、'与'、'牙齿', '।', '也')\n",
            "Translation BLEU Scores [0.037684991644924185, 0.19148978368719022, 0.07492442692259767, 0.22719108016695733, 0.10327616593776132, 0.2836367900126478]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9dIzWbybhedn",
        "outputId": "dd1f6a75-ec86-45d2-a54b-88fe3717dbbb"
      },
      "source": [
        "# Results:\n",
        "import statistics\n",
        "print(\"Number of Samples Tested Upon: \" + str(tested))\n",
        "print()\n",
        "\n",
        "print(\"Average BLEU Score using Various Smoothing Functions (considering all test samples)\")\n",
        "for key in total_BLEU:\n",
        "    print(\"Method \" + str(key) + \": \" + str(total_BLEU[key] / tested))\n",
        "print()\n",
        "print(\"Average BLEU Score using Various Smoothing Functions (considering test samples with at-least one word overlap)\")\n",
        "for key in total_BLEU:\n",
        "    print(\"Method \" + str(key) + \": \" + str(total_BLEU[key] / (tested - null_BLEU_count)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of Samples Tested Upon: 50\n",
            "\n",
            "Average BLEU Score using Various Smoothing Functions (considering all test samples\n",
            "Method 1: 0.04045334190563911\n",
            "Method 2: 0.18790274652264405\n",
            "Method 3: 0.08042839674590069\n",
            "Method 4: 0.15545775044775517\n",
            "Method 5: 0.019229913610012243\n",
            "Method 7: 0.1878296463217631\n",
            "\n",
            "Average BLEU Score using Various Smoothing Functions (considering test samples with at-least one word overlap\n",
            "Method 1: 0.04127892031187664\n",
            "Method 2: 0.1917374964516776\n",
            "Method 3: 0.08206979259785784\n",
            "Method 4: 0.15863035759975017\n",
            "Method 5: 0.07735132483192965\n",
            "Method 7: 0.19734686260161405\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}